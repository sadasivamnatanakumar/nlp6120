<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="editorial"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Hum Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Front Hum Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Hum. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Human Neuroscience</journal-title></journal-title-group><issn pub-type="epub">1662-5161</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">38116236</article-id><article-id pub-id-type="pmc">PMC10728818</article-id><article-id pub-id-type="doi">10.3389/fnhum.2023.1320536</article-id><article-categories><subj-group subj-group-type="heading"><subject>Human Neuroscience</subject><subj-group><subject>Editorial</subject></subj-group></subj-group></article-categories><title-group><article-title>Editorial: Advances in artificial intelligence (AI) in brain computer interface (BCI) and Industry 4.0 for human machine interaction (HMI)</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Asgher</surname><given-names>Umer</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="c001" ref-type="corresp">
<sup>*</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/534940/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Ayaz</surname><given-names>Yasar</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/1302752/overview"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Taiar</surname><given-names>Redha</given-names></name><xref rid="aff3" ref-type="aff">
<sup>3</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/313803/overview"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Quality Assurance &#x00026; NUST International Office Directorate (QA &#x00026; NIO Dte), National University of Sciences and Technology (NUST)</institution>, <addr-line>Islamabad</addr-line>, <country>Pakistan</country></aff><aff id="aff2"><sup>2</sup><institution>National Center of Artificial Intelligence (NCAI), National University of Sciences and Technology (NUST)</institution>, <addr-line>Islamabad</addr-line>, <country>Pakistan</country></aff><aff id="aff3"><sup>3</sup><institution>Laboratoire MAT&#x000e9;riaux et Ing&#x000e9;nierie M&#x000e9;canique (MATIM), Universit&#x000e9; de Reims Champagne-Ardenne</institution>, <addr-line>Reims</addr-line>, <country>France</country></aff><author-notes><fn fn-type="edited-by"><p>Edited and reviewed by: Gernot R. M&#x000fc;ller-Putz, Graz University of Technology, Austria</p></fn><corresp id="c001">*Correspondence: Umer Asgher <email>umer.asgher@smme.nust.edu.pk</email></corresp></author-notes><pub-date pub-type="epub"><day>05</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>17</volume><elocation-id>1320536</elocation-id><history><date date-type="received"><day>12</day><month>10</month><year>2023</year></date><date date-type="accepted"><day>21</day><month>11</month><year>2023</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2023 Asgher, Ayaz and Taiar.</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Asgher, Ayaz and Taiar</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><related-article related-article-type="commentary-article" id="RA1" xlink:href="https://www.frontiersin.org/research-topics/30872/advances-in-artificial-intelligence-ai-in-brain-computer-interface-bci-and-industry-40-for-human-machine-interaction-hmi" ext-link-type="uri">Editorial on the Research Topic <article-title>Advances in artificial intelligence (AI) in brain computer interface (BCI) and Industry 4.0 for human machine interaction (HMI)</article-title></related-article><kwd-group><kwd>advances in artificial intelligence</kwd><kwd>brain computer interface (BCI)</kwd><kwd>Industry 4.0&#x02013;advantages and challenges</kwd><kwd>human machine interaction</kwd><kwd>technologies</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that no financial support was received for the research, authorship, and/or publication of this article.</funding-statement></funding-group><counts><fig-count count="0"/><table-count count="0"/><equation-count count="0"/><ref-count count="0"/><page-count count="2"/><word-count count="1289"/></counts><custom-meta-group><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Brain-Computer Interfaces</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>The convergence of Artificial Intelligence (AI) and Brain-Computer Interface (BCI) has witnessed substantial advancements, particularly in the domains of emotion recognition and cognitive screening. This comprehensive editorial delves into the latest developments presented in the Research Topic &#x0201c;<italic>Advances in Artificial Intelligence (AI) in Brain-Computer Interface (BCI) and Industry 4.0 for Human-Machine Interaction (HMI)</italic>&#x0201d; of Front. Hum. Neurosci., Sec. Brain-Computer Interfaces. Over the past decade, noteworthy industrial progress has transpired in computerized control and monitoring applications, further catalyzing the integration of advanced technologies such as BCIs empowered by artificial intelligence. Modern BCIs are situated at the confluence of data acquisition, signal processing, artificial intelligence, and cyber-physical systems (CPS). Innovations in algorithms, particularly in cognitive computing, are fueling the continuous infusion of artificial intelligence into realms like BCIs, Industry 4.0, and Surgery 4.0 (healthcare), with the aim of establishing a robust industrial artificial intelligence ecosystem. Industry 4.0, a swiftly evolving sector, seeks to revolutionize traditional industrial methods through the deployment of digital tools such as artificial intelligence and brain-computer interfaces. Sophisticated artificial intelligence algorithms, including machine and deep learning, play a pivotal role in enhancing the performance of a BCI system, facilitating more effective management of real-life challenges. BCI-based solutions are gaining traction in bolstering industrial performance, from precise assessment to optimizing neuroergonomic systems, accurately evaluating the mental and cognitive workload of industrial operators, facilitating human-robot interactions, robot-assisted surgeries, and ensuring safety in critical conditions.</p><p>BCIs offer a methodology for manipulating computers and external mechatronic devices based on brain signals. Recently, the modern industrial sector has exhibited a growing interest in BCI-operated machines. The research and development of innovative BCIs, coupled with advancements in AI, may eventually give rise to a robust artificial intelligence-centric industry. Studies suggest that BCIs may find applications outside of laboratories and in ecologically relevant settings. However, deploying BCI systems in real-world ecological applications poses various challenges, including the need for accurate recognition of human mental states and emotions. Addressing these challenges in emotion detection, mental workload, and mental state recognition may require sophisticated approaches based on novel machine or deep learning models. In modern industry and healthcare, efforts are underway to develop hardware, software, machines, and devices with human-like intelligence.</p><p>This issue serves as a platform to share ideas, approaches, opinions, and comments on the latest research in AI and BCI, emphasizing challenges pertinent to the future deployment of intelligent AI-based BCI applications for Industry 4.0. Additionally, it aims to provide a forum for researchers to investigate the role of these AI methods in enhancing the performance of existing BCI applications. Four papers are included in this Research Topic, comprising three original research papers and one review. The contributions of these papers underscore the significance of advanced technologies like AI-augmented BCIs in modernizing and improving the quality of life and related applications.</p><p>In the first original research paper, <ext-link xlink:href="https://doi.org/10.3389/fnhum.2022.921346" ext-link-type="uri">Liang et al.</ext-link> introduce two primary contributions. The first is a multi-source joint-domain adaptation network proposal that addresses the challenge of cross-domain generalization in electroencephalography (EEG) emotion recognition. The second involves extensive cross-subject and cross-session transfer experiments on a publicly available emotion EEG dataset, validating the effectiveness of the proposed method. The second original research paper, by <ext-link xlink:href="https://doi.org/10.3389/fnhum.2023.1132254" ext-link-type="uri">Ran et al.</ext-link>, presents three key contributions. First, the authors propose a simplified style transfer mapping method based on the instance selection (SSTM-IS) algorithm, which enhances the accuracy and speed of cross-subject emotion recognition by selecting informative instances and simplifying the update strategy of hyperparameters in style transfer mapping. Second, the proposed algorithm is validated on both public and self-collected datasets, demonstrating higher accuracy in a shorter computing time for real-time emotion recognition applications. Third, the authors design and implement a real-time emotion recognition system that integrates EEG signal acquisition, data processing, emotion recognition, and result visualization. In the third original research paper, <ext-link xlink:href="https://doi.org/10.3389/fnhum.2023.1169949" ext-link-type="uri">Li et al.</ext-link> introduce a novel model, STGATE, which combines a Transformer Learning Block (TLB) and a Spatial-Temporal Graph Attention (STGAT) mechanism. The TLB leverages 2D convolutional layers and a transformer encoder to extract time-frequency information, while the STGAT incorporates spatial and temporal attention mechanisms to capture connections between brain regions and temporal information. The authors' approach treats EEG signals as graph data and integrates them into graph neural networks to capture correlations between EEG channels. In their review, <ext-link xlink:href="https://doi.org/10.3389/fnhum.2023.1133632" ext-link-type="uri">Sirilertmekasakul et al.</ext-link> focus on the application of three AI implementations: machine learning (ML) and deep learning (DL), computer vision, and automatic speech recognition (ASR). The authors discuss the advantages and limitations of digitized cognitive screening tests.</p></sec><sec sec-type="conclusions" id="s2"><title>Conclusion</title><p>The Research Topic &#x0201c;<italic>Advances in Artificial Intelligence (AI) in Brain-Computer Interface (BCI) and Industry 4.0 for Human-Machine Interaction (HMI)</italic>&#x0201d; collectively signifies the dynamic evolution of AI-BCI applications, ranging from real-time emotion recognition to the digitization of cognitive screening tests. These advancements not only contribute to understanding human emotions through EEG signals but also pave the way for accessible and efficient cognitive assessments. The interdisciplinary nature of these studies underscores the potential of AI-BCI in shaping the future of healthcare and human-machine interaction. This Research Topic serves as an invaluable resource for readers seeking in-depth knowledge and understanding of the intricacies of these modern technologies and their significance in contemporary life.</p></sec><sec sec-type="author-contributions" id="s3"><title>Author contributions</title><p>UA: Conceptualization, Investigation, Writing &#x02013; original draft. YA: Investigation, Validation, Visualization, Conceptualization, Writing &#x02013; original draft. RT: Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing.</p></sec></body><back><sec sec-type="COI-statement" id="conf1"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="disclaimer" id="s5"><title>Publisher's note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec></back></article>
