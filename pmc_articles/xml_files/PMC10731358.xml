<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="editorial"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Med (Lausanne)</journal-id><journal-id journal-id-type="iso-abbrev">Front Med (Lausanne)</journal-id><journal-id journal-id-type="publisher-id">Front. Med.</journal-id><journal-title-group><journal-title>Frontiers in Medicine</journal-title></journal-title-group><issn pub-type="epub">2296-858X</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">38126074</article-id><article-id pub-id-type="pmc">PMC10731358</article-id><article-id pub-id-type="doi">10.3389/fmed.2023.1339280</article-id><article-categories><subj-group subj-group-type="heading"><subject>Medicine</subject><subj-group><subject>Editorial</subject></subj-group></subj-group></article-categories><title-group><article-title>Editorial: Big data and artificial intelligence in ophthalmology - clinical application and future exploration</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Tan</surname><given-names>Yong Yu</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/2578466/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Rim</surname><given-names>Tyler Hyungtaek</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><xref rid="aff3" ref-type="aff">
<sup>3</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/1520436/overview"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/></contrib><contrib contrib-type="author"><name><surname>Ting</surname><given-names>Darren S. J.</given-names></name><xref rid="aff4" ref-type="aff">
<sup>4</sup>
</xref><xref rid="aff5" ref-type="aff">
<sup>5</sup>
</xref><xref rid="aff6" ref-type="aff">
<sup>6</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/908998/overview"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Hsieh</surname><given-names>Yi-Ting</given-names></name><xref rid="aff7" ref-type="aff">
<sup>7</sup>
</xref><xref rid="aff8" ref-type="aff">
<sup>8</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/908978/overview"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Kim</surname><given-names>Tae-im</given-names></name><xref rid="aff9" ref-type="aff">
<sup>9</sup>
</xref><xref rid="aff10" ref-type="aff">
<sup>10</sup>
</xref><xref rid="c001" ref-type="corresp">
<sup>*</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/1575125/overview"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Cork University Hospital</institution>, <addr-line>Cork</addr-line>, <country>Ireland</country></aff><aff id="aff2"><sup>2</sup><institution>Department of Ocular Epidemiology, Singapore Eye Research Institute</institution>, <addr-line>Singapore</addr-line>, <country>Singapore</country></aff><aff id="aff3"><sup>3</sup><institution>Mediwhale Inc.</institution>, <addr-line>Seoul</addr-line>, <country>Republic of Korea</country></aff><aff id="aff4"><sup>4</sup><institution>Academic Unit of Ophthalmology, Institute of Inflammation and Ageing, University of Birmingham</institution>, <addr-line>Birmingham</addr-line>, <country>United Kingdom</country></aff><aff id="aff5"><sup>5</sup><institution>Birmingham and Midland Eye Centre</institution>, <addr-line>Birmingham</addr-line>, <country>United Kingdom</country></aff><aff id="aff6"><sup>6</sup><institution>Academic Ophthalmology, School of Medicine, University of Nottingham</institution>, <addr-line>Nottingham</addr-line>, <country>United Kingdom</country></aff><aff id="aff7"><sup>7</sup><institution>Department of Ophthalmology, National Taiwan University Hospital</institution>, <addr-line>Taipei</addr-line>, <country>Taiwan</country></aff><aff id="aff8"><sup>8</sup><institution>Department of Ophthalmology, College of Medicine, National Taiwan University</institution>, <addr-line>Taipei</addr-line>, <country>Taiwan</country></aff><aff id="aff9"><sup>9</sup><institution>Department of Ophthalmology, The Institute of Vision Research, Yonsei University College of Medicine</institution>, <addr-line>Seoul</addr-line>, <country>Republic of Korea</country></aff><aff id="aff10"><sup>10</sup><institution>Department of Ophthalmology, Corneal Dystrophy Research Institute, Yonsei University College of Medicine</institution>, <addr-line>Seoul</addr-line>, <country>Republic of Korea</country></aff><author-notes><fn fn-type="edited-by"><p>Edited and reviewed by: Jodhbir Mehta, Singapore National Eye Center, Singapore</p></fn><corresp id="c001">*Correspondence: Tae-im Kim <email>tikim@yuhs.ac</email></corresp></author-notes><pub-date pub-type="epub"><day>06</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>10</volume><elocation-id>1339280</elocation-id><history><date date-type="received"><day>15</day><month>11</month><year>2023</year></date><date date-type="accepted"><day>23</day><month>11</month><year>2023</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2023 Tan, Rim, Ting, Hsieh and Kim.</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Tan, Rim, Ting, Hsieh and Kim</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><related-article related-article-type="commentary-article" id="RA1" xlink:href="https://www.frontiersin.org/research-topics/51849/big-data-and-artificial-intelligence-in-ophthalmology---clinical-application-and-future-exploration" ext-link-type="uri">Editorial on the Research Topic <article-title>Big data and artificial intelligence in ophthalmology - clinical application and future exploration</article-title></related-article><kwd-group><kwd>big data</kwd><kwd>ophthalmology</kwd><kwd>artificial intelligence</kwd><kwd>deep learning</kwd><kwd>machine learning</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that no financial support was received for the research, authorship, and/or publication of this article.</funding-statement></funding-group><counts><fig-count count="0"/><table-count count="0"/><equation-count count="0"/><ref-count count="0"/><page-count count="3"/><word-count count="1577"/></counts><custom-meta-group><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Ophthalmology</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>Artificial Intelligence (AI) stands at the forefront of innovation in ophthalmology, harnessing vast datasets to redefine diagnostics and treatment strategies. This Research Topic collates pioneering insights from global experts, emphasizing AI's transformative impact on ophthalmic healthcare. Contributors have adeptly navigated the challenges, offering novel algorithms and applications poised to elevate patient care, streamline service delivery, and broaden healthcare access. From intricate retinal imaging to expansive electronic health record analyses, the papers within this topic not only underscore AI's potent capabilities but also chart a course for its future roles in enhancing ophthalmological practice.</p></sec><sec id="s2"><title>Diagnostic and predictive analytics</title><p><ext-link xlink:href="https://doi.org/10.3389/fmed.2023.1162124" ext-link-type="uri">Won et al.</ext-link> presented a groundbreaking deep learning (DL) based classification system adept at distinguishing between bacterial and fungal keratitis through anterior segment photographs. Utilizing a dataset comprising 684 images from 107 patients confirmed with either bacterial or fungal keratitis, the study introduced two novel modules&#x02014;the Lesion Guiding Module and the Mask Adjusting Module &#x02014;which, when integrated with the ResNet-50 classifier, significantly outperformed the baseline model with an accuracy leap from 81.1 to 87.8%. The system's proficiency was further validated on an external set of 98 images, solidifying its potential as a rapid, reliable diagnostic tool in clinical settings.</p><p><ext-link xlink:href="https://doi.org/10.3389/fmed.2023.1115032" ext-link-type="uri">Li et al.</ext-link> compared DL with human graders for evaluation against 300 fundus photographs. The AI's accuracy for diagnosing diabetic retinopathy and macular degeneration was on par with ophthalmologists, achieving an AUC of 0.990 and 0.945, respectively. It excelled in identifying glaucomatous optic neuropathy with an AUC of 0.994, better than human graders.</p><p><ext-link xlink:href="https://doi.org/10.3389/fmed.2023.1188542" ext-link-type="uri">Liu et al.</ext-link> created ONION, a DL tool that discerns optic neuritis from optic neuropathy in acute phases, demonstrating an AUC of 0.903. Trained with EfficientNet-B0 on 871 eyes from 547 patients, ONION matched a retinal specialist's diagnostic ability, showing 79.6% sensitivity and 86.5% specificity in validation. It processed results in 23 s, highlighting its potential for rapid, accurate eye condition diagnosis in various healthcare settings.</p><p><ext-link xlink:href="https://doi.org/10.3389/fmed.2023.1165135" ext-link-type="uri">Wang et al.</ext-link> introduced a novel DL approach to forecast postoperative visual outcomes in patients undergoing cataract surgery. Leveraging a dataset of 2051 eyes, their Model V achieved the lowest mean absolute error of 0.1250 and 0.1194 logMAR, and RMSE of 0.2284 and 0.2362 logMAR in the validation and test datasets, respectively. It achieved up to 91.7% precision and 93.8% sensitivity, indicating high predictive reliability and marking progress in preoperative patient assessments.</p><p><ext-link xlink:href="https://doi.org/10.3389/fmed.2023.1157016" ext-link-type="uri">Shivananjaiah et al.'s</ext-link> study offerred a novel approach to predicting the likelihood of glaucoma progression to surgical intervention within 1 year, using DL. The researchers curated a cohort from electronic health records at Stanford University, capturing both structured data and free-text clinical notes from 2008 to 2020. The DL model was fed a blend of text embeddings from patient notes and structured clinical data, resulting in an impressive model performance&#x02014;most notably, the multimodal fusion model exhibited an AUC of 0.899 and an F1 score of 0.745. This work demonstrates the potential role of DL in improving glaucoma treatment predictions using comprehensive patient data.</p></sec><sec id="s3"><title>Image analysis and segmentation</title><p><ext-link xlink:href="https://doi.org/10.3389/fmed.2022.1040562" ext-link-type="uri">Imran et al.</ext-link> introduced Feature Preserving Mesh Network (FPM-Net), a network that segments retinal vasculature semantically without preprocessing, crucial for supporting the analysis of ophthalmic diseases, achieving exceptional accuracy (96.92% on DRIVE, 97.28% on CHASE-DB, 97.27% on STARE) and efficiency with only 2.45 million parameters. The research showcases FPM-Net's proficiency in preserving detailed spatial features for improved segmentation performance, essential for accurate retinal vessel analysis, making it a valuable tool for early diagnosis and management of ophthalmic diseases.</p><p><ext-link xlink:href="https://doi.org/10.3389/fmed.2023.1150295" ext-link-type="uri">Bai et al.</ext-link> developed DME-DeepLabV3+ model, a lightweight and proficient model for the extraction of diabetic macular oedema (DME) from optical coherence tomography (OCT) images. This study harnesses the DeepLabV3+ architecture to address the complexity of OCT images, where varied image quality and the blurred boundaries of DME regions pose a significant challenge. Evaluated using a dataset of 1711 OCT images and validated by experienced clinicians, the DME-DeepLabV3+ achieves remarkable performance metrics, including a mean Intersection over Union (MIoU) of 91.18% and high precision and recall rates. This innovation promises to streamline the diagnostic process, offering a rapid, automated, and accurate tool for DME extraction.</p></sec><sec id="s4"><title>Innovation in disease detection and management</title><p><ext-link xlink:href="https://doi.org/10.3389/fmed.2023.1270570" ext-link-type="uri">Gibson et al.</ext-link> harnessed the power of latent diffusion augmentation to enhance the DL analysis of neuro-morphology in limbal stem cell deficiency (LSCD). The study showcased a residual U-Net model, informed by the InceptionResNetV2 transfer learning model, to classify neuron morphology across various stages of LSCD compared to healthy controls. The model achieved accuracy in determining nerve fiber number (<italic>R</italic>-squared of 0.63), branching (<italic>R</italic>-squared of 0.63), and length (<italic>R</italic>-squared of 0.80). This method outperformed the same model trained only on original images, particularly in distinguishing LSCD with an AUC of 0.867. The results suggest that supplementing training data with latent diffusion-generated images can effectively enhance model performance.</p></sec><sec id="s5"><title>Privacy and continual learning in AI</title><p><ext-link xlink:href="https://doi.org/10.3389/fmed.2023.1227515" ext-link-type="uri">Verma et al.</ext-link> explored privacy-preserving methods in continual learning for medical image classification, focusing on retinal disease detection from OCT images and histology-based colon cancer classification. Their study revealed that Brain-Inspired Replay (BIR) excelled in retinal disease classification with notable accuracy, while Efficient Feature Transformations (EFT) were most accurate for colon cancer detection. They found that these methods, though slightly outperformed by joint retraining models, offer significant benefits for long-term clinical use by reducing catastrophic forgetting and enabling ongoing model updates without compromising patient privacy.</p></sec><sec sec-type="conclusions" id="s6"><title>Conclusion</title><p>The breadth of this Research Topic, with its collection of varied and insightful studies, underscores the remarkable potential of big data and AI within ophthalmology. The contributions from the authors, with their innovative algorithms and forward-thinking perspectives, hold significant promise for transforming the fabric of eye care. These studies serve as cornerstones upon which future collaborative endeavors can be built, driving the advancement of ophthalmological practices into a new era. As we stand on the precipice of this technological revolution, the integration of these AI-driven tools and methodologies heralds a progressive shift toward enhanced patient care.</p></sec><sec sec-type="author-contributions" id="s7"><title>Author contributions</title><p>YT: Conceptualization, Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing. TR: Writing &#x02013; review &#x00026; editing, Conceptualization. DT: Writing &#x02013; review &#x00026; editing. Y-TH: Writing &#x02013; review &#x00026; editing. T-iK: Writing &#x02013; review &#x00026; editing.</p></sec></body><back><ack><p>The authors would like to thank the hundreds of colleagues who contributed to this Research Topic. The authors would also like to thank the board and staff of the Frontiers Publishing House for their continuous and unflinching support.</p></ack><sec sec-type="COI-statement" id="conf1"><title>Conflict of interest</title><p>TR was employed by Mediwhale Inc. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The handling editor JM declared a shared affiliation with the author TR.</p><p>The author(s) declared that they were an editorial board member of Frontiers, at the time of submission. This had no impact on the peer review process and the final decision.</p></sec><sec sec-type="disclaimer" id="s9"><title>Publisher's note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec></back></article>
