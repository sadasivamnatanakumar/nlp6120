<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="letter"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Malays Fam Physician</journal-id><journal-id journal-id-type="iso-abbrev">Malays Fam Physician</journal-id><journal-id journal-id-type="publisher-id">MFP</journal-id><journal-title-group><journal-title>Malaysian Family Physician : the Official Journal of the Academy of Family Physicians of Malaysia</journal-title></journal-title-group><issn pub-type="ppub">1985-207X</issn><issn pub-type="epub">1985-2274</issn><publisher><publisher-name>Academy of Family Physician of Malaysia</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">38111832</article-id><article-id pub-id-type="pmc">PMC10726751</article-id><article-id pub-id-type="doi">10.51866/lte.527</article-id><article-id pub-id-type="other">jMFP.v18.i1.pg68</article-id><article-categories><subj-group subj-group-type="heading"><subject>Letter to Editor</subject></subj-group></article-categories><title-group><article-title>Hallucination: A key challenge to Artificial Intelligence-Generated writing</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jamaluddin</surname><given-names>Jazlan</given-names></name><xref rid="A1" ref-type="aff">1</xref><xref rid="COR1" ref-type="corresp">&#x02010;</xref></contrib><contrib contrib-type="author"><name><surname>Gaffar</surname><given-names>Nadia Abd</given-names></name><xref rid="A2" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Din</surname><given-names>Nor Shazatul Salwana</given-names></name><xref rid="A3" ref-type="aff">3</xref></contrib></contrib-group><aff id="A1"><label>1</label> MD, MMed, Klinik Kesihatan Sauk, Jalan Besar Lenggong, Sauk, Kuala Kangsar, Perak, Malaysia. Email: <email>jazlanjamaluddin@gmail.com</email></aff><aff id="A2"><label>2</label> MD, MMed, Klinik Kesihatan Tanjung Malim Jalan Besar, Tanjung Malim, Perak, Malaysia.</aff><aff id="A3"><label>3</label> MD, MMed, Klinik Kesihatan Bota Kiri, Jalan Bota Kiri, Bota Kiri, Perak Tengah, Perak, Malaysia.</aff><author-notes><corresp id="COR1">&#x02010;(Corresponding author) MD, MMed, Klinik Kesihatan Sauk, Jalan Besar Lenggong, Sauk, Kuala Kangsar, Perak, Malaysia. Email: <email>jazlanjamaluddin@gmail.com</email></corresp></author-notes><pub-date pub-type="epub"><day>28</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>18</volume><fpage>68</fpage><lpage>68</lpage><permissions><copyright-statement>&#x000a9; Academy of Family Physicians of Malaysia</copyright-statement><copyright-year>2023</copyright-year><ali:free_to_read xmlns:ali="http://www.niso.org/schemas/ali/1.0/"/><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article licensed under the Creative Commons Attribution 4.0 (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original author(s) and source are properly cited. See: <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>
</license-p></license></permissions><kwd-group kwd-group-type="key-words"><title>Keywords</title><kwd>Artificial intelligence</kwd><kwd>Hallucinations</kwd><kwd>Delusions</kwd><kwd>Medical writing</kwd><kwd>Biomedical research</kwd></kwd-group></article-meta></front><body><sec id="sec1"><title>Dear editor,</title><p>We read the article by Lee et al. with great interest.<sup><xref rid="ref1" ref-type="bibr">1</xref></sup> Artificial intelligence (AI) has become an indispensable tool not only in academic writing, particularly among students, trainees and other related individuals, but also in clinical and clerical work, especially in resource-limited and rural clinics, where it continues to aid clinicians in drafting letters, memos, dossiers and other documents. With the increasing use of AI, including but not limited to systems such as ChatGPT, in various facets of daily life, it is crucial to address the issue of &#x02018;hallucination&#x02019; in AIgenerated content. This issue has been gaining prominence in the field of AI this year after the launch of ChatGPT in late November 2022.<sup><xref rid="ref2" ref-type="bibr">2</xref></sup></p><p>In AI, hallucination (also known as artificial hallucination or confabulation) occurs when responses given are not factually accurate or do not correspond to any real-world data.<sup><xref rid="ref2" ref-type="bibr">2</xref>,<xref rid="ref3" ref-type="bibr">3</xref></sup> It arises when a generative model is asked to produce text, image or other content without strict constraints or clear guidance. Generative models are trained on vast amounts of data and learn to generate content based on patterns they have observed in such data. When they generate content without specific guidelines, they may draw upon unusual or unexpected patterns they have learnt, leading to outputs that seem strange, surreal or disconnected from reality. This issue can occur owing to errors in AI systems&#x02019; training data, overfitting of AI systems and a lack of understanding of the underlying biomedical concepts. This becomes especially problematic when citing references or sources that are non-existent or inaccurate. While AI tools can be used for citation and reference management, many of the references generated are often fabricated.<sup><xref rid="ref4" ref-type="bibr">4</xref></sup> When using AI-generated content, authors need to exercise vigilance when AI references external sources. The output provided by AI often depends on the prompts given by its users, the version of model used and its ability to access the internet. It is not enough to simply rely on AI&#x02019;s ability to generate citations. Authors should take the time to find and read through the referenced articles, ensuring their accuracy and relevance. This additional step is essential to maintain the integrity of the academic and professional work produced with the assistance of AI. Unfortunately, there have been increasing instances of authors simply referencing articles without reading them or, worse, referencing non-existent articles. This phenomenon has been reported worldwide and poses a significant challenge to the credibility of scholarly publications and even official documentation.<sup><xref rid="ref3" ref-type="bibr">3</xref>,<xref rid="ref5" ref-type="bibr">5</xref>,<xref rid="ref6" ref-type="bibr">6</xref></sup></p><p>As Lee et al. are part of the editorial team of this esteemed journal, it is important to also emphasise the proactive role of editors and reviewers in addressing this issue.<sup><xref rid="ref2" ref-type="bibr">2</xref></sup> When evaluating manuscripts submitted to the journal, editors and reviewers must double-check not only the content but also the references provided. The easiest way to confirm the validity of a reference is by visiting the webpage link provided. In addition to manual verification, there are various software tools available that can help confirm whether AI has been used in the writing of an article, most of which have been included in plagiarism detection software. These tools can assist in identifying potential instances of hallucinations in AI-generated content. Such software can be valuable in the fight against misinformation and inaccuracies in academic and professional discourses.</p><p>Just as hallucinations in human cognition are acknowledged and addressed as part of World Mental Health Day every year on October 10, it is equally important to recognise and address the phenomenon of hallucinations in AI-generated content. Notably, AI hallucinations are not actual hallucinations in the psychological sense, as they are not experienced by AI tools themselves but rather a result of models&#x02019; pattern recognition and generation processes. Researchers and developers are continually working to improve AI systems and reduce the occurrence of such hallucinatory outputs while making them more useful and aligned with human intent. However, managing hallucinations in AI is akin to managing cognitive distortions in human thinking. Authors must take the responsibility to corroborate the facts provided by AI and ensure the accuracy of their references. As the use of AI continues to grow in various aspects of life, including academia and professional work, it is imperative to address the issue of hallucinations in Al-generated content. In the realm of clinical papers and research, although there exists a genuine requirement for a validated AI tool designed specifically for referencing and citation purposes, authors, editors and reviewers all have roles to play in mitigating this problem and upholding the standards of accuracy and integrity in publications. Human touch remains important when using AI, and AI-generated content should be used with caution. While advancements in AI hold promise for increased reliability, complete replacement of humans by AI remains improbable at this point in time.</p></sec></body><back><ack><title>Acknowledgments</title><p>In the same manner as in the article by Lee et al., ChatGPT (version 3.5; OpenAI) was used to generate the preliminary text of this article, which was then revised by the authors. The references were manually cited using referencing software and checked by the authors to avoid hallucinations by AI.</p></ack><sec id="sec2"><title>Author contributions</title><p>JJ conceptualised the study, reviewed the literature and wrote the manuscript. NAG and NSSD interpreted the relevant literature and revised the manuscript. All authors approved the final version of this manuscript.</p></sec><sec sec-type="COI-statement" id="sec3"><title>Conflicts of interest</title><p>None.</p></sec><sec id="sec4"><title>Funding</title><p>None.</p></sec><ref-list><title>References</title><ref id="ref1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>PY</given-names></name>
<name><surname>Salim</surname><given-names>H</given-names></name>
<name><surname>Abdullah</surname><given-names>A</given-names></name>
<name><surname>Teo</surname><given-names>CH</given-names></name></person-group><article-title>Use of ChatGPT in medical research and scientific writing.</article-title><source>Malays Fam Physician.</source><year>2023</year><volume>18</volume><fpage>58</fpage><pub-id pub-id-type="doi">10.51866/cm0006</pub-id><!--<pub-id pub-id-type="pmcid">PMC10560470</pub-id>--></element-citation></ref><ref id="ref2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ang</surname><given-names>T</given-names></name>
<name><surname>Choolani</surname><given-names>M</given-names></name>
<name><surname>See</surname><given-names>K</given-names></name>
<name><surname>Poh</surname><given-names>K</given-names></name></person-group><article-title>The rise of artificial intelligence: addressing the impact of large language models such as ChatGPT on scientific publications.</article-title><source>Singapore Med J.</source><year>2023</year><volume>64</volume><issue>4</issue><fpage>219</fpage><pub-id pub-id-type="doi">10.4103/singaporemedj.SMJ-2023-055</pub-id><!--<pub-id pub-id-type="pmcid">PMC10144457</pub-id>--></element-citation></ref><ref id="ref3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Athaluri</surname><given-names>SA</given-names></name>
<name><surname>Manthena</surname><given-names>SV</given-names></name>
<name><surname>Kesapragada</surname><given-names>VSRKM</given-names></name>
<name><surname>Yarlagadda</surname><given-names>V</given-names></name>
<name><surname>Dave</surname><given-names>T</given-names></name>
<name><surname>Duddumpudi</surname><given-names>RTS</given-names></name></person-group><article-title>Exploring the Boundaries of Reality: Investigating the Phenomenon of Artificial Intelligence Hallucination in Scientific Writing Through ChatGPT References.</article-title><source>Cureus.</source><year>2023</year><volume>15</volume><issue>4</issue><fpage>e37432</fpage><comment>Published 2023 Apr 11</comment><pub-id pub-id-type="doi">10.7759/cureus.37432</pub-id><!--<pub-id pub-id-type="pmcid">PMC10173677</pub-id>--></element-citation></ref><ref id="ref4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gravel</surname><given-names>J</given-names></name>
<name><surname>D&#x02019;Amours-Gravel</surname><given-names>M</given-names></name>
<name><surname>Osmanlliu</surname><given-names>E</given-names></name></person-group><article-title>Learning to fake it: limited responses and fabricated references provided by ChatGPT for medical questions.</article-title><source>Mayo Clin Proc Digit Heal.</source><year>2023</year><volume>1</volume><issue>3</issue><fpage>226</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1016/j.mcpdig.2023.05.004</pub-id></element-citation></ref><ref id="ref5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jansz</surname><given-names>J</given-names></name>
<name><surname>Tran</surname><given-names>HW</given-names></name>
<name><surname>Sweiss</surname><given-names>NJ</given-names></name></person-group><article-title>Pitfalls in the Diagnosis and Management of an Unusual Presentation of Clinically Amyopathic Dermatomyositis: A Case Report Written With the Assistance of ChatGPT.</article-title><source>Cureus.</source><year>2023</year><volume>15</volume><issue>7</issue><fpage>e41879</fpage><comment>Published 2023 Jul 14</comment><pub-id pub-id-type="doi">10.7759/cureus.41879</pub-id></element-citation></ref><ref id="ref6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milmo</surname><given-names>D</given-names></name>
<collab>agency.</collab></person-group><article-title>Two US lawyers fined for submitting fake court citations from ChatGPT.</article-title><source>The Guardian.</source><comment>Published</comment><month>June</month><day>23</day><year>2023</year><date-in-citation content-type="access-date"><month>October</month>
<day>1</day>
<year>2023</year></date-in-citation><ext-link xlink:href="https://www.theguardian.com/technology/2023/jun/23/two-us-lawyers-fined-submitting-fake-court-citations-chatgpt" ext-link-type="uri">https://www.theguardian.com/technology/2023/jun/23/two-us-lawyers-fined-submitting-fake-court-citations-chatgpt</ext-link></element-citation></ref></ref-list></back></article>
