<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Neurosci</journal-id><journal-title>BMC Neuroscience</journal-title><issn pub-type="epub">1471-2202</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">15268765</article-id><article-id pub-id-type="pmc">PMC503386</article-id><article-id pub-id-type="publisher-id">1471-2202-5-24</article-id><article-id pub-id-type="doi">10.1186/1471-2202-5-24</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Attentional influences on functional mapping of speech sounds in human auditory cortex</article-title></title-group><contrib-group><contrib id="A1" corresp="yes" contrib-type="author"><name><surname>Obleser</surname><given-names>Jonas</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I2">2</xref><email>jonas.obleser@uni-konstanz.de</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Elbert</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>thomas.elbert@uni-konstanz.de</email></contrib><contrib id="A3" contrib-type="author"><name><surname>Eulitz</surname><given-names>Carsten</given-names></name><xref ref-type="aff" rid="I2">2</xref><xref ref-type="aff" rid="I3">3</xref><email>ceulitz@ukaachen.de</email></contrib></contrib-group><aff id="I1"><label>1</label>Department of Psychology, University of Konstanz, Germany</aff><aff id="I2"><label>2</label>Department of Linguistics, University of Konstanz, Germany</aff><aff id="I3"><label>3</label>Department of Psychiatry and Psychotherapy, School of Medicine, University of Aachen, German</aff><pub-date pub-type="collection"><year>2004</year></pub-date><pub-date pub-type="epub"><day>21</day><month>7</month><year>2004</year></pub-date><volume>5</volume><fpage>24</fpage><lpage>24</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2202/5/24"/><history><date date-type="received"><day>17</day><month>2</month><year>2004</year></date><date date-type="accepted"><day>21</day><month>7</month><year>2004</year></date></history><copyright-statement>Copyright &#x000a9; 2004 Obleser et al; licensee BioMed Central Ltd.</copyright-statement><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p></license><abstract><sec><title>Background</title><p>The speech signal contains both information about phonological features such as place of articulation and non-phonological features such as speaker identity. These are different aspects of the 'what'-processing stream (speaker vs. speech content), and here we show that they can be further segregated as they may occur in parallel but within different neural substrates. Subjects listened to two different vowels, each spoken by two different speakers. During one block, they were asked to identify a given vowel irrespectively of the speaker (phonological categorization), while during the other block the speaker had to be identified irrespectively of the vowel (speaker categorization). Auditory evoked fields were recorded using 148-channel magnetoencephalography (MEG), and magnetic source imaging was obtained for 17 subjects.</p></sec><sec><title>Results</title><p>During phonological categorization, a vowel-dependent difference of N100m source location perpendicular to the main tonotopic gradient replicated previous findings. In speaker categorization, the relative mapping of vowels remained unchanged but sources were shifted towards more posterior and more superior locations.</p></sec><sec><title>Conclusions</title><p>These results imply that the N100m reflects the extraction of abstract invariants from the speech signal. This part of the processing is accomplished in auditory areas anterior to AI, which are part of the auditory 'what' system. This network seems to include spatially separable modules for identifying the phonological information and for associating it with a particular speaker that are activated in synchrony but within different regions, suggesting that the 'what' processing can be more adequately modeled by a stream of parallel stages. The relative activation of the parallel processing stages can be modulated by attentional or task demands.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>This study explores attentional modulation within the 'what'-stream of the auditory modality during phoneme processing. Knowledge of speech sound representation in the auditory domain is still sparse. However, parallels to the extensively studied visual modality and also to the somatosensory domain are becoming evident. For example, columnar mapping of several stimulus properties (as known from the visual cortex) has been revealed in human and animal research: acoustic parameters like spectral bandwidth, periodicity, stimulus intensity [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B2">2</xref>] or &#x02013; for human speech sounds &#x02013; distance between spectral peaks [<xref ref-type="bibr" rid="B3">3</xref>,<xref ref-type="bibr" rid="B4">4</xref>] appear to be mapped perpendicularly to the main cochleotopic gradient. Recently, a segregation of a ventral 'what' and a dorsal 'where' stream &#x02013; as long established in the visual system [<xref ref-type="bibr" rid="B5">5</xref>] &#x02013; has also been proposed for the auditory system. This conclusion was based on neuroanatomical and functional studies in macaques [<xref ref-type="bibr" rid="B6">6</xref>-<xref ref-type="bibr" rid="B8">8</xref>] and has been substantiated in humans [<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B10">10</xref>].</p><p>Given these parallels between sensory domains and the increasing preference for complex stimuli along the auditory central pathway, more complex topologies such as language-specific maps in auditory cortex are also plausible, and evidence for individually ordered mapping of speech sounds is growing [<xref ref-type="bibr" rid="B11">11</xref>-<xref ref-type="bibr" rid="B15">15</xref>] (for speech-specific vocalizations in animals see [<xref ref-type="bibr" rid="B8">8</xref>,<xref ref-type="bibr" rid="B16">16</xref>]). More specifically, data from our lab imply map dimensions along phonological features which build the basic components of speech sounds: In Obleser et al. [<xref ref-type="bibr" rid="B15">15</xref>], responses to DORSAL vowels (which are articulated with the back of the tongue and which exhibit a small distance between spectral peaks, i.e., small F<sub>1</sub>-F<sub>2 </sub>distance) were located more posterior in auditory association cortex than responses to CORONAL vowels (which are articulated with the tip of the tongue and which exhibit a large distance between spectral peaks, i.e., larger F<sub>1</sub>-F<sub>2 </sub>distance), and a topographical shift between these classes of vowels even when embedded in non-words has been reported [<xref ref-type="bibr" rid="B15">15</xref>,<xref ref-type="bibr" rid="B17">17</xref>].</p><p>Research has long been tackling the question of attention and attentional top-down modulation that may tune cortical neurons and with it functional maps in a context-specific manner: In the visual domain, a top-down influence on receptive fields of areas as basic as VI has been shown [<xref ref-type="bibr" rid="B18">18</xref>,<xref ref-type="bibr" rid="B19">19</xref>], and in the somatosensory domain Ergenzinger and colleagues reported that drastic changes in functional maps can be experimentally induced even on a thalamic level [<xref ref-type="bibr" rid="B20">20</xref>]. The thalamic homuncular representation of a monkey's hand becomes blurred and distorted when top-down modulation from somatosensory cortex is blocked neurochemically within the cortex. These results emphasize the possibility of attention-dependent modulation of maps, a topic exemplified in a somatosensory MEG mapping study by Braun and colleagues [<xref ref-type="bibr" rid="B21">21</xref>]: In a somatosensory stimulation with small brushes moving back and forth across the digit tips, subjects either attended the movement of single brushes on single digits and reported the movement direction or they attended and reported the global direction of all brushes on all five digits. Magnetic source imaging of the somatosensory evoked field revealed a typical homuncular representation of the single digits spread along the post central gyrus only in the condition where the focus of attention was on single digits rather than on the hand as a whole. In the latter condition, top-down attentional demands temporarily seemed to blur the single digit mapping.</p><p>For the developing field of speech sound mapping, top-down influences of attentional demands on functional organization at the different stages in the processing streams have not been sufficiently studied. Nevertheless, it becomes a central issue when the functional architecture of the effortless and robust perception of speech shall be understood. It is common to study speech perception either in passive oddball paradigms [<xref ref-type="bibr" rid="B22">22</xref>,<xref ref-type="bibr" rid="B23">23</xref>] where the subject's attention is deliberately forced to a movie or to reading a book, or in passive listening conditions where no attentional control is experimentally induced (e.g. [<xref ref-type="bibr" rid="B24">24</xref>,<xref ref-type="bibr" rid="B25">25</xref>]), or in active target detection tasks where the attention is commonly focused on the phonological content of the speech material [<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B15">15</xref>,<xref ref-type="bibr" rid="B26">26</xref>].</p><p>We analyzed the magnetic N100 (N100m) response to two vowels [o] and [&#x000f8;], both produced by a male and a female speaker. Subject's attention was either on the vowel or on the speaker difference, in a counterbalanced order. How would a controlled shift of attention from specific phonological features of speech to features of speaker identity affect the speech sound mapping in timing and topography of the brain response? Two concurrent outcomes are conceivable here: First, from the numerous parallels between the auditory and other sensory domains, one might expect a blurring of differences of the phonological map in auditory cortex when features such as the speaker identity rather than phonological differences are attended over minutes. Second, phonological processing could be the default process needed in all speech-listening situations and should therefore activate phonological feature maps irrespectively of attentional demands. We would then expect that the separate mapping of DORSAL and CORONAL vowels described previously [<xref ref-type="bibr" rid="B15">15</xref>] is unaffected by an attentional focus on speaker identity. However, a shift of activational patterns as an entity would reveal more about the staging of parallel processing in the flow of the 'what' stream.</p></sec><sec><title>Results</title><p>In 21 of 22 subjects, a clear waveform deflection around 100 ms post vowel onset was observed (Fig. <xref ref-type="fig" rid="F2">2</xref>) in all conditions over both hemispheres and sensor space parameters peak latency and amplitude were obtained. Satisfying and physiologically plausible dipole fits (see methods) in both hemispheres could be obtained in 17 subjects and were subjected to statistical analysis.</p><sec><title>N100m latency, amplitude and source strength</title><p>Analysis of the N100m root mean square (RMS) peak latency revealed foremost a main effect of vowel (F<sub>1,20 </sub>= 44.8, p &#x0003c; .0001, Fig. <xref ref-type="fig" rid="F2">2</xref>), whereby the DORSAL vowel [o] consistently elicited N100m peaks 5 ms later than the CORONAL vowel [&#x000f8;]. In sensor space, an enhancement of RMS peak amplitude for the [&#x000f8;] vowel by 10 fT (Fig. <xref ref-type="fig" rid="F2">2</xref>) almost attained significance (F<sub>1,20 </sub>= 4.12, p &#x0003c; .06). However, the effect was significant in source space that is not influenced by varying head-to-sensor positions: The [&#x000f8;] dipole source strength, an estimate for the amount of massed neuronal activity, was larger for the [&#x000f8;] vowel than for the [o] by 25 % or 6 nAm (F<sub>1,16 </sub>= 9.36, p &#x0003c; .01). No hemispheric differences in signal power between vowel categories or tasks were apparent.</p></sec><sec><title>N100m source location and orientation</title><p>In agreement with previous findings with a more comprehensive set of vowels [<xref ref-type="bibr" rid="B15">15</xref>], the vowel categories [o] and [&#x000f8;] elicited statistically different centers of activity along the anterior-posterior axis (F<sub>1,16 </sub>= 7.73, p &#x0003c; .01), that is, the auditory processing in the DORSAL vowel [o] was reflected by a more posterior ECD location (Fig. <xref ref-type="fig" rid="F3">3</xref>). A difference in source configuration was also evident from a more superior position of the [o] source (F<sub>1,16 </sub>= 12.28, p &#x0003c; .01), a more vertical orientation (F<sub>1,16 </sub>= 5.81, p &#x0003c; .05) than the [&#x000f8;] source, and from an angular difference between the two vowel categories in the sagittal plane (i.e. the [o] source was located more posterior and inferior, F<sub>1,16 </sub>= 10.91, p &#x0003c; .01) and in the axial plane (i.e. the [o] source was also located more posterior and lateral, F<sub>1,16 </sub>= 6.82, p &#x0003c; .05, relative to the [&#x000f8;] source). None of these effects showed an interaction with hemisphere, but data gained further validity as the right-hemispheric sources were all located more posterior (F<sub>1,16 </sub>= 8.88, p &#x0003c; .01), more inferior (F<sub>1,16 </sub>= 4.27, p &#x0003c; .06) and were tilted more vertically (F<sub>1,16 </sub>= 14.29, p &#x0003c; .01) than their left-hemispheric counterpart. Such a difference is to be expected from previously reported N100 asymmetries between cerebral hemispheres [<xref ref-type="bibr" rid="B27">27</xref>-<xref ref-type="bibr" rid="B30">30</xref>].</p><p>The relative mapping of phonological features of the speech signal [<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B15">15</xref>] was not affected by the task-induced shifts of attention. However, shifts of subjects' attentional focus from phonological categorization to identification of the speaker's voice shifted vowel sources as a whole to more posterior and superior locations within the supratemporal plane. Statistically, the speaker categorization task produced more superior (F<sub>1,16 </sub>= 4.72, p &#x0003c; .05) and marginally more posterior (F<sub>1,16 </sub>= 3.36, p &#x0003c; .10) ECD locations, which was also evident by an angular displacement in the sagittal plane (F<sub>1,16 </sub>= 4.6, p &#x0003c; .05). The effect seemed to be driven by changes in the left hemisphere but the task &#x000d7; hemisphere interaction never attained significance (all F &#x0003c; 1).</p><p>When brain responses were analyzed separately for stimuli spoken by male and female speaker, which yielded satisfying dipole solutions only in 12 subjects, the most striking finding was a consistent speaker &#x000d7; task interaction of the dipole location in both the sagittal plane (F<sub>1,11 </sub>= 10.83, p &#x0003c; .01) and the axial plane (F<sub>1,11 </sub>= 7.16, p &#x0003c; .03). That is, subjects' attentional focus slightly affected the relative displacement of male and female voice-evoked brain responses: In both the sagittal plane and the axial plane, a significant 4&#x000b0; difference emerged in the phonological categorization task (both p &#x0003c; .05), which vanished in the speaker categorization task. In contrast, as reported above, no such task influence was evident in the relative position of vowel-evoked brain responses.</p></sec><sec><title>Performance</title><p>Overall target detection rate was 94.1 %, false alarms occurred in 5.5% of all trials. Responses of the 17 subjects whose brain responses were subjected to magnetic source imaging were analyzed in detail: The phonological categorization task (93.2 &#x000b1; 3.0 % correct, 4.9 &#x000b1; 2.2 % false alarms, M &#x000b1; SEM) and the speaker categorization task (95.0 &#x000b1; 2.9 % correct, 6.2 &#x000b1; 3.2 % false alarms) did not differ significantly (one-way repeated measures ANOVAs, all F &#x0003c; 1).</p></sec></sec><sec><title>Discussion</title><p>This study was set up to explore potential influences of the attentional focus on the mapping of speech sounds within the auditory cortex. With subject's attention either on the phonological differences or on the speaker difference between vowel stimuli, we mapped the auditory evoked N100m and localized its sources that fitted well with a single dipole per hemisphere. All responses were located in the perisylvian region. Furthermore, the relative distribution of sources indicated an interesting pattern. As hypothesized and expected from previous studies, the fundamental location difference between the sources of the DORSAL vowel [o] source and the CORONAL vowel [&#x000f8;] [<xref ref-type="bibr" rid="B15">15</xref>,<xref ref-type="bibr" rid="B17">17</xref>] could be replicated under both attentional conditions. In contrast, the corresponding difference between speaker-dependent sources was subject to task influences.</p><p>That is, a shift of subjects' attention to a non-phonological acoustic feature, the speaker identity, did not blur the spatial segregation within the speech sound map. In contrast, the [&#x000f8;] and [o] generators were slightly displaced towards more posterior and more superior locations when subjects focused on speaker identity.</p><p>In most situations, a listener may automatically extract the phonological invariants from the speech signal in order to access lexical information, for example the meaning of the information inherent in speech. Speaker-dependent features such as pitch and periodicity should not play a crucial role in this phonological decoding process. This is what we mimicked by asking our subjects to detect a certain vowel in a stream of varying speech sounds. However, in cocktail-party-like situations there is the additional demand to attend acoustic properties of certain speech streams or speakers, and we implemented it by asking our subjects to detect a certain voice in a stream of varying speakers. Speaker identification comprises an important but not necessarily orthogonal process to phonological decoding in speech perception: areas in the upper bank of the superior temporal sulcus (STS) have been identified previously [<xref ref-type="bibr" rid="B31">31</xref>] to be voice-selective (as opposed to other environmental sounds), and in many situations the selective tracking of one voice amongst others is a prerequisite for decoding the phonological content of this speaker's utterances. The displacement of dipolar sources seen here may mirror the involvement of additional cortical areas, such as the voice-specialized part in the STS [<xref ref-type="bibr" rid="B31">31</xref>] or pitch-specialized areas in the primary auditory cortex. An additional STS activation would most likely elicit an inferior shift of the dipole sources during speaker categorization. However, a shift into the opposite direction was obtained. This might indicate that the contribution of the voice-specialized part of the STS around 100 ms post-stimulus onset is small compared to other additional cortical areas, such as pitch-specialized areas in the primary auditory cortex. It is now well-established that a finegrained analysis of the speech signal takes place mainly in anterior parts of the supratemporal gyrus [<xref ref-type="bibr" rid="B17">17</xref>,<xref ref-type="bibr" rid="B32">32</xref>-<xref ref-type="bibr" rid="B34">34</xref>], thereby anterior of primary auditory areas. Consequently, the activity shift towards more posterior sites we observed in the speaker categorization task strongly argues for an additional involvement of these primary auditory areas. Unfortunately, we cannot dissociate speaker identification processes from pitch processing in the current study. However, pitch differences are among the primary cues dissociating male and female voices, and a clear involvement of auditory core areas in pitch processing has been shown in a recent MEG study focusing on pitch detection mechanisms [<xref ref-type="bibr" rid="B35">35</xref>].</p></sec><sec><title>Conclusions</title><p>Data presented here suggest that the systematic mapping of speech sounds within the auditory cortex is robust under changing attentional demands and not tied to phonological awareness. However, the general shift of activity when a non-phonological speaker categorization must be accomplished shows that speech sound representations are modulated in their locations in a context-dependent manner. Situational demands obviously influence the differential but time-synchronous involvement of specialized neuronal assemblies that contribute to speech sound decoding in a top-down fashion. Hence, the spectrally high-resolving analysis of the incoming speech stream is performed at the same time but in different locations, i.e. in a different mix of cell assemblies than the analysis of speaker-dependent features (such as pitch, periodicity, or other features inherent to voice quality).</p><p>Further spatially high-resolution brain imaging studies are needed to quantify as to which extent voice-selective areas in the upper bank of the STS [<xref ref-type="bibr" rid="B31">31</xref>] become involved when speaker categorization is accomplished. For the time being, this study increases our understanding of speech sound processing, as it replicates previous findings of an orderly mapping of phonological vowel features and as it shows that changing attentional foci affect the absolute but not the relative distribution of vowel-evoked activity within the auditory cortex.</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>Subjects</title><p>22 subjects (11 females, mean age 24.3 &#x000b1; 4 years, M &#x000b1; SD) participated in the procedure. All subjects were monolingual native speakers of German. Only right-handers as ascertained by the Edinburgh Handedness Questionnaire [<xref ref-type="bibr" rid="B36">36</xref>] were included. Subjects gave written informed consent and were paid &#x020ac;10 for their participation.</p></sec><sec><title>Experimental design</title><p>In an auditory target detection task, subjects listened to randomized sequences of four German natural vowel exemplars: The DORSAL rounded vowel [o] in two exemplars, in one spoken by a male voice and in the other by a female voice, and the CORONAL rounded vowel [&#x000f8;], also produced by both voices (Fig. <xref ref-type="fig" rid="F1">1</xref>). 200 ms long vowels free of formant transitions were cut out of spoken words, digitized with a 10 kHz sampling rate and faded with 50 ms Gaussian on- and offset ramps. Table <xref ref-type="table" rid="T1">1</xref> summarizes exact pitch and formant frequencies of the four exemplars. Prior to the measurement, individual hearing thresholds were determined for both ears and all four vowel exemplars. Stimuli were presented binaurally with at least 50 dB SL (respective to the vowel exemplar which showed the weakest sensation level, if any differences between exemplars occurred) via a non-magnetic echo-free stimulus delivery system with almost linear frequency characteristic in the critical range of 200&#x02013;4000 Hz.</p><p>In a test sequence, subjects repeated vowels aloud and recognized all stimuli correctly, i.e. they distinguished between both vowel categories and voices without difficulty. Binaural loudness was slightly re-adjusted where necessary to ensure perception in the head midline.</p><p>In the actual measurement, vowel exemplars were presented in two randomized sequences with equal probability and a randomized stimulus onset asynchrony of 1.6 &#x02013; 2 s. All subjects performed &#x02013; in a counterbalanced order &#x02013; two different tasks during these two sequences: In a task A (hereafter called <italic>phonological categorization</italic>), subjects had to press a button with their right index finger whenever a given vowel ([o] or [&#x000f8;], counterbalanced across subjects) occurred, irrespective of the speaking voice. In a task B (hereafter called <italic>speaker categorization</italic>), subjects had to press a button whenever a given voice (the male or the female voice, counterbalanced across subjects) uttered a vowel, irrespective of the uttered vowel category. Fig.<xref ref-type="fig" rid="F1">1</xref> (lower panel) which clarifies and visualizes the task.</p><p>That is, in the phonological categorization task, subject's attention was focused on a categorical distinction between speech sounds, [o] or [&#x000f8;], which closely resembles the tasks applied in most brain imaging studies testing active speech sound processing (e.g. [<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B15">15</xref>,<xref ref-type="bibr" rid="B37">37</xref>]) &#x02013; a process ubiquitously taking place when decoding running speech. In contrast, the speaker categorization task was intended to shift subject's attention to more general and more basic acoustic properties of the material [<xref ref-type="bibr" rid="B31">31</xref>] presented to accomplish speaker distinction.</p></sec><sec><title>Data reduction and statistical analyses</title><p>Data acquisition and analysis, including source modeling, closely followed the procedure described in [<xref ref-type="bibr" rid="B15">15</xref>]: Auditory magnetic fields were recorded using a whole head neuromagnetometer (MAGNES 2500, 4D Neuroimaging, San Diego) in a magnetically shielded room (Vaccumschmelze, Hanau, Germany). Epochs of 800 ms duration (including a 200 ms pre-trigger baseline) were recorded with a bandwidth from 0.1 to 200 Hz and a 687.17 Hz sampling rate. If the peak-to-peak amplitude exceeded 3.5 pT in one of the channels or the co-registered EOG signal was larger than 100 &#x003bc;V, epochs were rejected. Button-presses did not affect the auditory evoked field topography in the N100m time range.</p><p>We analyzed up to 150 artifact-free vowel responses that remained for both vowel categories [o] and [&#x000f8;] after off-line noise correction, and averaged them separately for vowel category but across speaker voice. Splitting up vowel conditions into male and female speaker sub-conditions was not possible due to a resulting small number of averages. However, we also performed separate averages and analyses of male and female speaker across vowel categories. In any case, the resulting averages thus contained brain responses to two acoustically variant exemplars which makes results more comparable to our previous studies [<xref ref-type="bibr" rid="B15">15</xref>,<xref ref-type="bibr" rid="B17">17</xref>]. A 20 Hz lowpass filter (Butterworth 12 dB/oct, zero phase shift) was subsequently applied to the averages.</p><p>The N100m component was defined as the prominent waveform deflection in the time range between 90 and 160 ms (Fig. <xref ref-type="fig" rid="F2">2</xref>). Isofield contour plots of the magnetic field distribution were visually inspected to ensure that N100m and not P50 m or P200 m were analyzed.</p><p>N100m peak latency was defined as the sampling point in this latency range by which the first derivative of the Root Mean Square (RMS) amplitude reached its minimum and second derivative was smaller than zero. RMS was calculated across 34 magnetometer channels selected to include the field extrema over the left and the right hemisphere, respectively.</p><p>Prior to statistical analyses, all brain response latencies were corrected for a constant sound conductance delay of 19 ms in the delivery system. Using the same sets of channels, an equivalent current dipole (ECD) in a spherical volume conductor (fitted to the shape of the regional head surface) was modeled at every sampling point separately for the left and the right hemisphere [<xref ref-type="bibr" rid="B38">38</xref>]. The N100m source parameters were determined as the median of 5 successive ECD solutions in the rising slope of the N100m. The resulting ECD solution represents the center of gravity for the massed and synchronized neuronal activity. To be included in this calculation, single ECD solutions had to meet the following criteria: (i) Goodness of fit greater than .90, (ii) ECD location larger than 1.5 cm in medial-lateral direction from the center of the brain and 3&#x02013;8 cm in superior direction, measured from the connecting line of the pre-auricular points. Statistical analysis of dependent variables N100m peak latency, amplitude and N100m source generator strength, location and orientation focused on 2 &#x000d7; 2 &#x000d7; 2 repeated measures analysis of variance with repeated factors hemisphere (left vs. right), vowel ([o] vs. [&#x000f8;]) and task (attend phonology vs. attend speaker).</p><p>As source location displacements do not appear exactly and exclusively along the Cartesian axes of the source space (cf. [<xref ref-type="bibr" rid="B21">21</xref>]), we additionally calculated differences in the polar angle &#x003a6; and the azimuth angle &#x003b8; which here describe angular displacements in the sagittal and the axial plane, respectively.</p></sec></sec><sec><title>Authors' contributions</title><p>J.O., T.E. and C.E. conceived the experiment and drafted the manuscript. J.O. and C.E. prepared the exact experimental setup. J.O. supervised data acquisition, and performed all data and statistical analyses. All authors read and approved the final manuscript.</p></sec></body><back><ack><sec><title>Acknowledgments</title><p>Research was supported by German Science Foundation. Sonja Schumacher and Barbara Awiszus helped collect and analyze the data. We thank three anonymous reviewers for their helpful comments on the manuscript.</p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schreiner</surname><given-names>CE</given-names></name><name><surname>Read</surname><given-names>HL</given-names></name><name><surname>Sutter</surname><given-names>ML</given-names></name></person-group><article-title>Modular organization of frequency integration in primary auditory cortex</article-title><source>Annu Rev Neurosci</source><year>2000</year><volume>23</volume><fpage>501</fpage><lpage>529</lpage><pub-id pub-id-type="pmid">10845073</pub-id><pub-id pub-id-type="doi">10.1146/annurev.neuro.23.1.501</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Langner</surname><given-names>G</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name><name><surname>Heil</surname><given-names>P</given-names></name><name><surname>Schulze</surname><given-names>H</given-names></name></person-group><article-title>Frequency and periodicity are represented in orthogonal maps in the human auditory cortex: evidence from magnetoencephalography</article-title><source>J Comp Physiol [A]</source><year>1997</year><volume>181</volume><fpage>665</fpage><lpage>676</lpage><pub-id pub-id-type="pmid">9449825</pub-id><pub-id pub-id-type="doi">10.1007/s003590050148</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ohl</surname><given-names>FW</given-names></name><name><surname>Scheich</surname><given-names>H</given-names></name></person-group><article-title>Orderly cortical representation of vowels based on formant interaction</article-title><source>Proc Natl Acad Sci U S A</source><year>1997</year><volume>94</volume><fpage>9440</fpage><lpage>9444</lpage><pub-id pub-id-type="pmid">9256501</pub-id><pub-id pub-id-type="doi">10.1073/pnas.94.17.9440</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Diesch</surname><given-names>E</given-names></name><name><surname>Luce</surname><given-names>T</given-names></name></person-group><article-title>Topographic and temporal indices of vowel spectral envelope extraction in the human auditory cortex</article-title><source>J Cogn Neurosci</source><year>2000</year><volume>12</volume><fpage>878</fpage><lpage>893</lpage><pub-id pub-id-type="pmid">11054929</pub-id><pub-id pub-id-type="doi">10.1162/089892900562480</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name><name><surname>Macko</surname><given-names>KA</given-names></name></person-group><article-title>Object vision and spatial vision: Two cortical pathways</article-title><source>Trends Neurosci</source><year>1983</year><volume>6</volume><fpage>414</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(83)90201-1</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kaas</surname><given-names>JH</given-names></name><name><surname>Hackett</surname><given-names>TA</given-names></name></person-group><article-title>'What' and 'where' processing in auditory cortex</article-title><source>Nat Neurosci</source><year>1999</year><volume>2</volume><fpage>1045</fpage><lpage>1047</lpage><pub-id pub-id-type="pmid">10570476</pub-id><pub-id pub-id-type="doi">10.1038/15967</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname><given-names>JP</given-names></name></person-group><article-title>Cortical processing of complex sounds</article-title><source>Curr Opin Neurobiol</source><year>1998</year><volume>8</volume><fpage>516</fpage><lpage>521</lpage><pub-id pub-id-type="pmid">9751652</pub-id><pub-id pub-id-type="doi">10.1016/S0959-4388(98)80040-8</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname><given-names>JP</given-names></name><name><surname>Tian</surname><given-names>B</given-names></name></person-group><article-title>Mechanisms and streams for processing of "what" and "where" in auditory cortex</article-title><source>Proc Natl Acad Sci U S A</source><year>2000</year><volume>97</volume><fpage>11800</fpage><lpage>11806</lpage><pub-id pub-id-type="pmid">11050212</pub-id><pub-id pub-id-type="doi">10.1073/pnas.97.22.11800</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Alain</surname><given-names>C</given-names></name><name><surname>Arnott</surname><given-names>SR</given-names></name><name><surname>Hevenor</surname><given-names>S</given-names></name><name><surname>Graham</surname><given-names>S</given-names></name><name><surname>Grady</surname><given-names>CL</given-names></name></person-group><article-title>"What" and "where" in the human auditory system</article-title><source>Proc Natl Acad Sci U S A</source><year>2001</year><volume>98</volume><fpage>12301</fpage><lpage>12306</lpage><pub-id pub-id-type="pmid">11572938</pub-id><pub-id pub-id-type="doi">10.1073/pnas.211209098</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>JD</given-names></name><name><surname>Zielinski</surname><given-names>BA</given-names></name><name><surname>Green</surname><given-names>GG</given-names></name><name><surname>Rauschecker</surname><given-names>JP</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name></person-group><article-title>Perception of sound-source motion by the human brain</article-title><source>Neuron</source><year>2002</year><volume>34</volume><fpage>139</fpage><lpage>148</lpage><pub-id pub-id-type="pmid">11931748</pub-id><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00637-2</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kohonen</surname><given-names>T</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name></person-group><article-title>Where the abstract feature maps of the brain might come from</article-title><source>Trends Neurosci</source><year>1999</year><volume>22</volume><fpage>135</fpage><lpage>139</lpage><pub-id pub-id-type="pmid">10199639</pub-id><pub-id pub-id-type="doi">10.1016/S0166-2236(98)01342-3</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Diesch</surname><given-names>E</given-names></name><name><surname>Eulitz</surname><given-names>C</given-names></name><name><surname>Hampson</surname><given-names>S</given-names></name><name><surname>Ross</surname><given-names>B</given-names></name></person-group><article-title>The neurotopography of vowels as mirrored by evoked magnetic field measurements</article-title><source>Brain Lang</source><year>1996</year><volume>53</volume><fpage>143</fpage><lpage>168</lpage><pub-id pub-id-type="pmid">8726531</pub-id><pub-id pub-id-type="doi">10.1006/brln.1996.0042</pub-id></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zielinski</surname><given-names>BA</given-names></name><name><surname>Rauschecker</surname><given-names>JP</given-names></name></person-group><article-title>Phoneme-specific functional maps in the human superior temporal cortex</article-title><source>Society of Neuroscience Abstracts</source><year>2000</year><volume>26</volume><fpage>1969</fpage></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Elbert</surname><given-names>T</given-names></name><name><surname>Lahiri</surname><given-names>A</given-names></name><name><surname>Eulitz</surname><given-names>C</given-names></name></person-group><article-title>Cortical representation of vowels reflects acoustic dissimilarity determined by formant frequencies</article-title><source>Brain Res Cogn Brain Res</source><year>2003</year><volume>15</volume><fpage>207</fpage><lpage>213</lpage><pub-id pub-id-type="pmid">12527095</pub-id><pub-id pub-id-type="doi">10.1016/S0926-6410(02)00193-3</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Lahiri</surname><given-names>A</given-names></name><name><surname>Eulitz</surname><given-names>C</given-names></name></person-group><article-title>Magnetic Brain Response Mirrors Extraction of Phonological Features from Spoken Vowels</article-title><source>J Cogn Neurosci</source><year>2004</year><volume>16</volume><fpage>31</fpage><lpage>39</lpage><pub-id pub-id-type="pmid">15006034</pub-id><pub-id pub-id-type="doi">10.1162/089892904322755539</pub-id></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name><name><surname>Beitel</surname><given-names>R</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><article-title>Representation of a species-specific vocalization in the primary auditory cortex of the common marmoset: temporal and spectral characteristics</article-title><source>J Neurophysiol</source><year>1995</year><volume>74</volume><fpage>2685</fpage><lpage>2706</lpage><pub-id pub-id-type="pmid">8747224</pub-id></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Lahiri</surname><given-names>A</given-names></name><name><surname>Eulitz</surname><given-names>C</given-names></name></person-group><article-title>Auditory Evoked Magnetic Field Codes Place of Articulation in Timing and Topography around 100 ms Post Syllable Onset</article-title><source>Neuroimage</source><year>2003</year><volume>20</volume><fpage>1839</fpage><lpage>1847</lpage><pub-id pub-id-type="pmid">14642493</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.07.019</pub-id></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname><given-names>S</given-names></name></person-group><article-title>Neural correlates of attention in primate visual cortex</article-title><source>Trends Neurosci</source><year>2001</year><volume>24</volume><fpage>295</fpage><lpage>300</lpage><pub-id pub-id-type="pmid">11311383</pub-id><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01814-2</pub-id></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>AK</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name></person-group><article-title>Dynamic predictions: oscillations and synchrony in top-down processing</article-title><source>Nat Rev Neurosci</source><year>2001</year><volume>2</volume><fpage>704</fpage><lpage>716</lpage><pub-id pub-id-type="pmid">11584308</pub-id><pub-id pub-id-type="doi">10.1038/35094565</pub-id></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ergenzinger</surname><given-names>ER</given-names></name><name><surname>Glasier</surname><given-names>MM</given-names></name><name><surname>Hahm</surname><given-names>JO</given-names></name><name><surname>Pons</surname><given-names>TP</given-names></name></person-group><article-title>Cortically induced thalamic plasticity in the primate somatosensory system</article-title><source>Nat Neurosci</source><year>1998</year><volume>1</volume><fpage>226</fpage><lpage>229</lpage><pub-id pub-id-type="pmid">10195147</pub-id><pub-id pub-id-type="doi">10.1038/673</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Braun</surname><given-names>C</given-names></name><name><surname>Haug</surname><given-names>M</given-names></name><name><surname>Wiech</surname><given-names>K</given-names></name><name><surname>Birbaumer</surname><given-names>N</given-names></name><name><surname>Elbert</surname><given-names>T</given-names></name><name><surname>Roberts</surname><given-names>LE</given-names></name></person-group><article-title>Functional Organization of Primary Somatosensory Cortex Depends on the Focus of Attention</article-title><source>Neuroimage</source><year>2002</year><volume>17</volume><fpage>1451</fpage><lpage>1458</lpage><pub-id pub-id-type="pmid">12414284</pub-id><pub-id pub-id-type="doi">10.1006/nimg.2002.1277</pub-id></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Naatanen</surname><given-names>R</given-names></name></person-group><article-title>The perception of speech sounds by the human brain as reflected by the mismatch negativity (MMN) and its magnetic equivalent (MMNm)</article-title><source>Psychophysiology</source><year>2001</year><volume>38</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="pmid">11321610</pub-id><pub-id pub-id-type="doi">10.1017/S0048577201000208</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kraus</surname><given-names>N</given-names></name><name><surname>Cheour</surname><given-names>M</given-names></name></person-group><article-title>Speech sound representation in the brain</article-title><source>Audiol Neurootol</source><year>2000</year><volume>5</volume><fpage>140</fpage><lpage>150</lpage><pub-id pub-id-type="pmid">10859409</pub-id><pub-id pub-id-type="doi">10.1159/000013876</pub-id></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gage</surname><given-names>NM</given-names></name><name><surname>Roberts</surname><given-names>TP</given-names></name><name><surname>Hickok</surname><given-names>G</given-names></name></person-group><article-title>Hemispheric asymmetries in auditory evoked neuromagnetic fields in response to place of articulation contrasts</article-title><source>Brain Res Cogn Brain Res</source><year>2002</year><volume>14</volume><fpage>303</fpage><lpage>306</lpage><pub-id pub-id-type="pmid">12067704</pub-id><pub-id pub-id-type="doi">10.1016/S0926-6410(02)00128-3</pub-id></citation></ref><ref id="B25"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sanders</surname><given-names>LD</given-names></name><name><surname>Newport</surname><given-names>EL</given-names></name><name><surname>Neville</surname><given-names>HJ</given-names></name></person-group><article-title>Segmenting nonsense: an event-related potential index of perceived onsets in continuous speech</article-title><source>Nat Neurosci</source><year>2002</year><volume>5</volume><fpage>700</fpage><lpage>703</lpage><pub-id pub-id-type="pmid">12068301</pub-id><pub-id pub-id-type="doi">10.1038/nn873</pub-id></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Yellin</surname><given-names>E</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Roberts</surname><given-names>TP</given-names></name><name><surname>Rowley</surname><given-names>HA</given-names></name><name><surname>Wexler</surname><given-names>K</given-names></name><etal></etal></person-group><article-title>Task-induced asymmetry of the auditory evoked M100 neuromagnetic field elicited by speech sounds</article-title><source>Brain Res Cogn Brain Res</source><year>1996</year><volume>4</volume><fpage>231</fpage><lpage>242</lpage><pub-id pub-id-type="pmid">8957564</pub-id><pub-id pub-id-type="doi">10.1016/S0926-6410(96)00643-X</pub-id></citation></ref><ref id="B27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Eulitz</surname><given-names>C</given-names></name><name><surname>Diesch</surname><given-names>E</given-names></name><name><surname>Pantev</surname><given-names>C</given-names></name><name><surname>Hampson</surname><given-names>S</given-names></name><name><surname>Elbert</surname><given-names>T</given-names></name></person-group><article-title>Magnetic and electric brain activity evoked by the processing of tone and vowel stimuli</article-title><source>J Neurosci</source><year>1995</year><volume>15</volume><fpage>2748</fpage><lpage>2755</lpage><pub-id pub-id-type="pmid">7722626</pub-id></citation></ref><ref id="B28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rockstroh</surname><given-names>B</given-names></name><name><surname>Kissler</surname><given-names>J</given-names></name><name><surname>Mohr</surname><given-names>B</given-names></name><name><surname>Eulitz</surname><given-names>C</given-names></name><name><surname>Lommen</surname><given-names>U</given-names></name><name><surname>Wienbruch</surname><given-names>C</given-names></name><etal></etal></person-group><article-title>Altered hemispheric asymmetry of auditory magnetic fields to tones and syllables in schizophrenia</article-title><source>Biol Psychiatry</source><year>2001</year><volume>49</volume><fpage>694</fpage><lpage>703</lpage><pub-id pub-id-type="pmid">11313037</pub-id><pub-id pub-id-type="doi">10.1016/S0006-3223(00)01023-4</pub-id></citation></ref><ref id="B29"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ohtomo</surname><given-names>S</given-names></name><name><surname>Nakasato</surname><given-names>N</given-names></name><name><surname>Kanno</surname><given-names>A</given-names></name><name><surname>Hatanaka</surname><given-names>K</given-names></name><name><surname>Shirane</surname><given-names>R</given-names></name><name><surname>Mizoi</surname><given-names>K</given-names></name><etal></etal></person-group><article-title>Hemispheric asymmetry of the auditory evoked N100m response in relation to the crossing point between the central sulcus and Sylvian fissure</article-title><source>Electroencephalogr Clin Neurophysiol</source><year>1998</year><volume>108</volume><fpage>219</fpage><lpage>225</lpage><pub-id pub-id-type="pmid">9607510</pub-id><pub-id pub-id-type="doi">10.1016/S0168-5597(97)00065-8</pub-id></citation></ref><ref id="B30"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Teale</surname><given-names>P</given-names></name><name><surname>Sheeder</surname><given-names>J</given-names></name><name><surname>Rojas</surname><given-names>DC</given-names></name><name><surname>Walker</surname><given-names>J</given-names></name><name><surname>Reite</surname><given-names>M</given-names></name></person-group><article-title>Sequential source of the M100 exhibits inter-hemispheric asymmetry</article-title><source>Neuroreport</source><year>1998</year><volume>9</volume><fpage>2647</fpage><lpage>2652</lpage><pub-id pub-id-type="pmid">9721949</pub-id></citation></ref><ref id="B31"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Lafaille</surname><given-names>P</given-names></name><name><surname>Ahad</surname><given-names>P</given-names></name><name><surname>Pike</surname><given-names>B</given-names></name></person-group><article-title>Voice-selective areas in human auditory cortex</article-title><source>Nature</source><year>2000</year><volume>403</volume><fpage>309</fpage><lpage>312</lpage><pub-id pub-id-type="pmid">10659849</pub-id><pub-id pub-id-type="doi">10.1038/35002078</pub-id></citation></ref><ref id="B32"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Hertz-Pannier</surname><given-names>L</given-names></name></person-group><article-title>Functional Neuroimaging of Speech Perception in Infants</article-title><source>Science</source><year>2002</year><volume>298</volume><fpage>2013</fpage><lpage>2015</lpage><pub-id pub-id-type="pmid">12471265</pub-id><pub-id pub-id-type="doi">10.1126/science.1077066</pub-id></citation></ref><ref id="B33"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>SK</given-names></name><name><surname>Johnsrude</surname><given-names>IS</given-names></name></person-group><article-title>The neuroanatomical and functional organization of speech perception</article-title><source>Trends Neurosci</source><year>2003</year><volume>26</volume><fpage>100</fpage><lpage>107</lpage><pub-id pub-id-type="pmid">12536133</pub-id><pub-id pub-id-type="doi">10.1016/S0166-2236(02)00037-1</pub-id></citation></ref><ref id="B34"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Eulitz</surname><given-names>C</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Lahiri</surname><given-names>A</given-names></name></person-group><article-title>Intra-subject replication of brain magnetic activity during the processing of speech sounds</article-title><source>Brain Res Cogn Brain Res</source><year>2004</year><volume>19</volume><fpage>82</fpage><lpage>91</lpage><pub-id pub-id-type="pmid">14972361</pub-id><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2003.11.004</pub-id></citation></ref><ref id="B35"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Krumbholz</surname><given-names>K</given-names></name><name><surname>Patterson</surname><given-names>RD</given-names></name><name><surname>Seither-Preisler</surname><given-names>A</given-names></name><name><surname>Lammertmann</surname><given-names>C</given-names></name><name><surname>Lutkenhoner</surname><given-names>B</given-names></name></person-group><article-title>Neuromagnetic evidence for a pitch processing center in Heschl's gyrus</article-title><source>Cereb Cortex</source><year>2003</year><volume>13</volume><fpage>765</fpage><lpage>772</lpage><pub-id pub-id-type="pmid">12816892</pub-id><pub-id pub-id-type="doi">10.1093/cercor/13.7.765</pub-id></citation></ref><ref id="B36"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Oldfield</surname><given-names>RC</given-names></name></person-group><article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title><source>Neuropsychologia</source><year>1971</year><volume>9</volume><fpage>97</fpage><lpage>113</lpage><pub-id pub-id-type="pmid">5146491</pub-id><pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id></citation></ref><ref id="B37"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Yellin</surname><given-names>E</given-names></name><name><surname>Rowley</surname><given-names>HA</given-names></name><name><surname>Roberts</surname><given-names>TP</given-names></name><name><surname>Marantz</surname><given-names>A</given-names></name></person-group><article-title>Processing of vowels in supratemporal auditory cortex</article-title><source>Neurosci Lett</source><year>1997</year><volume>221</volume><fpage>145</fpage><lpage>148</lpage><pub-id pub-id-type="pmid">9121685</pub-id><pub-id pub-id-type="doi">10.1016/S0304-3940(97)13325-0</pub-id></citation></ref><ref id="B38"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sarvas</surname><given-names>J</given-names></name></person-group><article-title>Basic mathematical and electromagnetic concepts of the biomagnetic inverse problem</article-title><source>Phys Med Biol</source><year>1987</year><volume>32</volume><fpage>11</fpage><lpage>22</lpage><pub-id pub-id-type="pmid">3823129</pub-id><pub-id pub-id-type="doi">10.1088/0031-9155/32/1/004</pub-id></citation></ref></ref-list><sec sec-type="display-objects"><title>Figures and Tables</title><fig position="float" id="F1"><label>Figure 1</label><caption><p>Upper panel: Illustration of the F1-F2 formant space for the vowel tokens used. Lower panel: Illustration of the stimulation paradigm and of the two tasks which all subjects performed. Attention was either focused on vowel category changes (Task A) or on changes in the voice speaking (Task B). Arrows indicate required button presses.</p></caption><graphic xlink:href="1471-2202-5-24-1"/></fig><fig position="float" id="F2"><label>Figure 2</label><caption><p>Grand average (N = 21) of root mean squared amplitudes for all conditions over time separately for left (upper panel) and right hemisphere (lower panel). N100m is clearly the most prominent waveform deflection, and the repeatedly reported N100m time lag between coronal vowel [&#x000f8;] (black) and dorsal vowel [o] (gray) is also obvious.</p></caption><graphic xlink:href="1471-2202-5-24-2"/></fig><fig position="float" id="F3"><label>Figure 3</label><caption><p>Mean two-dimensional source space locations and orientations separately for the left and the right hemisphere (posterior-anterior on abscissa, inferior-superior on ordinate) are shown. Results of the phonological categorization task are shown in open source symbols, results of the speaker categorization task in filled source symbols. Please note that in both conditions the [&#x000f8;] source (circle symbols) is more inferior and anterior than the [o] source (diamond symbols).</p></caption><graphic xlink:href="1471-2202-5-24-3"/></fig><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Formant Frequency Overview. Pitch (F<sub>0</sub>), formant frequencies (F<sub>1</sub>, F<sub>2</sub>, F<sub>3</sub>) and -distance (F<sub>2</sub>-F<sub>1</sub>) for the vowels used.</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">Vowel</td><td align="center">voice</td><td align="center"><bold>F<sub>0 </sub></bold>(Hz)</td><td align="center"><bold>F<sub>1 </sub></bold>(Hz)</td><td align="center"><bold>F<sub>2 </sub></bold>(Hz)</td><td align="center"><bold>F<sub>3 </sub></bold>(Hz)</td><td align="center"><bold>F<sub>2</sub>-F<sub>1 </sub></bold>(Hz)</td></tr></thead><tbody><tr><td align="center"><bold>[o]</bold></td><td align="center"><bold>male</bold></td><td align="center">123</td><td align="center">317</td><td align="center">516</td><td align="center">2601</td><td align="center">199</td></tr><tr><td align="center"><bold>[&#x000f8;]</bold></td><td align="center"><bold>male</bold></td><td align="center">123</td><td align="center">318</td><td align="center">1357</td><td align="center">1980</td><td align="center">1039</td></tr><tr><td align="center"><bold>[o]</bold></td><td align="center"><bold>female</bold></td><td align="center">223</td><td align="center">390</td><td align="center">904</td><td align="center">2871</td><td align="center">514</td></tr><tr><td align="center"><bold>[&#x000f8;]</bold></td><td align="center"><bold>female</bold></td><td align="center">223</td><td align="center">417</td><td align="center">1731</td><td align="center">2627</td><td align="center">1314</td></tr></tbody></table></table-wrap></sec></back></article>



