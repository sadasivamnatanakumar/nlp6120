<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Med Inform Decis Mak</journal-id><journal-title>BMC Medical Informatics and Decision Making</journal-title><issn pub-type="epub">1472-6947</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">15588311</article-id><article-id pub-id-type="pmc">PMC539260</article-id><article-id pub-id-type="publisher-id">1472-6947-4-21</article-id><article-id pub-id-type="doi">10.1186/1472-6947-4-21</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Quantitative evaluation of recall and precision of CAT Crawler, a search engine specialized on retrieval of Critically Appraised Topics</article-title></title-group><contrib-group><contrib id="A1" contrib-type="author"><name><surname>Dong</surname><given-names>Peng</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>cindy_dongpeng@yahoo.com</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Wong</surname><given-names>Ling Ling</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>lingling@raffles.org</email></contrib><contrib id="A3" contrib-type="author"><name><surname>Ng</surname><given-names>Sarah</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>sarahngxl@yahoo.com.sg</email></contrib><contrib id="A4" contrib-type="author"><name><surname>Loh</surname><given-names>Marie</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>marie_lohcs@yahoo.com</email></contrib><contrib id="A5" corresp="yes" contrib-type="author"><name><surname>Mondry</surname><given-names>Adrian</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>mondry@hotmail.com</email></contrib></contrib-group><aff id="I1"><label>1</label>Medical and Clinical Informatics Group, Bioinformatics Institute, BMRC, A*STAR, Singapore</aff><pub-date pub-type="collection"><year>2004</year></pub-date><pub-date pub-type="epub"><day>10</day><month>12</month><year>2004</year></pub-date><volume>4</volume><fpage>21</fpage><lpage>21</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1472-6947/4/21"/><history><date date-type="received"><day>20</day><month>8</month><year>2004</year></date><date date-type="accepted"><day>10</day><month>12</month><year>2004</year></date></history><copyright-statement>Copyright &#x000a9; 2004 Dong et al; licensee BioMed Central Ltd.</copyright-statement><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p></license><abstract><sec><title>Background</title><p>Critically Appraised Topics (CATs) are a useful tool that helps physicians to make clinical decisions as the healthcare moves towards the practice of Evidence-Based Medicine (EBM). The fast growing World Wide Web has provided a place for physicians to share their appraised topics online, but an increasing amount of time is needed to find a particular topic within such a rich repository.</p></sec><sec sec-type="methods"><title>Methods</title><p>A web-based application, namely the CAT Crawler, was developed by Singapore's Bioinformatics Institute to allow physicians to adequately access available appraised topics on the Internet. A meta-search engine, as the core component of the application, finds relevant topics following keyword input. The primary objective of the work presented here is to evaluate the quantity and quality of search results obtained from the meta-search engine of the CAT Crawler by comparing them with those obtained from two individual CAT search engines. From the CAT libraries at these two sites, all possible keywords were extracted using a keyword extractor. Of those common to both libraries, ten were randomly chosen for evaluation. All ten were submitted to the two search engines individually, and through the meta-search engine of the CAT Crawler. Search results were evaluated for relevance both by medical amateurs and professionals, and the respective recall and precision were calculated.</p></sec><sec><title>Results</title><p>While achieving an identical recall, the meta-search engine showed a precision of 77.26% (&#x000b1;14.45) compared to the individual search engines' 52.65% (&#x000b1;12.0) (p &#x0003c; 0.001).</p></sec><sec><title>Conclusion</title><p>The results demonstrate the validity of the CAT Crawler meta-search engine approach. The improved precision due to inherent filters underlines the practical usefulness of this tool for clinicians.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>Healthcare has been steadily moving towards Evidence-Based Medicine (EBM) since the term was formally introduced in 1992 by a group led by Gordon Guyatt at McMaster University, Canada [<xref ref-type="bibr" rid="B1">1</xref>-<xref ref-type="bibr" rid="B3">3</xref>]. EBM promotes systematic literature review, critical appraisal skills and integrates scientific evidence with clinical expertise in the daily management of patients. The first three steps involved in the practice of EBM can comprehensively be summarized as a one-page written paper on a particular clinical topic, which is most commonly called a 'Critically Appraised Topic' (CAT) [<xref ref-type="bibr" rid="B4">4</xref>]. Different acronyms have emerged in various specialties, such as Best Evidence Topics (BET) [<xref ref-type="bibr" rid="B5">5</xref>] in emergency medicine and Evidence-Based Journal Club Reviews (EBJCR) [<xref ref-type="bibr" rid="B6">6</xref>] in pediatric critical care medicine. All these essentially provide physicians with a systematic method of formulating a clinical question and then critically evaluating the literature to answer the question posed.</p><p>With the use of resources on the World Wide Web becoming common practice, several academic and healthcare organizations have built online CAT libraries for knowledge sharing with peer physicians. The repository of CATs has been growing steadily since the setup of the first accessible CATBank developed by the Centre for Evidence Based Medicine, Oxford in 1992 [<xref ref-type="bibr" rid="B7">7</xref>]. Among those, BestBETs developed by the Emergency Department, Manchester Royal Infirmary [<xref ref-type="bibr" rid="B8">8</xref>] and UMHS by the Department of Pediatric, University of Michigan Health System, Ann Arbor [<xref ref-type="bibr" rid="B9">9</xref>] hold hundreds of distinct topics. They are furnished with individual search engines for fast and direct access to a particular topic. Given the wealth of such medical information scattered in cyberspace, the effectiveness of locating the correct information has become an important issue [<xref ref-type="bibr" rid="B10">10</xref>].</p><sec><title>The CAT Crawler application</title><p>It is believed that more CATs will be added into the repositories as more people participate in EBM practice. However, the non-standardized electronic format of CATs has created much difficulty for physicians to access a particular topic. Accordingly, the CAT Crawler was developed at the Bioinformatics Institute, Singapore [<xref ref-type="bibr" rid="B11">11</xref>,<xref ref-type="bibr" rid="B12">12</xref>] to provide a one-stop search and download site for physicians by setting up a common platform to access eight popular online CAT libraries. CAT Crawler is freely accessible online [<xref ref-type="bibr" rid="B12">12</xref>].</p><p>The core component of the CAT Crawler is a meta-search engine. Its search is currently based on CAT resources from eight public online libraries [<xref ref-type="bibr" rid="B11">11</xref>]. Once the user chooses the libraries he intends to use in the search, information tailored to his needs can be produced. The matched results are sorted according to their origins.</p><p>Following the user input of a query keyword, a partial search is done through information extracted during an off-line process from six websites that do not hold search engines.</p><p>The remaining search is carried out by querying the two individual search engines at BestBETs and UMHS. Use of the CAT Crawler is expected to have a quantitative and qualitative improvement of the retrieved results by post-processing obtained raw results from both libraries.</p></sec><sec><title>Motivation of the evaluation</title><p>The work presented here aims to evaluate the quantity and quality of the obtained results from the CAT Crawler meta-search engine, and thus to evaluate the validity and the usefulness of the application. Recall and precision were estimated to measure the performance of this meta-search engine versus the two individual search engines at BestBETs and UMHS.</p></sec></sec><sec sec-type="methods"><title>Methods</title><p>The workflow of this study is demonstrated in Figure <xref ref-type="fig" rid="F1">1</xref>.</p><sec><title>Selection of ten query keywords</title><p>To find a viable sample of keywords for a test search, the titles of all CATs stored in the two CAT libraries, namely BestBETs and UMHS were submitted to <italic>AnalogX Keyword Extractor</italic>, which is freely available online [<xref ref-type="bibr" rid="B13">13</xref>]. This led to a list of around 2000 keywords, of which approximately 500 were present in both libraries, of which ten were randomly chosen. In a second step, that list was curated so that only medically relevant keywords remained, excluding words such as <italic>and </italic>and <italic>day</italic>.</p></sec><sec><title>Search for technically relevant documents in the dataset</title><p>In order to be able to calculate recall as detailed below, the <italic>technical relevance </italic>of all documents in the dataset must be assessed. In this study, a document is called technically relevant for a given search term if it contains this term in the full-text. <italic>Perl </italic>scripts were developed to examine all CATs in the two libraries BestBETs and UMHS and the total number of relevant documents as per the above definition in each library was collected for further calculation. This was done for each selected keyword and the process was independent from the search using the three search engines: the CAT Crawler, BestBETs and UMHS.</p></sec><sec><title>Relevance evaluation of the retrieval results</title><p>In the next step, those ten keywords were submitted to the search engines at BestBETs and UMHS, and to the CAT Crawler meta-search engine. The retrieved links were evaluated for their relevance by 13 volunteers, who are categorized into three groups. Among them, one physician in Group I represents medical professionals, six persons in Group II represent people who were trained in biology or medicine, and six persons in Group III represent people who do not have any medical background.</p></sec><sec><title>Calculation of recall and precision</title><p>Recall and precision are two accepted measurements to determine the utility of an information retrieval system or search strategy [<xref ref-type="bibr" rid="B14">14</xref>]. They are defined as:</p><p><inline-graphic xlink:href="1472-6947-4-21-i1.gif"/></p><p>Despite the relevance evaluation from 13 volunteers, it is necessary to know the total number of the relevant documents in a database for each query keyword in order to estimate the recall. In the present study, a particular CAT in a database was defined as technically relevant if the keyword could be found in its full-text article.</p><p>The CAT Crawler is designed not to hold permanently any full-text CATs [<xref ref-type="bibr" rid="B11">11</xref>]. When a query is done choosing the option to search only BestBETs and UMHS, the total number of relevant document in its acute database is equivalent to the sum of the number of relevant documents in the two libraries BestBETs and UMHS. Accordingly, the recall and precision of the CAT Crawler meta-search engine are revised as:</p><p><inline-graphic xlink:href="1472-6947-4-21-i2.gif"/></p><p>Similarly, the recall and precision of the search engines at BestBETs and UMHS are estimated based on the combined repository of the two individual sites. The revised formula are shown below:</p><p><inline-graphic xlink:href="1472-6947-4-21-i3.gif"/></p></sec><sec><title>Performance evaluation of the CAT Crawler versus BestBETs and UMHS</title><p>The averaged precision and recall over all evaluators are used to evaluate the performance of the CAT Crawler meta-search engine. These values are compared to the estimate based on the search results from the two individual search engines at BestBETs and UMHS.</p></sec></sec><sec><title>Results</title><sec><title>Ten keywords for the search engine evaluation</title><p>According to the predefined selection criteria, the ten keywords listed in Table <xref ref-type="table" rid="T1">1</xref> were selected as the seed for a test search. The number of retrieved results from each search engine was gathered with respect to each keyword query. For the selected ten medically relevant keywords, the total number of matched results are 116, 65 and132 corresponding to the three search engines at BestBETs, UMHS and CAT Crawler. The difference of 49 retrievals between the CAT Crawler and the sum of BestBETs and UMHS reflects the meta-search engine's inherent filter function which is described previously [<xref ref-type="bibr" rid="B11">11</xref>].</p></sec><sec><title>Performance evaluation of the CAT Crawler versus BestBETs and UMHS</title><p>To compare the performance of the CAT Crawler meta-search engine to that of the two individual search engines, recall and precision were computed and averaged over the evaluation of all 13 participators. The data recorded are shown in Table <xref ref-type="table" rid="T2">2</xref>. As the CAT Crawler meta-search engine is built upon the two individual search engines, the document collection for evaluation is the combined repository of BestBETs and UMHS. The retrieved relevant documents from the CAT Crawler are the same as that from the individual search engines. This leads to the identical recall for both cases (Table <xref ref-type="table" rid="T2">2</xref>). The average precision is increased from the individual search engines' 52.65% (&#x000b1;12.0) to the CAT Crawler's 77.26% (&#x000b1;14.45). Figure <xref ref-type="fig" rid="F2">2</xref> provides a more intuitive comparison corresponding to each keyword.</p></sec></sec><sec><title>Discussion</title><p>The performance evaluation clearly places the CAT Crawler meta-search engine on par with the individual search engines at BestBETs and UMHS as far as recall is concerned, and well above them for precision (see Table <xref ref-type="table" rid="T2">2</xref> and Figure <xref ref-type="fig" rid="F2">2</xref>). According to these results, the application can be called successful: by using the CAT Crawler to look for relevant information at specific sites, the medical professional will obtain as much information as by going to the sites directly, but the precision of the obtained results will be higher.</p><p>Benoit [<xref ref-type="bibr" rid="B15">15</xref>] has analyzed various methods of information retrieval and their impact on user behavior. He finds that users wish for greater interactive opportunities to determine for themselves the potential relevance of documents, and that a parts-of-document approach is preferable for many information retrieval situations. At present, the CAT Crawler allows a number of interactive opportunities [<xref ref-type="bibr" rid="B11">11</xref>], but their implementation would have no impact on the calculation of recall and precision under the condition of the present study. Benoit's reasoning should be kept in mind, however, for improving the user friendliness in the sense that some further useful filter functions can be included in future versions of the application. While such advanced search functions will be profitable when large datasets are studied, the currently still manageable information in the online CAT libraries [<xref ref-type="bibr" rid="B11">11</xref>] will serve the user better if initially displayed in a broader way. For example, some of the information displayed here may be older than 18 months, which makes it undesirable according to the strict rules for CAT updating as defined by Sackett et al [<xref ref-type="bibr" rid="B3">3</xref>]. Formally outdated information, however, may in a given situation still be "best evidence" and positively influence the decision-making. Use of filters to block aged information will certainly influence this process.</p><p>Despite the encouraging results, some fundamental questions regarding the evaluation of this meta-search engine in particular, and also meta-search engines in general remain unsolved.</p><p>With regard to recall, there is the theoretical possibility that manually searching all documents at a given repository will yield a higher recall for a given search term. In view of hundreds of CAT documents per repository, however, it seems unlikely that a human evaluator's attention will not wander, leading to less than optimal scrutiny of the documents and introducing a non-quantifiable error to the evaluation. This is a general problem of knowledge databases, especially when indexing is done by humans, whose decisions are not consistent. In a study of 700 Medline references indexed in duplicate, the consistency of main subject-heading indexing was only 68% and that for heading-subheading combinations was significantly less [<xref ref-type="bibr" rid="B16">16</xref>]. Also, in two studies [<xref ref-type="bibr" rid="B17">17</xref>,<xref ref-type="bibr" rid="B18">18</xref>] on Medline searching, there was considerable disagreement by those judging relevance of the retrieved documents regarding which documents were relevant to a given query.</p><p>In order to overcome this problem, the number of documents that contained a given keyword as found by the keyword extractor was used as the basis for calculating the technical recall. This may (or may not) lead to numerical results for recall that differ from the absolute true value as determined above. As the same numbers are used throughout, however, the comparison of search results obtained by the individual search engines and the CAT Crawler meta-search engine remains valid.</p><p>Critics have pointed out the over-reliance of researchers on the use of recall and precision in evaluation studies [<xref ref-type="bibr" rid="B18">18</xref>] and the difficulty to design an experiment that allows both laboratory-style control and operational realism [<xref ref-type="bibr" rid="B19">19</xref>]. For instance, recall may be of only little consequence once the user has found a useful document. Rhodes and Maes [<xref ref-type="bibr" rid="B20">20</xref>] evaluated both with a traditional field user test and then asked for relevance feedback. In their experiment, users gave a score 1&#x02013;5 to each document that was delivered to calculate an overall average value for perceived precision. While a document can get a high score for precision, it may at the same time get a low score for practical usefulness. This was often due to the fact that the documents were already known to the users, in some cases had even been written by them. Accordingly, Rhodes and Maes [<xref ref-type="bibr" rid="B20">20</xref>] added features to the system that weeded out relevant documents that by some predefined criteria would not be useful. As a result, the measurable precision could be worse, but the overall usefulness could be better. In the study presented here, a similar approach was chosen in the instructions to the evaluators in the sense that they could make the distinction between 'irrelevant' (e.g. the retrieved document was only a web hosted clinical question) and 'medically irrelevant' (e.g. the word <italic>Appendicitis </italic>appeared only in the reference section of a document dealing with questions of abdominal pain relief). Due to the relatively small number, no difference could be detected between the various grades of relevance, and results were pooled to relevant/irrelevant and used for calculating recall and precision as described above. If a larger number of volunteers could be recruited, repetition of this evaluation might yield interesting results.</p><p>Other approaches have been spawned to evaluating system effectiveness in order to minimize these problems with recall and precision. One example are task-oriented methods that measure how well the user can perform certain tasks [<xref ref-type="bibr" rid="B21">21</xref>-<xref ref-type="bibr" rid="B24">24</xref>]. These different approaches were not chosen in this study for a reason: the primary aim was to compare the search engines. Under the present restrictions, recall and precision allow to answer this question.</p></sec><sec><title>Conclusions</title><p>In summary, the data obtained from the analysis of search results obtained from identical queries submitted to the two CAT libraries at BestBETs and UMHS, using either their respective search engines or the CAT Crawler meta-search engine, showed a competitive recall, and superior precision of the meta-search engine compared to the individual search engines.</p></sec><sec><title>Competing interests</title><p>The author(s) declare that they have no competing interests.</p></sec><sec><title>Authors' contributions</title><p>PD participated in the design of the study, data analysis and drafting of the manuscript. LLW and SN generated raw data for the study. ML was involved in drafting the manuscript. AM designed the study and participated in the drafting of the manuscript.</p></sec><sec><title>Pre-publication history</title><p>The pre-publication history for this paper can be accessed here:</p><p><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1472-6947/4/21/prepub"/></p></sec></body><back><ack><sec><title>Acknowledgements</title><p>The authors would like to thank the staff and students of the Bioinformatics Institute for volunteering to evaluate the performance of search.</p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Group</surname><given-names>EBMW</given-names></name></person-group><article-title>Evidence-Based Medicine. A new approach to teaching the practice of medicine.</article-title><source>JAMA</source><year>1992</year><volume>268</volume><fpage>2420</fpage><lpage>2425</lpage><pub-id pub-id-type="pmid">1404801</pub-id><pub-id pub-id-type="doi">10.1001/jama.268.17.2420</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sackett</surname><given-names>DL</given-names></name><name><surname>Rosenberg</surname><given-names>WM</given-names></name><name><surname>Gray</surname><given-names>JA</given-names></name><name><surname>Haynes</surname><given-names>RB</given-names></name><name><surname>Richardson</surname><given-names>WS</given-names></name></person-group><article-title>Evidence based medicine: what it is and what it isn't</article-title><source>BMJ</source><year>1996</year><volume>312</volume><fpage>71</fpage><lpage>72</lpage><pub-id pub-id-type="pmid">8555924</pub-id></citation></ref><ref id="B3"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Sackett</surname><given-names>DL</given-names></name><name><surname>Straus</surname><given-names>SE</given-names></name><name><surname>Richardson</surname><given-names>WS</given-names></name><name><surname>Rosenberg</surname><given-names>W</given-names></name><name><surname>Haynes</surname><given-names>RB</given-names></name></person-group><source>Evidence-Based Medicine: How to practice and teach EBM</source><year>2000</year><publisher-name> London, Churchill Livingstone</publisher-name></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sauve</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>HN</given-names></name><name><surname>Meade</surname><given-names>MO</given-names></name><name><surname>Lang</surname><given-names>JD</given-names></name><name><surname>Farkouh</surname><given-names>M</given-names></name><name><surname>Cook</surname><given-names>DJ</given-names></name><name><surname>Sackett</surname><given-names>DL</given-names></name></person-group><article-title>The critically appraised topic: a practical approach to learning critical appraisal.</article-title><source>Ann Roy Soc Phys Surg Canada</source><year>1995</year><volume>28</volume><fpage>396</fpage><lpage>398</lpage></citation></ref><ref id="B5"><citation citation-type="other"><article-title>BETs and CATs, Emergency Department, Manchester Royal Infirmary</article-title><fpage> [http://www.bestbets.org/background/betscats.html]</fpage></citation></ref><ref id="B6"><citation citation-type="other"><article-title>Pediatric Critical Care Medicine, Evidence-Based Journal Club Review</article-title><fpage> [http://pedsccm.wustl.edu/ebjournal_club.html]</fpage></citation></ref><ref id="B7"><citation citation-type="other"><article-title>CAT Library, Oxford-Centre for Evidence Based Medicine</article-title><fpage> [http://www.minervation.com/cebm2/cats/allcats.html]</fpage></citation></ref><ref id="B8"><citation citation-type="other"><article-title>BET Database Search Engine, Emergency Department, Manchester Royal Infirmary</article-title><fpage> [http://www.bestbets.org/database/search.html]</fpage></citation></ref><ref id="B9"><citation citation-type="other"><article-title>CAT Library Search Engine, University of Michigan</article-title><fpage> [http://www.med.umich.edu/pediatrics/ebm/Search.htm]</fpage></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bin</surname><given-names>L</given-names></name><name><surname>Lun</surname><given-names>KC</given-names></name></person-group><article-title>The retrieval effectiveness of medical information on the web.</article-title><source>Int J Med Inf</source><year>2001</year><volume>62</volume><fpage>155</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/S1386-5056(01)00159-9</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Dong</surname><given-names>P</given-names></name><name><surname>Mondry</surname><given-names>A</given-names></name></person-group><article-title>Enhanced quality and quantity of retrieval of Critically Appraised Topics using the CAT Crawler</article-title><source>Med Inform Internet Med</source><year>2004</year><volume>29</volume><fpage>43</fpage><lpage>55</lpage><pub-id pub-id-type="pmid">15204609</pub-id><pub-id pub-id-type="doi">10.1080/14639230310001655849</pub-id></citation></ref><ref id="B12"><citation citation-type="other"><article-title>CAT Crawler - an online resource for Critically Appraised Topics (CATs)</article-title><ext-link ext-link-type="uri" xlink:href="http://www.bii.as-tar.edu.sg/research/mig/cat.asp"/></citation></ref><ref id="B13"><citation citation-type="other"><article-title>AnalogX Keyword Extractor</article-title><ext-link ext-link-type="uri" xlink:href="http://www.analogx.com"/></citation></ref><ref id="B14"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Hersh</surname><given-names>WR</given-names></name><name><surname>Detmer</surname><given-names>WM</given-names></name><name><surname>Frisse</surname><given-names>ME</given-names></name></person-group><person-group person-group-type="editor"><name><surname>H SE and E PL</surname></name></person-group><article-title>Information-Retrieval Systems</article-title><source>Medical Informatics</source><year>2001</year><publisher-name>New York, Springer</publisher-name><fpage>539</fpage><lpage>572</lpage></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Benoit</surname><given-names>G</given-names></name></person-group><article-title>Properties-based retrieval and user decision states: User control and behavior modeling.</article-title><source>JASIST</source><year>2004</year><volume>55</volume><fpage>488</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1002/asi.10399</pub-id></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Funk</surname><given-names>ME</given-names></name><name><surname>Reid</surname><given-names>CA</given-names></name></person-group><article-title>Indexing consistency in MEDLINE</article-title><source>Bull Med Libr Assoc</source><year>1983</year><volume>71</volume><fpage>176</fpage><lpage>183</lpage><pub-id pub-id-type="pmid">6344946</pub-id></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>RB</given-names></name><name><surname>McKibbon</surname><given-names>KA</given-names></name><name><surname>Walker</surname><given-names>CJ</given-names></name><name><surname>Ryan</surname><given-names>N</given-names></name><name><surname>Fitzgerald</surname><given-names>D</given-names></name><name><surname>Ramsden</surname><given-names>MF</given-names></name></person-group><article-title>Online access to MEDLINE in clinical settings: A study of use and usefulness</article-title><source>Ann Intern Med</source><year>1990</year><volume>112</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="pmid">2403476</pub-id></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hersh</surname><given-names>WR</given-names></name></person-group><article-title>Relevance and retrieval evaluation: perspectives from medicine</article-title><source>J Am Soc Inform Sci</source><year>1994</year><volume>45</volume><fpage>201</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-4571(199404)45:3&#x0003c;201::AID-ASI9&#x0003e;3.0.CO;2-W</pub-id></citation></ref><ref id="B19"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Robertson</surname><given-names>SE</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Agosti M, Crestani F and Pasi G</surname></name></person-group><article-title>Evaluation in information retrieval</article-title><source>ESSIR LNCS</source><year>2000</year><publisher-name>, Springer-Verlag</publisher-name><fpage>81</fpage><lpage>92</lpage></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rhodes</surname><given-names>BJ</given-names></name><name><surname>Maes</surname><given-names>P</given-names></name></person-group><article-title>Just-in-time information retrieval agents</article-title><source>IBM Systems Journal</source><year>2000</year><volume>39</volume><fpage>685</fpage><lpage>704</lpage></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Egan</surname><given-names>DE</given-names></name><name><surname>Remde</surname><given-names>JR</given-names></name><name><surname>Gomez</surname><given-names>LM</given-names></name><name><surname>Landauer</surname><given-names>TK</given-names></name><name><surname>Eberhardt</surname><given-names>J</given-names></name><name><surname>Lochbaum</surname><given-names>CC</given-names></name></person-group><article-title>Formative design-evaluation of Superbook</article-title><source>ACM Trans Inf Syst</source><year>1989</year><volume>7</volume><fpage>30</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1145/64789.64790</pub-id></citation></ref><ref id="B22"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Hersh</surname><given-names>WR</given-names></name><name><surname>Elliot</surname><given-names>DL</given-names></name><name><surname>Hickam</surname><given-names>DH</given-names></name><name><surname>Wolf</surname><given-names>SL</given-names></name><name><surname>Molnar</surname><given-names>A</given-names></name><name><surname>Leichtenstein</surname><given-names>C</given-names></name></person-group><source>Towards new measures of information retrieval evaluation: Jul 9-13; Seattle, Washington, USA.</source><year>1995</year><publisher-name>New York: ACM Press</publisher-name><fpage>164</fpage><lpage>170</lpage></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Whitely</surname><given-names>WP</given-names></name><name><surname>Rennie</surname><given-names>D</given-names></name><name><surname>Hafner</surname><given-names>AW</given-names></name></person-group><article-title>The scientific community's response to evidence of fraudulent publication. The Robert Slutsky case.</article-title><source>JAMA</source><year>1994</year><volume>272</volume><fpage>170</fpage><lpage>173</lpage><pub-id pub-id-type="pmid">8015137</pub-id><pub-id pub-id-type="doi">10.1001/jama.272.2.170</pub-id></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hersh</surname><given-names>WR</given-names></name><name><surname>Pentecost</surname><given-names>J</given-names></name><name><surname>Hickham</surname><given-names>DH</given-names></name></person-group><article-title>A task-oriented approach to information retrieval evaluation</article-title><source>J Am Soc Inform Sci</source><year>1996</year><volume>47</volume><fpage>50</fpage><lpage>56</lpage></citation></ref></ref-list><sec sec-type="display-objects"><title>Figures and Tables</title><fig position="float" id="F1"><label>Figure 1</label><caption><p>Workflow for evaluation of the CAT Crawler meta-search engine</p></caption><graphic xlink:href="1472-6947-4-21-1"/></fig><fig position="float" id="F2"><label>Figure 2</label><caption><p>Precision plot of the CAT Crawler meta-search engine and two individual search engines</p></caption><graphic xlink:href="1472-6947-4-21-2"/></fig><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Ten random keywords and corresponding number of retrieved results from search engine at BestBETs, UMHS and CAT Crawler</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Keyword</td><td align="center" colspan="3">Search Engine</td></tr><tr><td></td><td colspan="3"><hr></hr></td></tr><tr><td></td><td align="center">BestBETs</td><td align="center">UMHS</td><td align="center">CAT Crawler</td></tr></thead><tbody><tr><td align="left">Appendicitis</td><td align="center">7</td><td align="center">3</td><td align="center">8</td></tr><tr><td align="left">Colic</td><td align="center">15</td><td align="center">2</td><td align="center">9</td></tr><tr><td align="left">Intubation</td><td align="center">26</td><td align="center">5</td><td align="center">22</td></tr><tr><td align="left">Ketoacidosis</td><td align="center">2</td><td align="center">2</td><td align="center">2</td></tr><tr><td align="left">Octreotide</td><td align="center">3</td><td align="center">2</td><td align="center">3</td></tr><tr><td align="left">Palsy</td><td align="center">6</td><td align="center">5</td><td align="center">10</td></tr><tr><td align="left">Prophylaxis</td><td align="center">18</td><td align="center">19</td><td align="center">30</td></tr><tr><td align="left">Sleep</td><td align="center">5</td><td align="center">13</td><td align="center">16</td></tr><tr><td align="left">Tape</td><td align="center">4</td><td align="center">2</td><td align="center">3</td></tr><tr><td align="left">Ultrasound</td><td align="center">30</td><td align="center">12</td><td align="center">29</td></tr><tr><td></td><td align="center">116</td><td align="center">65</td><td align="center">132</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Numerical recall and precision for the CAT Crawler meta-search engine and two individual search engines at BestBETs and UMHS</p></caption><table frame="hsides" rules="groups"><thead><tr><td></td><td align="center" colspan="2">Recall (%)</td><td align="center" colspan="3">Precision (%)</td></tr></thead><tbody><tr><td></td><td align="center">BestBETs &#x00026; UMHS</td><td align="center">CAT Crawler</td><td align="center">BestBETs &#x00026; UMHS</td><td align="center">CAT Crawler</td><td align="center">p-value</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="left">Appendicitis</td><td align="center">96.15</td><td align="center">96.15</td><td align="center">76.92 (&#x000b1;4.80)</td><td align="center">96.15 (&#x000b1;6.00)</td><td align="center">0.000</td></tr><tr><td align="left">Colic</td><td align="center">54.81</td><td align="center">54.81</td><td align="center">51.58 (&#x000b1;2.58)</td><td align="center">97.44 (&#x000b1;4.87)</td><td align="center">0.000</td></tr><tr><td align="left">Intubation</td><td align="center">44.12</td><td align="center">44.12</td><td align="center">48.39 (&#x000b1;13.56)</td><td align="center">68.18 (&#x000b1;19.10)</td><td align="center">0.130</td></tr><tr><td align="left">Ketoacidosis</td><td align="center">48.72</td><td align="center">48.72</td><td align="center">36.54 (&#x000b1;12.97)</td><td align="center">73.08 (&#x000b1;25.94)</td><td align="center">0.001</td></tr><tr><td align="left">Octreotide</td><td align="center">59.62</td><td align="center">59.62</td><td align="center">47.69 (&#x000b1;10.13)</td><td align="center">79.49 (&#x000b1;16.88)</td><td align="center">0.000</td></tr><tr><td align="left">Palsy</td><td align="center">70.77</td><td align="center">70.77</td><td align="center">64.34 (&#x000b1;16.37)</td><td align="center">70.77 (&#x000b1;18.01)</td><td align="center">0.002</td></tr><tr><td align="left">Prophylaxis</td><td align="center">67.03</td><td align="center">67.03</td><td align="center">63.41 (&#x000b1;11.60)</td><td align="center">78.21 (&#x000b1;14.31)</td><td align="center">0.074</td></tr><tr><td align="left">Sleep</td><td align="center">57.95</td><td align="center">57.95</td><td align="center">48.29 (&#x000b1;19.82)</td><td align="center">54.33 (&#x000b1;22.30)</td><td align="center">0.038</td></tr><tr><td align="left">Tape</td><td align="center">46.15</td><td align="center">46.15</td><td align="center">46.15 (&#x000b1;7.31)</td><td align="center">92.31 (&#x000b1;14.62)</td><td align="center">0.000</td></tr><tr><td align="left">Ultrasound</td><td align="center">42.22</td><td align="center">42.22</td><td align="center">43.22 (&#x000b1;8.06)</td><td align="center">62.60 (&#x000b1;11.68)</td><td align="center">0.017</td></tr><tr><td align="left">Average</td><td align="center">58.75 (&#x000b1;16.25)</td><td align="center">58.75 (&#x000b1;16.25)</td><td align="center">52.65 (&#x000b1;12.0)</td><td align="center">77.26 (&#x000b1;14.45)</td><td align="center">0.000</td></tr></tbody></table></table-wrap></sec></back></article>



