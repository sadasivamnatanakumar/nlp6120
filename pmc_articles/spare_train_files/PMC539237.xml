<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Neurosci</journal-id><journal-title>BMC Neuroscience</journal-title><issn pub-type="epub">1471-2202</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">15588277</article-id><article-id pub-id-type="pmc">PMC539237</article-id><article-id pub-id-type="publisher-id">1471-2202-5-56</article-id><article-id pub-id-type="doi">10.1186/1471-2202-5-56</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Age-dependent plasticity in the superior temporal sulcus in deaf humans: a functional MRI study</article-title></title-group><contrib-group><contrib id="A1" corresp="yes" contrib-type="author"><name><surname>Sadato</surname><given-names>Norihiro</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I2">2</xref><email>sadato@nips.ac.jp</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Yamada</surname><given-names>Hiroki</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>yamadahi@fmsrsa.fukui-med.ac.jp</email></contrib><contrib id="A3" contrib-type="author"><name><surname>Okada</surname><given-names>Tomohisa</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>tomokada@ibri-kobe.org</email></contrib><contrib id="A4" contrib-type="author"><name><surname>Yoshida</surname><given-names>Masaki</given-names></name><xref ref-type="aff" rid="I4">4</xref><email>masa.yoshi.50-0514@vodafone.ne.jp</email></contrib><contrib id="A5" contrib-type="author"><name><surname>Hasegawa</surname><given-names>Takehiro</given-names></name><xref ref-type="aff" rid="I4">4</xref><email>thase@abmes.twmu.ac.jp</email></contrib><contrib id="A6" contrib-type="author"><name><surname>Matsuki</surname><given-names>Ken-Ichi</given-names></name><xref ref-type="aff" rid="I4">4</xref><email>matsuki@edu00.f-edu.fukui-u.ac.jp</email></contrib><contrib id="A7" contrib-type="author"><name><surname>Yonekura</surname><given-names>Yoshiharu</given-names></name><xref ref-type="aff" rid="I5">5</xref><email>yonekura@fmsrsa.fukui-med.ac.jp</email></contrib><contrib id="A8" contrib-type="author"><name><surname>Itoh</surname><given-names>Harumi</given-names></name><xref ref-type="aff" rid="I3">3</xref><email>hitoh@fmsrsa.fukui-med.ac.jp</email></contrib></contrib-group><aff id="I1"><label>1</label>National Institute for Physiological Sciences, 38 Nishigonaka, Myodaiji, Okazaki, 444-8585, Japan</aff><aff id="I2"><label>2</label>JST/RISTEX, 2-5-1, Atago, Minato-ku, Tokyo, 105-6218, Japan</aff><aff id="I3"><label>3</label>Fukui University School of Medicine, Fukui, 910-1193, Japan</aff><aff id="I4"><label>4</label>Fukui University School of Education, Fukui, 910-1193, Japan</aff><aff id="I5"><label>5</label>Biomedical Imaging Research Center, Fukui University School of Medicine, Fukui, 910-1193, Japan</aff><pub-date pub-type="collection"><year>2004</year></pub-date><pub-date pub-type="epub"><day>8</day><month>12</month><year>2004</year></pub-date><volume>5</volume><fpage>56</fpage><lpage>56</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2202/5/56"/><history><date date-type="received"><day>25</day><month>7</month><year>2004</year></date><date date-type="accepted"><day>8</day><month>12</month><year>2004</year></date></history><copyright-statement>Copyright &#x000a9; 2004 Sadato et al; licensee BioMed Central Ltd.</copyright-statement><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p></license><abstract><sec><title>Background</title><p>Sign-language comprehension activates the auditory cortex in deaf subjects. It is not known whether this functional plasticity in the temporal cortex is age dependent. We conducted functional magnetic-resonance imaging in six deaf signers who lost their hearing before the age of 2 years, five deaf signers who were &#x0003e;5 years of age at the time of hearing loss and six signers with normal hearing. The task was sentence comprehension in Japanese sign language.</p></sec><sec><title>Results</title><p>The sign-comprehension tasks activated the planum temporale of both early- and late-deaf subjects, but not that of hearing signers. In early-deaf subjects, the middle superior temporal sulcus was more prominently activated than in late-deaf subjects.</p></sec><sec><title>Conclusions</title><p>As the middle superior temporal sulcus is known to respond selectively to human voices, our findings suggest that this subregion of the auditory-association cortex, when deprived of its proper input, might make a functional shift from human voice processing to visual processing in an age-dependent manner.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>There is evidence that cross-modal plasticity induced by auditory deprivation is apparent during sign-language perception. Sign languages involve the use of the hands and face, and are perceived visually [<xref ref-type="bibr" rid="B1">1</xref>-<xref ref-type="bibr" rid="B3">3</xref>]. Using functional MRI (fMRI), Neville <italic>et al. </italic>[<xref ref-type="bibr" rid="B1">1</xref>] observed increased activity in the superior temporal sulcus (STS) during the comprehension of American Sign Language (ASL) in both congenital deaf subjects and hearing native signers. The authors therefore suggested that the STS is related to the linguistic analysis of sign language. Nishimura <italic>et al. </italic>[<xref ref-type="bibr" rid="B2">2</xref>] found that activity was increased in the auditory-association cortex but not the primary auditory cortex of a prelingual-deaf individual during the comprehension of Japanese sign language (JSL). After this patient received a cochlear implant, the primary auditory cortex was activated by the sound of spoken words, but the auditory association cortex was not. The authors suggested that audio-visual cross-modal plasticity is confined to the auditory-association cortex and that cognitive functions (such as sign language) might trigger functional plasticity in the under-utilized auditory-association cortex. In addition, Pettito <italic>et al. </italic>[<xref ref-type="bibr" rid="B3">3</xref>] observed increased activity in the superior temporal gyrus (STG) in native deaf signers compared with hearing non-signers. These findings suggest that the changes associated with audio-visual cross-modal plasticity occur in the auditory-association cortex. However, the age dependency of this plasticity is not known. To depict the age dependency of the cross-modal plasticity, we conducted a functional MRI study of deaf signers with both early and late deafness, as well as hearing signers, performing a sign-comprehension task. 'Early deaf' subjects were defined as those who lost their ability to hear before the age of 2 years, whereas 'late deaf' subjects lost their hearing after the age of 5 years.</p></sec><sec><title>Results</title><p>Performance on the JSL comprehension task was similar across the groups (F(2, 14) = 1.279, P = 0.309, one-way ANOVA). The patterns of activity evoked during the sign-comprehension task in the hearing signers and the deaf groups are shown in Figure <xref ref-type="fig" rid="F1">1</xref>. Within the temporal cortex, all groups showed activation in the occipito-temporal junction extending to the portion of the STG posterior to the Vpc line (an imaginary vertical line in the mid-sagittal plane passing through the anterior margin of the posterior commissure). In the early- and late-deaf subjects, the activation of the posterior STG extended anteriorly to the Vpc line to reach the Vac line (an imaginary vertical line in the mid-sagittal plane passing through the posterior margin of the anterior commissure). The activation was confined to the STG, extending into the superior temporal sulcus, and was more prominent on the left side. A direct comparison between early- and late-deaf subjects revealed significantly more prominent activation of the bilateral middle STS in the early-deaf subjects (Figure <xref ref-type="fig" rid="F1">1</xref>).</p></sec><sec><title>Discussion</title><p>The onset of deafness is related to language acquisition. Prelingual deafness occurs before spoken language is learned. Hearing people generally learn their first language before 5 years of age; hence, prelingual deaf individuals are either deaf at birth or became deaf prior to developing the grammatical basis of their native language, which is usually before the age of 5 years. Postlingual deafness is the loss of acoustic senses, either suddenly due to an accident or as a gradual progression after native-language acquisition [<xref ref-type="bibr" rid="B4">4</xref>]. Hence, the early-deaf subjects in the present study are categorized as 'prelingual deaf' and the late-deaf subjects are categorized as 'postlingual deaf'. More than 90% of children with prelingual hearing loss have parents with normal hearing [<xref ref-type="bibr" rid="B5">5</xref>]. Furthermore, in Japan, the traditional teaching method for deaf children includes aural/oral methods, such as lipreading. Native signers are usually limited to those who were brought up by deaf parents. Because of this, the majority of prelingual deaf subjects learn spoken language (Japanese) in artificial ways, such as aural/oral methods. In the present study, the parents of the deaf subjects all had normal hearing. Five out of six of the early-deaf subjects started JSL training after the age of 6 years. Thus, JSL is not the first language for any of the groups in the present study.</p><p>The posterior STS was activated in all groups during sign comprehension, which is consistent with the proposed neural substrates that subserve human movement perception [<xref ref-type="bibr" rid="B6">6</xref>]. The posterior STS region is adjacent to MT/V5, which is consistently activated during the perception of human body movement [<xref ref-type="bibr" rid="B7">7</xref>-<xref ref-type="bibr" rid="B9">9</xref>]. Hence, the activation of the posterior STS in both hearing and deaf subjects is related to the perception of the movement of the hands and mouth.</p><p>Both the early- and late-deaf groups showed activation in the planum temporale, whereas hearing signers did not. Anatomically, the anterior border of the PT is the sulcus behind Heschl's gyrus and the medial border is the point where the PT fades into the insula. The posterior border of the PT involves the ascending and descending rami of the Sylvian fissure [<xref ref-type="bibr" rid="B10">10</xref>]. Functionally, the left PT is involved in word detection and generation, due to its ability to process rapid frequency changes [<xref ref-type="bibr" rid="B11">11</xref>,<xref ref-type="bibr" rid="B12">12</xref>]. The right homologue is specialized for the discrimination of melody, pitch and sound intensity [<xref ref-type="bibr" rid="B13">13</xref>,<xref ref-type="bibr" rid="B14">14</xref>].</p><p>It has been shown that non-linguistic visual stimuli (moving stimuli) activate the auditory cortex in deaf individuals, but not in hearing subjects [<xref ref-type="bibr" rid="B15">15</xref>,<xref ref-type="bibr" rid="B16">16</xref>]. McSweeney <italic>et al. </italic>[<xref ref-type="bibr" rid="B17">17</xref>] showed that the planum temporale is activated in deaf native signers in response to visual sign-language images and this activation is larger for native deaf signers compared to hearing signers. Our previous study [<xref ref-type="bibr" rid="B18">18</xref>] revealed that cross-modal activation in the temporal cortex of the deaf subjects was triggered not only by signs but also by non-linguistic biological motion (lip movement) and non-biological motion (moving dots). Signs did not activate the temporal cortex of either the hearing signers or the hearing non-signers. Thus, in the present study, the activation of the planum temporale in the early- and late-deaf subjects is probably due to the effects of auditory deprivation, rather than linguistic processes. This theory is also supported by the fact that the hearing signers in the present study did not show temporal-lobe activity during JSL comprehension, whereas the PT was more prominently activated in the deaf subjects irrespective of the timing of the onset of deafness. These findings indicate that auditory deprivation plays a significant role in mediating visual responses in the auditory cortex of deaf subjects. This is analogous with findings related to visual deprivation: irrespective of the onset of blindness, the visual-association cortex of blind subjects was activated by tactile-discrimination tasks [<xref ref-type="bibr" rid="B19">19</xref>,<xref ref-type="bibr" rid="B20">20</xref>] that were unrelated to learning Braille [<xref ref-type="bibr" rid="B20">20</xref>]. These results suggest that the processing of visual and tactile stimuli is competitively balanced in the occipital cortex. A similar competitive mechanism might occur in the PT following auditory deprivation. Activation of the STG in hearing subjects during lipreading [<xref ref-type="bibr" rid="B21">21</xref>] indicates which cortico-cortical circuits might be involved in the competitive balance between the modalities. In fact, we found that the cross-modal plasticity in the deaf subjects occurred within the neural substrates that are involved in lipreading in hearing subjects [<xref ref-type="bibr" rid="B18">18</xref>].</p><p>The middle STS, anterior to the Vpc line, was activated more prominently in the early- than the late-deaf subjects. This difference is probably not related to linguistic processes, as both early- and late-deaf subjects are equally capable of learning JSL with the same amount of training. The middle STS region is presumably the area that is selective to human voice processing [<xref ref-type="bibr" rid="B22">22</xref>]. This area is known to receive predominantly auditory input, being involved in the high-level analysis of complex acoustic information, such as the extraction of speaker-related cues, as well as the transmission of this information to other areas for multimodal integration and long-term memory storage [<xref ref-type="bibr" rid="B22">22</xref>]. This implies that early auditory deprivation (at &#x0003c;2 years of age) might shift the role of the middle STS from human voice processing to the processing of biological motion, such as hand and face movements (cross-modal plasticity). It has been suggested that once cross-modal plasticity occurs in the auditory cortex, the restoration of auditory function by means of cochlear implants is ineffective [<xref ref-type="bibr" rid="B23">23</xref>]. Hence, the first 2 years of life might be the sensitive period for the processing of human voices.</p><p>Considering that the STS voice-selective area is not sensitive to speech <italic>per se </italic>but rather to vocal features that carry nonlinguistic information [<xref ref-type="bibr" rid="B22">22</xref>], the functional role of this region in early-deaf subjects with regard to the paralinguistic aspects of sign language is of particular interest and further investigation will be necessary.</p></sec><sec><title>Conclusions</title><p>The results of the present study suggest that in early-deaf subjects, non-auditory processing, such as that involved in the perception and comprehension of sign language, involves the under-utilized area of the cortex that is thought to be selective to the human voice (middle STS). This indicates that the sensitive period for the establishment of human voice processing in the STS might be during the first 2 years of life.</p></sec><sec sec-type="methods"><title>Methods</title><p>The subjects comprised six early-deaf signers (mean age: 22.8 &#x000b1; 3.1 years), five late-deaf signers (mean age: 34.4 &#x000b1; 16.2 years) and six hearing signers (mean age: 33.7 &#x000b1; 12.1 years; Table <xref ref-type="table" rid="T1">1</xref>). The early-deaf subjects lost their hearing before 2 years of age, whereas the late-deaf subjects became deaf after the age of 5 years. The parents of all subjects had normal hearing. None of the subjects exhibited any neurological abnormalities and all had normal MRI scans. None of the cases of deafness were due to a progressive neurological disorder. All deaf and hearing subjects were strongly right handed, except for one late-deaf subject who was ambidextrous, according to the Edinburgh handedness inventory [<xref ref-type="bibr" rid="B24">24</xref>]. The study protocol was approved by the Ethical Committee of Fukui University School of Medicine, Japan, and all subjects gave their written informed consent.</p><p>The tasks involved the passive perception of JSL sentences that are frequently used in the deaf community. JSL, which has its own grammar, morphemes and phonemes, is different from spoken Japanese at all levels. JSL utilizes facial expressions as obligatory grammatical markers, as does ASL [<xref ref-type="bibr" rid="B25">25</xref>]. The fMRI session with JSL consisted of two rest and two task periods, each of 30 seconds duration, with alternating rest and task periods. During the 30-second task period, the subjects were instructed to observe a JSL sentence presented every 5 seconds by a male deaf signer in a video, which was projected onto a screen at the foot of the scanner bed and viewed through a mirror. The sentences were relatively short and straightforward; for example, "I cut a piece of paper with scissors". During the 30-second rest period, the subjects fixed their eyes on the face of a still image of the same person. Each session started with a rest period and two fMRI sessions were conducted. The procedure was identical for all hearing and deaf subjects. After the fMRI session, outside of the scanner, the subjects were presented the JSL sentences used during the session. These were shown one by one on the video screen and the subjects were required to write down the presented sentences in Japanese. On each presentation, the subjects were asked if they had seen the JSL sentence in the scanner, in order to confirm that they had been engaged in the task during the session. The percentage of correct responses was calculated as the number of correctly written sentences divided by the number of presented sentences.</p><p>A time-course series of 43 volumes was produced using T2*-weighted gradient-echo EPI sequences with a 1.5 Tesla MR imager (Signa Horizon, General Electric, Milwaukee, Wisc., USA) and a standard birdcage head coil. Each volume consisted of 11 slices, with a slice thickness of 8 mm and a 1-mm gap, which covered the entire cerebral cortex. The time interval between two successive acquisitions of the same image was 3,000 ms, the echo time was 50 ms and the flip angle was 90 degrees. The field of view was 22 cm. The digital in-plane resolution was 64 &#x000d7; 64 pixels. For anatomical reference, T1-weighted images were also obtained for each subject.</p><p>The first three volumes of each fMRI session were discarded because of unstable magnetization. The remaining 40 volumes per session were used for statistical parametric mapping (SPM99, Wellcome Department of Cognitive Neurology, London, UK) implemented in Matlab (Mathworks, Sherborn, Mass., USA) [<xref ref-type="bibr" rid="B26">26</xref>,<xref ref-type="bibr" rid="B27">27</xref>]. Following realignment and anatomical normalization, all images were filtered with a Gaussian kernel of 10 mm (full width at half maximum) in the <italic>x</italic>, <italic>y </italic>and <italic>z </italic>axes.</p><p>Statistical analysis was conducted at two levels. First, the individual task-related activation was evaluated. Second, the summary data for each individual were incorporated into the second-level analysis using a random-effects model to make inferences at a population level. The signal was proportionally scaled by setting the whole-brain mean value to 100 arbitrary units. The signal time course for each subject was modeled using a box-car function convolved with a hemodynamic-response function and temporally high-pass filtered. Session effects were also included in the model. The explanatory variables were centered at zero. To test hypotheses about regionally-specific condition effects (that is, sentence comprehension compared with rest), estimates for each model parameter were compared using the linear contrasts. The resulting set of voxel values for each contrast constituted a statistical parametric map (SPM) of the <italic>t </italic>statistic (SPM{<italic>t</italic>}).</p><p>The weighted sum of the parameter estimates in the individual analyses constituted 'contrast' images that were used for the group analysis. Contrast images obtained via individual analyses represent the normalized task-related increment of the MR signal of each subject. To examine group differences (prelingual deaf, postlingual deaf and hearing signers) in activation due to the sign-comprehension task, a random-effect model was performed with the contrast images (1 per subject) for every voxel. Using the <italic>a priori </italic>hypothesis that there would be more prominent activation in the early- than late-deaf subjects, we focused on the temporal cortex, which was anatomically defined in standard stereotaxic space [<xref ref-type="bibr" rid="B28">28</xref>]. The threshold for SPM{<italic>t</italic>} was set at <italic>P </italic>&#x0003c; .001 without a correction for multiple comparisons.</p></sec><sec><title>Authors' contributions</title><p>NS carried out the fMRI studies, data analysis and drafted the manuscript. HY and TO conducted the MR imaging. MY, TH and KM prepared the task materials. YY and HI participated in the task design and coordination. All authors read and approved the final manuscript.</p></sec></body><back><ack><sec><title>Acknowledgements</title><p>This study was supported by a Grant-in Aid for Scientific Research B#14380380 (NS) from the Japan Society for the Promotion of Science, and by Special Coordination Funds for Promoting Science and Technology from the Ministry of Education, Culture, Sports, Science and Technology of the Japanese Government.</p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Neville</surname><given-names>HJ</given-names></name><name><surname>Bavelier</surname><given-names>D</given-names></name><name><surname>Corina</surname><given-names>D</given-names></name><name><surname>Rauschecker</surname><given-names>J</given-names></name><name><surname>Karni</surname><given-names>A</given-names></name><name><surname>Lalwani</surname><given-names>A</given-names></name><name><surname>Braun</surname><given-names>A</given-names></name><name><surname>Clark</surname><given-names>V</given-names></name><name><surname>Jezzard</surname><given-names>P</given-names></name><name><surname>Turner</surname><given-names>R</given-names></name></person-group><article-title>Cerebral organization for language in deaf and hearing subjects: biological constraints and effects of experience</article-title><source>Proc Natl Acad Sci USA</source><year>1998</year><volume>95</volume><fpage>922</fpage><lpage>929</lpage><pub-id pub-id-type="pmid">9448260</pub-id><pub-id pub-id-type="doi">10.1073/pnas.95.3.922</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Nishimura</surname><given-names>H</given-names></name><name><surname>Hashikawa</surname><given-names>K</given-names></name><name><surname>Doi</surname><given-names>K</given-names></name><name><surname>Iwaki</surname><given-names>T</given-names></name><name><surname>Watanabe</surname><given-names>Y</given-names></name><name><surname>Kusuoka</surname><given-names>H</given-names></name><name><surname>Nishimura</surname><given-names>T</given-names></name><name><surname>Kubo</surname><given-names>T</given-names></name></person-group><article-title>Sign language 'heard' in the auditory cortex</article-title><source>Nature</source><year>1999</year><volume>397</volume><fpage>116</fpage><pub-id pub-id-type="pmid">9923672</pub-id><pub-id pub-id-type="doi">10.1038/16376</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pettito</surname><given-names>LA</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Gauna</surname><given-names>K</given-names></name><name><surname>Nikelski</surname><given-names>EJ</given-names></name><name><surname>Dostie</surname><given-names>D</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name></person-group><article-title>Speech-like cerebral activity in profoundly deaf people processing signed languages: implication for the neural basis of human language</article-title><source>Proc Nat Acad Sci USA</source><year>2000</year><volume>97</volume><fpage>13961</fpage><lpage>13966</lpage><pub-id pub-id-type="pmid">11106400</pub-id><pub-id pub-id-type="doi">10.1073/pnas.97.25.13961</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Okazawa</surname><given-names>H</given-names></name><name><surname>Naito</surname><given-names>Y</given-names></name><name><surname>Yonekura</surname><given-names>Y</given-names></name><name><surname>Sadato</surname><given-names>N</given-names></name><name><surname>Hirano</surname><given-names>S</given-names></name><name><surname>Nishizawa</surname><given-names>S</given-names></name><name><surname>Magata</surname><given-names>Y</given-names></name><name><surname>Ishizu</surname><given-names>K</given-names></name><name><surname>Tamaki</surname><given-names>N</given-names></name><name><surname>Honjo</surname><given-names>I</given-names></name><name><surname>Nishizawa</surname><given-names>S</given-names></name><name><surname>Magata</surname><given-names>Y</given-names></name><name><surname>Ishizu</surname><given-names>K</given-names></name><name><surname>Tamaki</surname><given-names>N</given-names></name><name><surname>Honjo</surname><given-names>I</given-names></name><name><surname>Konishi</surname><given-names>J</given-names></name></person-group><article-title>Cochlear implant efficiency in pre- and postlingual deafness A study with H<sub>2</sub><sup>15</sup>O and PET</article-title><source>Brain</source><year>1996</year><volume>119</volume><fpage>1297</fpage><lpage>1306</lpage><pub-id pub-id-type="pmid">8813292</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Eleweke</surname><given-names>CJ</given-names></name><name><surname>Rodda</surname><given-names>M</given-names></name></person-group><article-title>Factors contributing to parents' selection of a communication mode to use with their deaf children</article-title><source>Am Ann Deaf</source><year>2000</year><volume>145</volume><fpage>375</fpage><lpage>383</lpage><pub-id pub-id-type="pmid">11037069</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Decety</surname><given-names>J</given-names></name><name><surname>Grezes</surname><given-names>J</given-names></name></person-group><article-title>Neural mechanisms subserving the perception of human actions</article-title><source>Trends Cogn Sci</source><year>1999</year><volume>3</volume><fpage>172</fpage><lpage>178</lpage><pub-id pub-id-type="pmid">10322473</pub-id><pub-id pub-id-type="doi">10.1016/S1364-6613(99)01312-1</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bonda</surname><given-names>E</given-names></name><name><surname>Petrides</surname><given-names>M</given-names></name><name><surname>Evans</surname><given-names>A</given-names></name></person-group><article-title>Neural systems for tactual memories</article-title><source>J Neurophysiol</source><year>1996</year><volume>75</volume><fpage>1730</fpage><lpage>1737</lpage><pub-id pub-id-type="pmid">8727409</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>RJ</given-names></name><name><surname>Brammer</surname><given-names>M</given-names></name><name><surname>Wright</surname><given-names>I</given-names></name><name><surname>Woodruff</surname><given-names>PW</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name><name><surname>Zeki</surname><given-names>S</given-names></name></person-group><article-title>A direct demonstration of functional specialization within motion-related visual and auditory cortex of the human brain</article-title><source>Curr Biol</source><year>1996</year><volume>6</volume><fpage>1015</fpage><lpage>1019</lpage><pub-id pub-id-type="pmid">8805334</pub-id><pub-id pub-id-type="doi">10.1016/S0960-9822(02)00646-2</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Puce</surname><given-names>A</given-names></name><name><surname>Allison</surname><given-names>T</given-names></name><name><surname>Bentin</surname><given-names>S</given-names></name><name><surname>Gore</surname><given-names>JC</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><article-title>Temporal cortex activation in humans viewing eye and mouth movements</article-title><source>J Neurosci</source><year>1998</year><volume>18</volume><fpage>2188</fpage><lpage>2199</lpage><pub-id pub-id-type="pmid">9482803</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Westbury</surname><given-names>CF</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name></person-group><article-title>Quantifying variability in the planum temporale: a probability map</article-title><source>Cereb Cortex</source><year>1999</year><volume>9</volume><fpage>392</fpage><lpage>405</lpage><pub-id pub-id-type="pmid">10426418</pub-id><pub-id pub-id-type="doi">10.1093/cercor/9.4.392</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>JH</given-names></name><name><surname>Tallal</surname><given-names>P</given-names></name></person-group><article-title>Rate of acoustic change may underlie hemispheric specialization for speech perception</article-title><source>Science</source><year>1980</year><volume>207</volume><fpage>1380</fpage><lpage>1381</lpage><pub-id pub-id-type="pmid">7355297</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Zilbovicius</surname><given-names>M</given-names></name><name><surname>Crozier</surname><given-names>S</given-names></name><name><surname>Thivard</surname><given-names>L</given-names></name><name><surname>Fontaine</surname><given-names>A</given-names></name><name><surname>Masure</surname><given-names>M</given-names></name><name><surname>Samson</surname><given-names>Y</given-names></name></person-group><article-title>Lateralization of speech and auditory temporal processing</article-title><source>J Cogn Neurosci</source><year>1998</year><volume>10</volume><fpage>536</fpage><lpage>540</lpage><pub-id pub-id-type="pmid">9712682</pub-id><pub-id pub-id-type="doi">10.1162/089892998562834</pub-id></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name><name><surname>Meyer</surname><given-names>E</given-names></name></person-group><article-title>Neural mechanisms underlying melodic perception and memory for pitch</article-title><source>J Neurosci</source><year>1994</year><volume>14</volume><fpage>1908</fpage><lpage>1919</lpage><pub-id pub-id-type="pmid">8158246</pub-id></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>McAdams</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>B</given-names></name><name><surname>Savel</surname><given-names>S</given-names></name><name><surname>Thivard</surname><given-names>L</given-names></name><name><surname>Samson</surname><given-names>S</given-names></name><name><surname>Samson</surname><given-names>Y</given-names></name></person-group><article-title>The functional anatomy of sound intensity discrimination</article-title><source>J Neurosci</source><year>1998</year><volume>18</volume><fpage>6388</fpage><lpage>6394</lpage><pub-id pub-id-type="pmid">9698330</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Finney</surname><given-names>EM</given-names></name><name><surname>Fine</surname><given-names>I</given-names></name><name><surname>Dobkins</surname><given-names>KR</given-names></name></person-group><article-title>Visual stimuli activate auditory cortex in the deaf</article-title><source>Nat Neurosci</source><year>2001</year><volume>4</volume><fpage>1171</fpage><lpage>1173</lpage><pub-id pub-id-type="pmid">11704763</pub-id><pub-id pub-id-type="doi">10.1038/nn763</pub-id></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Finney</surname><given-names>EM</given-names></name><name><surname>Clementz</surname><given-names>BA</given-names></name><name><surname>Hickok</surname><given-names>G</given-names></name><name><surname>Dobkins</surname><given-names>KR</given-names></name></person-group><article-title>Visual stimuli activate auditory cortex in deaf subjects: evidence from MEG</article-title><source>Neuroreport</source><year>2003</year><volume>14</volume><fpage>1425</fpage><lpage>1427</lpage><pub-id pub-id-type="pmid">12960757</pub-id><pub-id pub-id-type="doi">10.1097/00001756-200308060-00004</pub-id></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>MacSweeney</surname><given-names>M</given-names></name><name><surname>Campbell</surname><given-names>R</given-names></name><name><surname>Woll</surname><given-names>B</given-names></name><name><surname>Giampietro</surname><given-names>V</given-names></name><name><surname>David</surname><given-names>AS</given-names></name><name><surname>McGuire</surname><given-names>PK</given-names></name><name><surname>Calvert</surname><given-names>GA</given-names></name><name><surname>Brammer</surname><given-names>MJ</given-names></name></person-group><article-title>Dissociating linguistic and nonlinguistic gestural communication in the brain</article-title><source>Neuroimage</source><year>2004</year><volume>22</volume><fpage>1605</fpage><lpage>1618</lpage><pub-id pub-id-type="pmid">15275917</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.03.015</pub-id></citation></ref><ref id="B18"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Sadato</surname><given-names>N</given-names></name><name><surname>Okada</surname><given-names>T</given-names></name><name><surname>Honda</surname><given-names>M</given-names></name><name><surname>Matsuki</surname><given-names>K-I</given-names></name><name><surname>Yoshida</surname><given-names>M</given-names></name><name><surname>Kashikura</surname><given-names>K-I</given-names></name><name><surname>Takei</surname><given-names>W</given-names></name><name><surname>Sato</surname><given-names>T</given-names></name><name><surname>Kochiyama</surname><given-names>T</given-names></name><name><surname>Yonekura</surname><given-names>Y</given-names></name></person-group><article-title>Cross-modal integration and plastic changes revealed by lip movement, random-dot motion and sign languages in the hearing and deaf</article-title><source>Cereb Cortex</source><comment></comment></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sadato</surname><given-names>N</given-names></name><name><surname>Okada</surname><given-names>T</given-names></name><name><surname>Honda</surname><given-names>M</given-names></name><name><surname>Yonekura</surname><given-names>Y</given-names></name></person-group><article-title>Critical period for cross-modal plasticity in blind humans: a functional MRI study</article-title><source>Neuroimage</source><year>2002</year><volume>16</volume><fpage>389</fpage><lpage>400</lpage><pub-id pub-id-type="pmid">12030824</pub-id><pub-id pub-id-type="doi">10.1006/nimg.2002.1111</pub-id></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sadato</surname><given-names>N</given-names></name><name><surname>Okada</surname><given-names>T</given-names></name><name><surname>Kubota</surname><given-names>K</given-names></name><name><surname>Yonekura</surname><given-names>Y</given-names></name></person-group><article-title>Tactile discrimination activates the visual cortex of the recently blind naive to Braille: a functional magnetic resonance imaging study in humans</article-title><source>Neurosci Lett</source><year>2004</year><volume>359</volume><fpage>49</fpage><lpage>52</lpage><pub-id pub-id-type="pmid">15050709</pub-id><pub-id pub-id-type="doi">10.1016/j.neulet.2004.02.005</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Calvert</surname><given-names>GA</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name><name><surname>Brammer</surname><given-names>MJ</given-names></name><name><surname>Campbell</surname><given-names>R</given-names></name><name><surname>Williams</surname><given-names>SC</given-names></name><name><surname>McGuire</surname><given-names>PK</given-names></name><name><surname>Woodruff</surname><given-names>PW</given-names></name><name><surname>Iversen</surname><given-names>SD</given-names></name><name><surname>David</surname><given-names>AS</given-names></name></person-group><article-title>Activation of auditory cortex during silent lipreading</article-title><source>Science</source><year>1997</year><volume>276</volume><fpage>593</fpage><lpage>596</lpage><pub-id pub-id-type="pmid">9110978</pub-id><pub-id pub-id-type="doi">10.1126/science.276.5312.593</pub-id></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Lafaille</surname><given-names>P</given-names></name><name><surname>Ahad</surname><given-names>P</given-names></name><name><surname>Pike</surname><given-names>B</given-names></name></person-group><article-title>Voice-selective areas in human auditory cortex</article-title><source>Nature</source><year>2000</year><volume>403</volume><fpage>309</fpage><lpage>312</lpage><pub-id pub-id-type="pmid">10659849</pub-id><pub-id pub-id-type="doi">10.1038/35002078</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>DS</given-names></name><name><surname>Lee</surname><given-names>JS</given-names></name><name><surname>Oh</surname><given-names>SH</given-names></name><name><surname>Kim</surname><given-names>S-K</given-names></name><name><surname>Kim</surname><given-names>J-W</given-names></name><name><surname>Chung</surname><given-names>J-K</given-names></name><name><surname>Lee</surname><given-names>MC</given-names></name><name><surname>Kim</surname><given-names>CS</given-names></name></person-group><article-title>Cross-modal plasticity and cochlear implants</article-title><source>Nature</source><year>2001</year><volume>409</volume><fpage>149</fpage><lpage>150</lpage><pub-id pub-id-type="pmid">11196628</pub-id><pub-id pub-id-type="doi">10.1038/35051653</pub-id></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Oldfield</surname><given-names>RC</given-names></name></person-group><article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title><source>Neuropsychologia</source><year>1971</year><volume>9</volume><fpage>97</fpage><lpage>113</lpage><pub-id pub-id-type="pmid">5146491</pub-id><pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id></citation></ref><ref id="B25"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Reilly</surname><given-names>JS</given-names></name><name><surname>Bellugi</surname><given-names>U</given-names></name></person-group><article-title>Competition on the face: affect and language in ASL motherese</article-title><source>J Child Lang</source><year>1996</year><volume>23</volume><fpage>219</fpage><lpage>239</lpage><pub-id pub-id-type="pmid">8733568</pub-id></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name><name><surname>Worsley</surname><given-names>KJ</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name><name><surname>Frackowiak</surname><given-names>RSJ</given-names></name></person-group><article-title>Statistical parametric maps in functional imaging: a general linear approach</article-title><source>Hum Brain Mapp</source><year>1995</year><volume>2</volume><fpage>189</fpage><lpage>210</lpage></citation></ref><ref id="B27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Ashburner</surname><given-names>J</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name><name><surname>Heather</surname><given-names>JD</given-names></name><name><surname>Frackowiak</surname><given-names>RSJ</given-names></name></person-group><article-title>Spatial registration and normalization of images</article-title><source>Hum Brain Mapp</source><year>1995</year><volume>2</volume><fpage>165</fpage><lpage>189</lpage></citation></ref><ref id="B28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Maldjian</surname><given-names>JA</given-names></name><name><surname>Laurienti</surname><given-names>PJ</given-names></name><name><surname>Burdette</surname><given-names>JB</given-names></name><name><surname>Kraft</surname><given-names>RA</given-names></name></person-group><article-title>An automated method for neuroanatomic and cytoarchitectonic atlas-based interrogation of fMRI data sets</article-title><source>Neuroimage</source><year>2003</year><volume>19</volume><fpage>1233</fpage><lpage>1239</lpage><pub-id pub-id-type="pmid">12880848</pub-id><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00169-1</pub-id></citation></ref></ref-list><sec sec-type="display-objects"><title>Figures and Tables</title><fig position="float" id="F1"><label>Figure 1</label><caption><p>The results of group analysis. Statistical parametric maps of the average neural activity during JSL comprehension compared with rest are shown in standard anatomical space, combining hearing signers (left column), early-deaf signers (Early Deaf; second column) and late-deaf signers (Late Deaf; third column). The region of interest was confined to the temporal cortex bilaterally. The three-dimensional information was collapsed into two-dimensional sagittal and transverse images (that is, maximum-intensity projections viewed from the right and top of the brain). A direct comparison between the early- and late-deaf groups is also shown (E &#x02013; L, right column). The statistical threshold is P &#x0003c; 0.001 (uncorrected). Right bottom, the group difference of the task-related activation (E &#x02013; L) was superimposed on sagittal and coronal sections of T1-weighted high-resolution MRIs unrelated to the subjects of the present study. fMRI data were normalized in stereotaxic space. The blue lines indicate the projections of each section that cross at (-52, -22, -2). The black arrowhead indicates the STS. Bottom middle, the percent MR signal increase during JSL comprehension compared with the rest condition in the STS (-52, -22, -2) in hearing signers (H), early-deaf (E) and late-deaf signers (L). There was a significant group effect (F(2, 14) = 23.5, P &#x0003c; 0.001). * indicates P &#x0003c; 0.001, + indicates P = 0.001 (Scheffe's post hoc test). Bottom left, task-related activation in the deaf (early + late) groups. The blue lines indicate the projections of each section that cross at (-56, -26, 4). In the deaf subjects, the superior temporal cortices are extensively activated bilaterally.</p></caption><graphic xlink:href="1471-2202-5-56-1"/></fig><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Subject profiles</p></caption><table frame="hsides" rules="groups"><thead><tr><td></td><td></td><td></td><td></td><td></td><td></td><td align="left">Lost audition (dB)</td><td></td><td></td></tr><tr><td></td><td align="left">Age (years)</td><td align="left">Sex</td><td align="left">Age of deafness onset (years)</td><td align="left">Age of beginning JSL training (years)</td><td align="left">Duration of JSL training</td><td align="left">Right ear</td><td align="left">Left ear</td><td align="left">Performance (% correct)</td></tr></thead><tbody><tr><td align="left">Early-deaf signers</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td align="left">1</td><td align="left">27</td><td align="left">M</td><td align="left">0</td><td align="left">20</td><td align="left">7</td><td align="left">95</td><td align="left">95</td><td align="left">83.3</td></tr><tr><td align="left">2</td><td align="left">20</td><td align="left">M</td><td align="left">0</td><td align="left">3.5</td><td align="left">16.5</td><td align="left">90</td><td align="left">90</td><td align="left">66.7</td></tr><tr><td align="left">3</td><td align="left">24</td><td align="left">M</td><td align="left">0</td><td align="left">14</td><td align="left">10</td><td align="left">71</td><td align="left">86</td><td align="left">75.0</td></tr><tr><td align="left">4</td><td align="left">19</td><td align="left">M</td><td align="left">0</td><td align="left">6</td><td align="left">13</td><td align="left">100</td><td align="left">100</td><td align="left">58.3</td></tr><tr><td align="left">5</td><td align="left">22</td><td align="left">M</td><td align="left">0</td><td align="left">7</td><td align="left">15</td><td align="left">95</td><td align="left">95</td><td align="left">66.7</td></tr><tr><td align="left">6</td><td align="left">25</td><td align="left">M</td><td align="left">2</td><td align="left">19</td><td align="left">6</td><td align="left">90</td><td align="left">90</td><td align="left">66.7</td></tr><tr><td align="left">Average</td><td align="left">22.8</td><td></td><td align="left">0.3</td><td align="left">11.58</td><td align="left">11.3</td><td align="left">90.2</td><td align="left">92.7</td><td align="left">69.4</td></tr><tr><td align="left">Late-deaf signers</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td align="left">7</td><td align="left">21</td><td align="left">F</td><td align="left">9</td><td align="left">11</td><td align="left">10</td><td align="left">100</td><td align="left">100</td><td align="left">83.3</td></tr><tr><td align="left">8</td><td align="left">22</td><td align="left">M</td><td align="left">5</td><td align="left">6</td><td align="left">16</td><td align="left">120</td><td align="left">120</td><td align="left">66.7</td></tr><tr><td align="left">9</td><td align="left">35</td><td align="left">F</td><td align="left">10</td><td align="left">20</td><td align="left">15</td><td align="left">120</td><td align="left">120</td><td align="left">100.0</td></tr><tr><td align="left">10</td><td align="left">61</td><td align="left">F</td><td align="left">11</td><td align="left">55</td><td align="left">6</td><td align="left">90</td><td align="left">120</td><td align="left">58.3</td></tr><tr><td align="left">11</td><td align="left">33</td><td align="left">F</td><td align="left">6</td><td align="left">27</td><td align="left">6</td><td align="left">90</td><td align="left">90</td><td align="left">91.7</td></tr><tr><td align="left">Average</td><td align="left">34.4</td><td></td><td align="left">8.2</td><td align="left">23.8</td><td align="left">10.6</td><td align="left">104</td><td align="left">110</td><td align="left">80.0</td></tr><tr><td align="left">Hearing signers</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td align="left">12</td><td align="left">46</td><td align="left">M</td><td></td><td align="left">25</td><td align="left">21</td><td></td><td></td><td align="left">75.0</td></tr><tr><td align="left">13</td><td align="left">40</td><td align="left">F</td><td></td><td align="left">32</td><td align="left">8</td><td></td><td></td><td align="left">66.7</td></tr><tr><td align="left">14</td><td align="left">26</td><td align="left">F</td><td></td><td align="left">24</td><td align="left">2</td><td></td><td></td><td align="left">83.3</td></tr><tr><td align="left">15</td><td align="left">20</td><td align="left">F</td><td></td><td align="left">16</td><td align="left">4</td><td></td><td></td><td align="left">75.0</td></tr><tr><td align="left">16</td><td align="left">23</td><td align="left">F</td><td></td><td align="left">21</td><td align="left">2</td><td></td><td></td><td align="left">83.3</td></tr><tr><td align="left">17</td><td align="left">47</td><td align="left">F</td><td></td><td align="left">27</td><td align="left">20</td><td></td><td></td><td align="left">75.0</td></tr><tr><td align="left">Average</td><td align="left">33.7</td><td></td><td></td><td align="left">24.2</td><td align="left">9.5</td><td></td><td></td><td align="left">76.4</td></tr></tbody></table></table-wrap></sec></back></article>



