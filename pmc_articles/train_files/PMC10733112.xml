<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="data-paper" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Data Brief</journal-id><journal-id journal-id-type="iso-abbrev">Data Brief</journal-id><journal-title-group><journal-title>Data in Brief</journal-title></journal-title-group><issn pub-type="epub">2352-3409</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">38125371</article-id><article-id pub-id-type="pmc">PMC10733112</article-id><article-id pub-id-type="pii">S2352-3409(23)00965-4</article-id><article-id pub-id-type="doi">10.1016/j.dib.2023.109933</article-id><article-id pub-id-type="publisher-id">109933</article-id><article-categories><subj-group subj-group-type="heading"><subject>Data Article</subject></subj-group></article-categories><title-group><article-title>EEG dataset for energy data visualizations</article-title></title-group><contrib-group><contrib contrib-type="author" id="au0001"><name><surname>Kucukler</surname><given-names>Omer Faruk</given-names></name><email>p2611950@my365.dmu.ac.uk</email><ext-link ext-link-type="uri" xlink:href="https://twitter.com/ofkcklr">@ofkcklr</ext-link><xref rid="aff0001" ref-type="aff">a</xref><xref rid="cor0001" ref-type="corresp">&#x0204e;</xref></contrib><contrib contrib-type="author" id="au0002"><name><surname>Amira</surname><given-names>Abbes</given-names></name><xref rid="aff0001" ref-type="aff">a</xref><xref rid="aff0002" ref-type="aff">b</xref></contrib><contrib contrib-type="author" id="au0003"><name><surname>Malekmohamadi</surname><given-names>Hossein</given-names></name><xref rid="aff0001" ref-type="aff">a</xref></contrib><aff id="aff0001"><label>a</label>Institute of Artificial Intelligence, De Montfort University, Leicester, UK</aff><aff id="aff0002"><label>b</label>Department of Computer Science, University of Sharjah, Sharjah, UAE</aff></contrib-group><author-notes><corresp id="cor0001"><label>&#x0204e;</label>Corresponding author. <email>p2611950@my365.dmu.ac.uk</email><ext-link ext-link-type="uri" xlink:href="https://twitter.com/ofkcklr">@ofkcklr</ext-link></corresp></author-notes><pub-date pub-type="pmc-release"><day>10</day><month>12</month><year>2023</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.--><pub-date pub-type="collection"><month>2</month><year>2024</year></pub-date><pub-date pub-type="epub"><day>10</day><month>12</month><year>2023</year></pub-date><volume>52</volume><elocation-id>109933</elocation-id><history><date date-type="received"><day>23</day><month>10</month><year>2023</year></date><date date-type="rev-recd"><day>20</day><month>11</month><year>2023</year></date><date date-type="accepted"><day>5</day><month>12</month><year>2023</year></date></history><permissions><copyright-statement>&#x000a9; 2023 Published by Elsevier Inc.</copyright-statement><copyright-year>2023</copyright-year><copyright-holder/><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p></license></permissions><abstract id="abs0001"><p>User behavior plays a substantial role in shaping household energy use. Nevertheless, the methodologies employed by researchers to examine user behavior exhibit certain limitations in terms of their reach. The present article introduces an openly accessible collection of electroencephalography (EEG) recordings, comprising EEG data collected from individuals who were subjected to energy data visualizations. The dataset comprises EEG recordings obtained from 28 individuals who were in good health. The EEG recordings were collected using a 32-channel EMOTIV EEG device, and the international 10-20 electrode system was employed for precise electrode placement. The energy data visualizations were generated and showcased utilizing the PsychoPy software. To ascertain the participants' affective state, they were requested to rate the valence and arousal of each stimulus through the utilization of a self-assessment manikin (SAM). Additionally, three inquiries were posed for every stimulation. The dataset includes both original data visualizations and ratings. Additionally, the raw EEG data has been divided into segments consisting of data visualizations and neutral images, with the use of event markers, in order to assist analysis. The EEG recordings were recorded and stored utilizing the EMOTIVPro application, whereas the subjective reactions were captured and preserved using the PsychoPy application. Furthermore, the generation of synthetic EEG data is accomplished by employing the Generative Adversarial Network (GAN) architecture on the acquired EEG dataset. The synthetic EEG data created is integrated with empirical EEG data, and afterwards subjected to qualitative and quantitative analysis in order to improve performance. The dataset presented herein showcases a pioneering utilization of EEG investigation and offers a valuable foundation for scholars in the domains of computer science, energy conservation, artificial intelligence, brain-computer interfaces, and human-computer interaction.</p></abstract><kwd-group id="keys0001"><title>Keywords</title><kwd>Electroencephalography</kwd><kwd>Energy efficiency</kwd><kwd>Data visualization</kwd><kwd>Brain-computer interface</kwd><kwd>Human-computer interaction</kwd><kwd>Generative adversarial networks</kwd></kwd-group></article-meta></front><body><p id="para0004a">Specifications Table<table-wrap position="float" id="utbl0001"><table frame="hsides" rules="groups"><tbody><tr><td valign="top">Subject</td><td valign="top">Human-Computer Interaction</td></tr><tr><td valign="top">Specific subject area</td><td valign="top">Analysis of energy consumption patterns using electroencephalographic (EEG) signals</td></tr><tr><td valign="top">Data format</td><td valign="top">Raw EEG data, synthetic EEG data and augmented EEG data</td></tr><tr><td valign="top">Type of data</td><td valign="top">Raw EEG data (.csv or .rar format)<break/>Artificial EEG data (.csv or .rar format)<break/>SAM ratings and answers to questions (.csv or .rar format)</td></tr><tr><td valign="top">Data collection</td><td valign="top">EEGs were recorded utilizing a 32-channel wireless EMOTIV EPOC Flex gel kit, with a sampling rate of 128 Hz, specifically targeting the 10-20 electrode locations. The data collection process involved the utilization of the EMOTIVPro software. Energy data visualizations are employed as stimuli. The Pavlovia program, which serves as a platform for conducting PsychoPy studies, was utilized to present the participants with the generated stimuli. In addition, Generative Adversarial Networks (GANs) are employed for the purpose of generating synthetic data.<break/>Twenty-eight participants had their EEGs recorded while being presented with visual representations of energy data. In a bright room, each participant sat at a desk with a computer in front of them. Participants had an EEG cap placed on their heads, and data was recorded from the caps' sensors via a wireless connection to a laptop. The EEG signal was annotated with the aid of PsychoPy's integration with EMOTIVPro. Furthermore, empirical EEG samples are used to generate synthetic data for all channels.</td></tr><tr><td valign="top">Data source location</td><td valign="top"><list list-type="simple" id="celist0001"><list-item id="celistitem0001"><label>&#x02022;</label><p id="para0003">Institution: De Montfort University</p></list-item><list-item id="celistitem0002"><label>&#x02022;</label><p id="para0004">City/Town/Region: Leicester</p></list-item><list-item id="celistitem0003"><label>&#x02022;</label><p id="para0005">Country: United Kingdom</p></list-item></list></td></tr><tr><td valign="top">Data accessibility</td><td valign="top">Repository name: EEG Dataset Collected During Energy Data Visualization Stimuli Presentation (EDAVIS)<break/>Data identification number: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17632/w9nk4mvgbb.3" id="interref0001sew">10.17632/w9nk4mvgbb.3</ext-link><break/>Direct URL to data: <ext-link ext-link-type="uri" xlink:href="https://data.mendeley.com/datasets/w9nk4mvgbb" id="interref0003">https://data.mendeley.com/datasets/w9nk4mvgbb</ext-link></td></tr></tbody></table></table-wrap></p><sec id="sec0002"><label>1</label><title>Value of the Data</title><p id="para9003">
<list list-type="simple" id="celist0002"><list-item id="celistitem0004"><label>&#x02022;</label><p id="para0006">The dataset comprises EEG recordings pertaining to various visualizations of energy data. The distinguishing feature of the open-access energy data visualization (EDAVIS) EEG dataset is its inclusion of a diverse range of graph stimuli representing energy consumption.</p></list-item><list-item id="celistitem0005"><label>&#x02022;</label><p id="para0007">This dataset serves as a foundational resource for exploring a novel research domain focused on discerning the energy consumers' inclinations towards energy data visualization.</p></list-item><list-item id="celistitem0006"><label>&#x02022;</label><p id="para0008">Synthetic data has the potential to augment the size of existing datasets and contribute to the enhancement of classification performance.</p></list-item><list-item id="celistitem0007"><label>&#x02022;</label><p id="para0009">The dataset holds potential benefits for students and researchers alike, as it can be utilized for the purpose of training machine learning models and other applications within the domains of neuroscience, computer science, and data science.</p></list-item><list-item id="celistitem0008"><label>&#x02022;</label><p id="para0010">The dataset has potential applications in various domains, including signal processing techniques such as feature extraction, conversion, and pre-processing. Additionally, it can be utilized for human behavior analysis, specifically in the examination of EEG features alongside subjective responses. Furthermore, the dataset holds relevance in the field of machine learning, particularly in the classification of emotions.</p></list-item></list>
</p></sec><sec id="sec0003"><label>2</label><title>Data Description</title><p id="para0011">The dataset includes two data containers (1) raw EEG data for energy data visualizations and (2) augmented data for energy data visualizations. The data format is presented as tables in .csv format or compressed zip files.</p><sec id="sec0004"><label>2.1</label><title>Objective</title><p id="para0012">Data visualizations are an effective instrument for presenting the requisite information to analyze consumer behavior and proficiently monitor energy consumption <xref rid="bib0001" ref-type="bibr">[1]</xref>. The inclusion of specific visualizations and data types is of utmost importance when employing data visualizations to analyze energy consumption <xref rid="bib0002" ref-type="bibr">[2]</xref>, <xref rid="bib0003" ref-type="bibr">[3]</xref>, <xref rid="bib0004" ref-type="bibr">[4]</xref>. The primary focus of this dataset is the analysis of participant behavior towards data visualization, rather than the prediction of consumption patterns for the purpose of enhancing consumption control, as explored in previous studies <xref rid="bib0005" ref-type="bibr">[5</xref>,<xref rid="bib0006" ref-type="bibr">6]</xref>. Moreover, the utilization of synthetic EEG data can prove to be a valuable instrument in augmenting the analysis and results of associated task accomplishment <xref rid="bib0007" ref-type="bibr">[7</xref>,<xref rid="bib0008" ref-type="bibr">8]</xref>. Furthermore, the dataset presents a novel resource for researchers investigating the advancement of machine learning models in the field of behavior analysis and other disciplines within data science.</p></sec><sec id="sec0005"><label>2.2</label><title>Raw Data: EEG data for Energy Data Visualizations</title><p id="para0013">The dataset encompasses data collected from a sample of 28 participants who were exposed to a series of 10 distinct data visualizations and neutral images during a single task. The positioning of the electrodes adheres to the established 10-20 electrode system. Emotion recognition is consistently linked to the frontal cortex <xref rid="bib0009" ref-type="bibr">[9]</xref>, <xref rid="bib0010" ref-type="bibr">[10]</xref>, <xref rid="bib0011" ref-type="bibr">[11]</xref>. In this context, the EEG channels that are taken into consideration include Fp1, Fp2, F3, F4, F7, and F8. The EEG data is recorded and stored in the comma-separated values (.csv) format. <xref rid="tbl0001" ref-type="table">Table&#x000a0;1</xref> provides a comprehensive overview of the specific details pertaining to the EEG data file.<table-wrap position="float" id="tbl0001"><label>Table 1</label><caption><p>Definition of dataset files.</p></caption><alt-text id="alt0001">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Data visualization</th><th valign="top">Subject</th><th valign="top">Channel</th><th valign="top">Valence</th><th valign="top">Arousal</th></tr></thead><tbody><tr><td valign="top">1</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">HV or LV</td><td valign="top">HA or LA</td></tr><tr><td valign="top">0 (Neutral)</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">-</td><td valign="top">-</td></tr><tr><td valign="top">2</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">HV or LV</td><td valign="top">HA or LA</td></tr><tr><td valign="top">0 (Neutral)</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">-</td><td valign="top">-</td></tr><tr><td valign="top">3</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">HV or LV</td><td valign="top">HA or LA</td></tr><tr><td valign="top">0 (Neutral)</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">-</td><td valign="top">-</td></tr><tr><td valign="top">4</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">HV or LV</td><td valign="top">HA or LA</td></tr><tr><td valign="top">0 (Neutral)</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">-</td><td valign="top">-</td></tr><tr><td valign="top">5</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">HV or LV</td><td valign="top">HA or LA</td></tr><tr><td valign="top">0 (Neutral)</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">-</td><td valign="top">-</td></tr><tr><td valign="top">6</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">HV or LV</td><td valign="top">HA or LA</td></tr><tr><td valign="top">0 (Neutral)</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">-</td><td valign="top">-</td></tr><tr><td valign="top">7</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">HV or LV</td><td valign="top">HA or LA</td></tr><tr><td valign="top">0 (Neutral)</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">-</td><td valign="top">-</td></tr><tr><td valign="top">8</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">HV or LV</td><td valign="top">HA or LA</td></tr><tr><td valign="top">0 (Neutral)</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">-</td><td valign="top">-</td></tr><tr><td valign="top">9</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">HV or LV</td><td valign="top">HA or LA</td></tr><tr><td valign="top">0 (Neutral)</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">-</td><td valign="top">-</td></tr><tr><td valign="top">10</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">HV or LV</td><td valign="top">HA or LA</td></tr><tr><td valign="top">0 (Neutral)</td><td valign="top">1Sub - 28Sub</td><td valign="top">Fp1, Fp2, F3, F4, F7 and F8</td><td valign="top">-</td><td valign="top">-</td></tr></tbody></table></table-wrap></p><p id="para0014">The dataset files are categorized into two distinct folders. The folder containing the EEG data comprises recordings in .csv format, specifically pertaining to EEG data obtained from six channels and artificial data. Additionally, the .csv folder contains data related to visualisation numbers, valence class, arousal class, and subjects. The classification of the class is determined by utilizing a rating value of 5 as the central point. Ratings of 5 and above are categorized as high (H), while ratings below 5 are categorized as low (L) on a rating scale ranging from 1 to 9. In addition to the provision of rating scores, the customization of classes can be achieved through the utilization of various thresholds. The event markers serve the purpose of extracting data visualizations and neutral images, and they are not encompassed within the file. The folder contains the original data visualizations utilized in the experiments, presented in .png format. The subjective response folder encompasses both the Self-Assessment Manikin (SAM) ratings <xref rid="bib0012" ref-type="bibr">[12]</xref> and the participants' responses to questions for each data visualisation. <xref rid="tbl0002" ref-type="table">Tables&#x000a0;2</xref> and <xref rid="tbl0003" ref-type="table">3</xref> present the mean values of SAM ratings for valence and arousal, as well as the ratings for the questions. <xref rid="fig0001" ref-type="fig">Fig.&#x000a0;1</xref> presents the distribution of valence and arousal ratings for data visualizations. EEG data and subjective responses location in the repository is illustrated below:<table-wrap position="float" id="tbl0002"><label>Table 2</label><caption><p>SAM ratings for each data visualization stimulus.</p></caption><alt-text id="alt0003">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Data visualization</th><th valign="top">Average Valence Responses</th><th valign="top">Average Arousal Responses</th></tr></thead><tbody><tr><td valign="top">1</td><td valign="top">4.95</td><td valign="top">4.52</td></tr><tr><td valign="top">2</td><td valign="top">5.48</td><td valign="top">5.59</td></tr><tr><td valign="top">3</td><td valign="top">5.48</td><td valign="top">4.77</td></tr><tr><td valign="top">4</td><td valign="top">6.41</td><td valign="top">4.91</td></tr><tr><td valign="top">5</td><td valign="top">5.56</td><td valign="top">5.15</td></tr><tr><td valign="top">6</td><td valign="top">5.77</td><td valign="top">4.49</td></tr><tr><td valign="top">7</td><td valign="top">4.57</td><td valign="top">4.93</td></tr><tr><td valign="top">8</td><td valign="top">5.81</td><td valign="top">4.85</td></tr><tr><td valign="top">9</td><td valign="top">5.97</td><td valign="top">5.08</td></tr><tr><td valign="top">10</td><td valign="top">6.14</td><td valign="top">4.93</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl0003"><label>Table 3</label><caption><p>The answers to the questions for each data visualization.</p></caption><alt-text id="alt0004">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Data visualization</th><th valign="top">Average answers for Q1<xref rid="tb3fn1" ref-type="table-fn">a</xref></th><th valign="top">Average answers for Q2<xref rid="tb3fn2" ref-type="table-fn">b</xref></th><th valign="top">Average answers for Q3<xref rid="tb3fn3" ref-type="table-fn">c</xref></th></tr></thead><tbody><tr><td valign="top">1</td><td valign="top">3.00</td><td valign="top">2.89</td><td valign="top">2.82</td></tr><tr><td valign="top">2</td><td valign="top">3.11</td><td valign="top">2.93</td><td valign="top">2.96</td></tr><tr><td valign="top">3</td><td valign="top">2.96</td><td valign="top">2.82</td><td valign="top">2.89</td></tr><tr><td valign="top">4</td><td valign="top">4.04</td><td valign="top">4.36</td><td valign="top">3.21</td></tr><tr><td valign="top">5</td><td valign="top">2.93</td><td valign="top">3.11</td><td valign="top">2.86</td></tr><tr><td valign="top">6</td><td valign="top">2.96</td><td valign="top">2.96</td><td valign="top">2.75</td></tr><tr><td valign="top">7</td><td valign="top">2.46</td><td valign="top">2.64</td><td valign="top">2.29</td></tr><tr><td valign="top">8</td><td valign="top">3.25</td><td valign="top">3.54</td><td valign="top">3.18</td></tr><tr><td valign="top">9</td><td valign="top">3.64</td><td valign="top">4.18</td><td valign="top">2.79</td></tr><tr><td valign="top">10</td><td valign="top">3.71</td><td valign="top">3.64</td><td valign="top">3.11</td></tr></tbody></table><table-wrap-foot><fn id="tb3fn1"><label>a</label><p id="notep0001">How effective was the power consumption visualization?</p></fn></table-wrap-foot><table-wrap-foot><fn id="tb3fn2"><label>b</label><p id="notep0002">Was the graph's information clear to understand?</p></fn></table-wrap-foot><table-wrap-foot><fn id="tb3fn3"><label>c</label><p id="notep0003">Do you find the presented visualization aesthetically appealing?</p></fn></table-wrap-foot></table-wrap><fig id="fig0001"><label>Fig. 1</label><caption><p>Valence and arousal ratings from all subjects for each data visualization.</p></caption><alt-text id="alt0002">Fig 1</alt-text><graphic xlink:href="gr1" id="celink0001"/></fig></p><p id="para0015">EEG data location in the repository folders:<list list-type="simple" id="list0001a"><list-item id="listi0001a"><p id="para0001a">root&#x0003e;EEG data<list list-type="simple" id="list0001b"><list-item id="listi0001b"><p id="para0017">&#x0003e;&#x0003e;EEGdataset.csv</p></list-item><list-item id="listi0001c"><p id="para0018">&#x0003e;&#x0003e;additionally data visualizations in .png format</p></list-item></list></p></list-item></list></p><p id="para0019">Subjective responses location in the repository folders:<list list-type="simple" id="list0001c"><list-item id="listi0001d"><p id="para0001d">root&#x0003e;Subjective response<list list-type="simple" id="list0001d"><list-item id="listi0001e"><p id="para0021">&#x0003e;&#x0003e;SAM ratings and question answers.csv</p></list-item></list></p></list-item></list></p><p id="para0022"><xref rid="tbl0002" ref-type="table">Table&#x000a0;2</xref> presents the mean valence and arousal ratings obtained from the entire participant pool for ten distinct data visualizations. Values below 5 are indicative of a low-class status, whereas values above 5 are indicative of a high-class status. Furthermore, for each of the ten data visualizations, a set of three inquiries is posed, and the mean ratings for all participants are presented in <xref rid="tbl0003" ref-type="table">Table&#x000a0;3</xref>. The responses provided by participants reflect their level of comprehension of the presented information through the use of data visualizations, with a rating scale ranging from 1 (indicating the lowest level of understanding) to 5 (indicating the highest level of understanding). The effectiveness of the questionnaire is evaluated using reliability statistics. The Cronbach's alpha calculated as 0.814 which delivers a promising internal reliability.</p></sec><sec id="sec0006"><label>2.3</label><title>Augmented Data: EEG data for Energy Data Visualizations</title><p id="para0026">The provided dataset encompasses augmented EEG data specifically collected for energy data visualization stimuli. The repository's &#x0201c;Artificial EEG Data&#x0201d; folder encompasses samples that have been augmented with synthetic data as well as those that have been generated artificially. &#x0201c;Valence&#x0201d; and &#x0201c;Arousal&#x0201d; are two emotion folders contained within this directory. Thereafter, individual folders for each channel are created within the emotion folders. The dataset comprises three discrete sample groups, namely mixed, male, and female. In addition, one thousand synthetic samples that have been produced are included. The location of one channel data folder within the repository, containing various samples, is illustrated below:</p><p id="para0027">Folders that symbolize data pertaining to the subsequent categories:<list list-type="simple" id="celist0003a"><list-item id="celistitem0009a"><p id="para0031">root&#x0003e; Artificial EEG data<list list-type="simple" id="celist0003b"><list-item id="celistitem0009b"><p id="para0029">&#x0003e;&#x0003e;Arousal</p></list-item><list-item id="celistitem0009c"><p id="para0030">&#x0003e;&#x0003e;&#x0003e;Fp1<list list-type="simple" id="celist0003"><list-item id="celistitem0009"><label>-</label><p id="para0031a">Male synthetic data (1,000 samples)</p></list-item><list-item id="celistitem0010"><label>-</label><p id="para0032">Female synthetic data (1,000 samples)</p></list-item><list-item id="celistitem0011"><label>-</label><p id="para0033">Mixed synthetic data (1,000 samples)</p></list-item><list-item id="celistitem0012"><label>-</label><p id="para0034">Male empirical data</p></list-item><list-item id="celistitem0013"><label>-</label><p id="para0035">Female empirical data</p></list-item><list-item id="celistitem0014"><label>-</label><p id="para0036">Mixed empirical data</p></list-item><list-item id="celistitem0015"><label>-</label><p id="para0037">Male augmented data (50 samples and 1,000 samples)</p></list-item><list-item id="celistitem0016"><label>-</label><p id="para0038">Female augmented data (50 samples and 1,000 samples)</p></list-item><list-item id="celistitem0017"><label>-</label><p id="para0039">Mixed augmented data (50 samples and 1,000 samples)</p></list-item><list-item id="celistitem0018"><label>-</label><p id="para0040">Test data (Not used in generation)</p></list-item></list></p></list-item></list></p></list-item></list></p></sec></sec><sec id="sec0007"><label>3</label><title>Experimental Design, Materials and Methods</title><sec id="sec0008"><label>3.1</label><title>Empirical EEG Data Collection for Energy Data Visualizations</title><p id="para0041">The first step in the process of experimental design entails the creation of visual representations of energy data. The initial step involves the generation of visualizations utilizing the openly accessible dataset referred to as UK-DALE <xref rid="bib0013" ref-type="bibr">[13]</xref>. Ten unique data visualizations are produced by utilizing samples extracted from the dataset. To procure EEGs, a team of researchers obtained an EMOTIV EPOC Flex EEG cap. The selected configuration of this equipment is a gel kit consisting of 32 channels, with a standardized dimension of 56 cm. Another component of the experiment involves the stimuli. The stimuli are prepared and presented utilizing the PsychoPy platform <xref rid="bib0014" ref-type="bibr">[14]</xref>, which additionally incorporates the transmission of event markers. During the course of the experimental protocol, the participants were initially subjected to the presentation of a data visualisation, which was subsequently followed by the display of a neutral image. Following that, participants were instructed to provide subjective ratings by evaluating their perceived levels of valence and arousal. Ultimately, the participants were obligated to provide answers to a set of inquiries. The aforementioned protocol is consistently followed for each occurrence of data visualisation. The concept of the neutral image, represented by an empty page, is widely recognized as a primary stimulus that reduces emotional arousal in anticipation of the following image. The process in PsychoPy is both intentionally designed and effectively implemented. To ensure effective communication with participants, a timer has been integrated to facilitate the process of SAM ratings and question slides. The valence and arousal ratings are systematically categorized on a numerical continuum spanning from 1 to 9, operating as independent constructs. The assessment of each question's rating is established based on the participants' responses, utilizing a 5-point scale for measurement. The participants are given instructions to disclose personal information on the first page of the stimuli, accompanied by a guarantee that their identities will be protected. The following section presents succinct instructions to be followed before initiating the experiment. The commencement of the experiment is facilitated by depressing the space bar, as specified on the instructional page. <xref rid="fig0002" ref-type="fig">Fig.&#x000a0;2</xref> illustrates a group of participants who participated in the experimental investigation. The research involved a cohort of 28 participants, consisting of 23 males and 5 females. The researchers computed the average age of the participants to be 27.8. Participants invited through the posters around faculties of De Montfort University and some leaflets are prepared to invite people for the study. A small amount of reward is proposed for participation as a voucher. The study encompasses a heterogeneous cohort, with no requirement for a specific medical condition as a prerequisite for inclusion. Before the initiation of the experiment, all participants were required to provide informed consent by signing a consent form, thereby granting authorization for the utilization of their data. The current study has obtained ethical approval from the research ethics committee at De Montfort University with the reference number of 421051.<fig id="fig0002"><label>Fig. 2</label><caption><p>Participants in the experiment.</p></caption><alt-text id="alt0005">Fig 2</alt-text><graphic xlink:href="gr2" id="celink0002"/></fig></p><p id="para0042">Before initiating the experiment, the EEG cap is placed on the subject's scalp. The sensors are connected in a sequential manner through the application of an electrolyte gel after the sensor gaps have been cleaned using isopropyl alcohol. The EMOTIVPro software is employed for the purpose of establishing and validating the connections of sensors, thereby ensuring a level of quality that attains a 100% threshold. The individuals are situated in close proximity to a computer screen in a setting that is adequately illuminated. Upon the completion of the requisite preparations, the commencement of the experiment takes place. The information pertaining to the duration and sequence of the stimulus is presented in <xref rid="fig0003" ref-type="fig">Fig.&#x000a0;3</xref>. The experiment has a total duration of 17 minutes, during which the first-cycle SAM rating and question parts are extended to 30 seconds. The purpose of this extension is to enhance the understanding of participants regarding these tasks.<fig id="fig0003"><label>Fig. 3</label><caption><p>Procedure of stimuli.</p></caption><alt-text id="alt0006">Fig 3</alt-text><graphic xlink:href="gr3" id="celink0003"/></fig></p><p id="para0043">The integrity of the gathered dataset was ensured by conducting each experiment using the exact same procedures. To determine the experimental procedure and acquire variables for assessing the quality of work, a preliminary study is conducted <xref rid="bib0015" ref-type="bibr">[15]</xref>. The statistical significance of experiment stimuli is examined in this prior research, which suggests encouraging results for the primary experiments. The experimenter, having acquired experience during the data collection phase of a previous study, has made any required adjustments to the current experimental phase. For instance, participant-specific information regarding sensor connection capabilities, device sensitivity, and requisite instructions has been gathered. During the main experiment phase, participant noise generation, device control, and sensor connection checks have been under controlled. The experimenter ensured that the sensor and signal quality were both 100 percent and was prepared to intervene if necessary. During the experiment, participants were provided with concise explanations and directives, including the importance of maintaining stillness, reducing intentional blinking, and minimizing body movement. The aforementioned measures were implemented on every participant, ensuring that the experiment maintained the highest possible quality. In order to ensure uniformity, all investigations were conducted under identical laboratory conditions. Throughout the experiment, the experimenter monitored recordings and carefully examined them for inconsistencies in order to annotate the data as necessary. EEG data are preprocessed subsequent to the experiment in order to eliminate artifacts including muscle activity, eye blinks, and other sounds. In fact, as part of its internal preprocessing capability, EMOTIV EPOC Flex filters the output signal at 0.2&#x02013;45 Hz with a digital 5th-order Sinc filter and at 50 Hz and 60 Hz with a digital notch filter to reduce noise <xref rid="bib0016" ref-type="bibr">[16]</xref>. EEG signals are recorded with a sampling frequency of 128 Hz. Additionally, prior to analyzing EEGs, we implemented a Butterworth band-pass filter spanning from 0.5 to 60 Hz and a notch filter at 50 Hz. In addition to other preprocessing steps, it is crucial to specify that EEG signal segments associated with each energy data visualization are extracted. As was previously stated, event markers are utilized during recording. The event markers for each data visualization are identified manually, and these segments are extracted from EEG recordings. Furthermore, recordings of neutral image stimuli are extracted and appended to the data folder. Class information is subsequently appended in conjunction with the corresponding stimulus. Class information is extracted manually from folders containing subjective responses and letter conversions are applied using a 1 to 9 scale, with 5 serving as the midpoint. Aside from that, labels ranging from 1 to 10 are assigned to each data visualization, with 0 representing neutral images. Additionally, subject identifiers are added manually to distinguish the recordings of each subject. The datasets that have been made public consist of preprocessed EEG recordings; any image conversions, segmentations, windowing, feature extraction, and statistical analysis performed on the recordings are permitted for use in further research and validation by researchers.</p><p id="para0044">It is worth noting that in order to demonstrate the power of a dataset, its effectiveness and generalizability are crucial factors to consider. Effect sizes are regarded as a dependable method for demonstrating the diversity of a dataset's components in this context. The experiment in which some participants' demographic information was gathered for this investigation. Age, gender, education, occupation, background, and vision aid are some of these factors. One can employ a sensitivity analysis to examine the impact of each group within the dataset. In order to assess the effect sizes and p-values of each demographic, the one-way ANOVA statistical test is utilized. With the goal of depicting EEG signals, the mean values of each participant are utilized. Subgroups within demographics are defined by distinct parameters as displayed in <xref rid="tbl0004" ref-type="table">Table&#x000a0;4</xref>. The background contains the greatest variety with 16 different areas. For statistical analysis, age is divided into three subgroups: ages 0 to 25, ages 25 to 40, and ages over 40.<table-wrap position="float" id="tbl0004"><label>Table 4</label><caption><p>Subgroup division for dataset's demographics.</p></caption><alt-text id="alt0007">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Demographics</th><th valign="top">Subgroup Information</th></tr></thead><tbody><tr><td valign="top">Age</td><td valign="top">0 &#x02013; 25, 25 &#x02013; 40, and above 40</td></tr><tr><td valign="top">Gender</td><td valign="top">Male and Female</td></tr><tr><td valign="top">Education</td><td valign="top">Primary School, Secondary School, High School, BSC, MSc</td></tr><tr><td valign="top">Occupation</td><td valign="top">Student, Engineer, Developer, Taxi Driver, Chef, Salesman, Senior Lecturer</td></tr><tr><td valign="top">Background</td><td valign="top">Computer Science, Graphic Design, Electrical Engineering, Computer Games Programming, Mechanical Engineering, Architecture, Politics, Mechanic, International Business and Management, Data Science, Economics, Software Engineer, Chef, Cyber Security, Salesman, Art and Design</td></tr><tr><td valign="top">Vision Aid</td><td valign="top">None and Glasses</td></tr></tbody></table></table-wrap></p><p id="para0045">After the production of subgroups, the calculated means of EEG signals for each participant are appended to the demographics-containing dataset to conduct statistical analysis. The results of statistical tests conducted on each demographic are detailed in <xref rid="tbl0005" ref-type="table">Table&#x000a0;5</xref>. Consequently, the results demonstrate that demographics influence outcomes differently. The effect size of the background on the dataset is 0.895 eta-squared, and the outcome of the ANOVA test indicates that the background is a statistically significant factor in the dataset. This indicates that datasets with greater participant diversity yield superior results. Likewise, occupation exhibits a substantial impact, as evidenced by its effect size of 0.766 and statistical significance as indicated by p-values below 0.001. These demographics demonstrate the variety of occupations and backgrounds represented in the dataset. Conversely, the effect sizes for Vision Aid and Education are small and lack statistical significance (p&#x0003e;0.05), indicating that these variables exert a comparatively modest influence on the final result.<table-wrap position="float" id="tbl0005"><label>Table 5</label><caption><p>Sensitivity analysis results for dataset's demographics.</p></caption><alt-text id="alt0008">Table 5</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Demographics</th><th valign="top">Effect Sizes (eta-squared)</th><th valign="top">P values</th></tr></thead><tbody><tr><td valign="top">Age</td><td valign="top">0.29</td><td valign="top">0.014</td></tr><tr><td valign="top">Gender</td><td valign="top">0.178</td><td valign="top">0.025</td></tr><tr><td valign="top">Education</td><td valign="top">0.013</td><td valign="top">0.998</td></tr><tr><td valign="top">Occupation</td><td valign="top"><bold>0.766</bold></td><td valign="top"><bold>&#x0003c;0.001</bold></td></tr><tr><td valign="top">Background</td><td valign="top"><bold>0.895</bold></td><td valign="top"><bold>&#x0003c;0.001</bold></td></tr><tr><td valign="top">Vision Aid</td><td valign="top">0.026</td><td valign="top">0.408</td></tr></tbody></table></table-wrap></p><p id="para0046">In relation to limitations, the influence of gender group on the results of the dataset is negligible. The dataset exhibits a comparatively limited representation of female participants, indicating potential for increased female representation. Participant recruitment was subject to certain restrictions, particularly with regard to females. It is noteworthy to mention that this data collection commenced during the COVID-19 pandemic, and after the pandemic, individuals, particularly women, exhibited reluctance to participate in experiments involving close proximity. One possible explanation is the experimental protocol, which involves the application of electrolyte gel and the connection of an EEG sensor to the head. Females have shown little interest in the experiment, despite its promotion through the distribution of leaflets and posters and in-person invitations. In contrast, age demographics indicate a small effect, spanning from 19 to 48 years old. Random sampling is employed to select participants, ensuring that the dataset remains relatively diverse. This is in contrast to comparable studies that focus solely on a specific type of participant subgroup, such as students, where age demographics might have a minimal influence <xref rid="bib0017" ref-type="bibr">[17]</xref>. In addition, the dataset contains a distinct cognitive assignment that employs visualizations of energy data as stimuli. A total of ten different data visualizations were employed in this research to illustrate energy consumption. A greater quantity of data visualizations could potentially enhance the efficacy of analysis.</p><p id="para0047">In general, this research demonstrates distinctiveness with respect to stimuli for visualizing energy data and the examination of participants' reactions to those stimuli. Furthermore, an examination of sample size calculations in a review article emphasizes the lack of literature containing effect sizes and the power of EEG datasets <xref rid="bib0018" ref-type="bibr">[18]</xref>. Upon reviewing comparable and well-known studies related to EEG emotion recognition, we find that such details have been absent <xref rid="bib0017" ref-type="bibr">[17</xref>,<xref rid="bib0019" ref-type="bibr">[19]</xref>, <xref rid="bib0020" ref-type="bibr">[20]</xref>, <xref rid="bib0021" ref-type="bibr">[21]</xref>. The demographics of the dataset and the effect sizes are presented in this paper as a point of reference for future research. Subsequent investigations may augment the diversity of demographic factors (e.g., age, gender, education, and vision aid) and employ a wider array of data visualizations to rectify the shortcomings of this study.</p></sec><sec id="sec0009"><label>3.2</label><title>Augmented Data with Generative AI: EEG Data for Energy Data Visualizations</title><p id="para0048">The restricted maneuverability for classification is mostly attributed to the poor signal-to-noise ratio of EEGs. To address this issue, huge data sets are required for effective mitigation <xref rid="bib0022" ref-type="bibr">[22]</xref>. The concept of General Adversarial Networks (GANs) was introduced by the author mentioned in reference <xref rid="bib0023" ref-type="bibr">[23]</xref>. GANs have the capability to generate highly realistic samples from intricate datasets. However, EEGs are infrequently utilized in conjunction with GANs due to the relatively new development of GANs and the necessity for further investigation <xref rid="bib0008" ref-type="bibr">[8]</xref>. GANs comprise a pair of neural networks, namely the generator and the discriminator, which function as a machine learning framework. The generator is trained to produce novel samples that are indistinguishable from real samples. In the present context, the generator is employed to generate continuous samples of EEG data that encompass the underlying EEG properties associated with the task being performed. In contrast, the discriminator is responsible for determining, based on the provided data, whether it originates from the real world or has been generated synthetically by the generator. The generator will consistently generate synthetic samples in order to deceive the discriminator by making them indistinguishable from actual samples. Subsequently, it may be employed as synthetic EEG data. The utilization of augmented EEG data has demonstrated enhanced classification capabilities, as evidenced by previous studies <xref rid="bib0024" ref-type="bibr">[24</xref>,<xref rid="bib0025" ref-type="bibr">25]</xref>. Nevertheless, this application is in its nascent stage and requires further elucidation across various dimensions. This study aims to investigate methods for increasing the number of data and assess the impact on classification performance.</p><sec id="sec0010"><label>3.2.1</label><title>Generative Adversarial Networks</title><p id="para0049">The GAN is composed of two distinct neural network designs. The generator <inline-formula><mml:math id="M1" altimg="si1.svg"><mml:mi>G</mml:mi></mml:math></inline-formula> accepts an input of a randomly sampled vector <inline-formula><mml:math id="M2" altimg="si2.svg"><mml:mi>z</mml:mi></mml:math></inline-formula> from the probability distribution <inline-formula><mml:math id="M3" altimg="si3.svg"><mml:msub><mml:mi>P</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:math></inline-formula> . It then computes a sample <inline-formula><mml:math id="M4" altimg="si4.svg"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x000b4;</mml:mo></mml:mover></mml:math></inline-formula> according to the following procedure:<disp-formula id="eqn0001"><label>(1)</label><mml:math id="M5" altimg="si5.svg"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x000b4;</mml:mo></mml:mover><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M6" altimg="si6.svg"><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:math></inline-formula> represents the parameters of the generator. The discriminator receives input in the form of generated samples, denoted as <inline-formula><mml:math id="M7" altimg="si4.svg"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x000b4;</mml:mo></mml:mover></mml:math></inline-formula>, which are drawn from the distribution <inline-formula><mml:math id="M8" altimg="si7.svg"><mml:msub><mml:mi>P</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub></mml:math></inline-formula> generated by the generator. Alternatively, it also takes in real samples, denoted as <italic>y</italic>, which are drawn from the distribution <inline-formula><mml:math id="M9" altimg="si8.svg"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of a real dataset. The discriminator employs a validation process to ascertain the authenticity of the samples:<disp-formula id="eqn0002"><label>(2)</label><mml:math id="M10" altimg="si9.svg"><mml:mrow><mml:mi>v</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x000b4;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M11" altimg="si10.svg"><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:math></inline-formula> represents the parameters of the discriminator. The validation score, denoted as <inline-formula><mml:math id="M12" altimg="si11.svg"><mml:mi>v</mml:mi></mml:math></inline-formula>, quantifies the likelihood of the dataset's real samples. The discriminator aims to optimize the likelihood of samples by awarding validation ratings to both genuine and counterfeit samples. The generator functions by employing a formula that aims to minimize the output of the discriminator in order to generate samples that closely resemble real data:<disp-formula id="eqn0003"><label>(3)</label><mml:math id="M13" altimg="si12.svg"><mml:mrow><mml:mfrac><mml:mtext>min</mml:mtext><mml:mi>G</mml:mi></mml:mfrac><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">&#x02212;</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x000b4;</mml:mo></mml:mover><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="para0050">The game being referred to is commonly known as a minimax game <xref rid="bib0023" ref-type="bibr">[23]</xref>, in which two players, namely the generator and the discriminator, engage in strategic decision-making. The training process of the GAN can be characterized by the following function:<disp-formula id="eqn0004"><label>(4)</label><mml:math id="M14" altimg="si13.svg"><mml:mrow><mml:mfrac><mml:mi>min</mml:mi><mml:mi>G</mml:mi></mml:mfrac><mml:mfrac><mml:mi>max</mml:mi><mml:mi>D</mml:mi></mml:mfrac><mml:mi mathvariant="normal">v</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">G</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">D</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">&#x02212;</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="para0051">We implemented an architectural framework based on the work of Willams et&#x000a0;al. <xref rid="bib0008" ref-type="bibr">[8]</xref>. In summary, the first GAN architecture is plagued by issues of training instability and mode collapse. In order to tackle these concerns, the Wasserstein-GAN (WGAN) proposed by Arjovsky et&#x000a0;al. in 2017 <xref rid="bib0024" ref-type="bibr">[24]</xref> is employed, wherein the initial objective function is substituted with the Wasserstein distance. The utilization of weight clipping or gradient penalty (GP) has been proposed as a means to mitigate training instability <xref rid="bib0025" ref-type="bibr">[25]</xref>. In this study, a generator and discriminator for time series (TTS) are developed using a transformer-encoder architecture, with the aim of capturing temporal relations <xref rid="bib0026" ref-type="bibr">[26]</xref>. The time data is tokenized using the patches described by Li et&#x000a0;al. <xref rid="bib0026" ref-type="bibr">[26]</xref>. The input to the GAN comprises data pertaining to the experimental condition denoted as c, while the corresponding architecture is referred to as conditional GANs <xref rid="bib0027" ref-type="bibr">[27]</xref>. According to the literature, the conditional TTS-WGANGP architecture has been proposed as a means to attain enhanced training stability and reduced instances of collapsing <xref rid="bib0008" ref-type="bibr">[8]</xref>. Additionally, this architecture leverages the benefits offered by transformers when applied to time series data <xref rid="bib0008" ref-type="bibr">[8]</xref>. The GAN parameters provided in <xref rid="tbl0006" ref-type="table">Table&#x000a0;6</xref> were revised to incorporate a more robust calculation method for our EEG data, which encompasses a greater number of data points for each condition. This study examines pairings of situations characterized by low valence and high valence, as well as low arousal and high arousal. Each pair of conditions is trained independently for each channel. The dataset consists of six channels of EEG data, and the GAN is trained individually on each channel. The training process was executed using the ADAM optimizer, with a learning rate of 0.001. A fourth-order bandpass filter with a frequency range of 0.1Hz to 30Hz is implemented prior to the input of a discriminator. The discriminator undergoes five iterations as described in reference <xref rid="bib0008" ref-type="bibr">[8]</xref>. The window slicing technique is employed to generate samples.<table-wrap position="float" id="tbl0006"><label>Table 6</label><caption><p>The GAN parameters.</p></caption><alt-text id="alt0009">Table 6</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Description</th><th valign="top">Value</th></tr></thead><tbody><tr><td valign="top">Number of Epochs</td><td valign="top">10</td></tr><tr><td valign="top">Learning Rate</td><td valign="top">0.001</td></tr><tr><td valign="top">Batch Size</td><td valign="top">16</td></tr><tr><td valign="top">Patch Size</td><td valign="top">5</td></tr><tr><td valign="top">Total Sequence Length</td><td valign="top">50</td></tr><tr><td valign="top">Generated Sequence Length</td><td valign="top">30</td></tr><tr><td valign="top">Number of Generated Channels</td><td valign="top">1</td></tr></tbody></table></table-wrap></p></sec><sec id="sec0011"><label>3.2.2</label><title>Methodology</title><p id="para0052">In this study, we conducted an evaluation to determine the extent to which the use of GANs in augmenting EEG data resulted in the generation of realistic EEG samples that accurately represented the underlying brain properties. Subsequently, we investigated whether the incorporation of GAN-augmented EEG data had a positive impact on the performance of classification algorithms. The class-level EEG data was subjected to visual inspection, encompassing both empirical and synthetic data. A methodology was employed wherein both empirical and augmented EEG data were trained and validated using a subset of test data from six participants. This approach was utilized to assess the performance of the classifier. A GAN was utilized to train on a dataset consisting of class-level data from 22 participants. Subsequently, 1,000 samples were created for each of the valence (low-high) and arousal (low-high) classes for both analyses. In this study, a random forest classifier is employed. Three sample sizes are examined for the extent of possible augmentation. Sample sizes are adopted using data samples of males, females and mixed empirical data. Mixed empirical data consists of 22 participants, while males are 17 and females are 5. A single sample corresponds to a certain category of data, while the EEG dataset used consists of six channels of data, each of which encompasses ten distinct classes for visualizations. For example, mixed samples equal 1320 (22 participants x 6 channels x 10 data visualizations). Three GAN training sessions are carried out for these separate empirical samples. A total of 5020 data points were utilized for each class data. Classification performances are evaluated on both empirical and augmented data samples. Augmented data for three sample sizes are obtained by aggregating 1,000 samples into bins of 20 for 50 synthetic samples and combined with empirical data for related training samples. <xref rid="tbl0007" ref-type="table">Table&#x000a0;7</xref> shows sample sizes for three groups and generated total samples. In addition, a further comparison of synthetically generated data performances is evaluated using an 80% training and 20% test split procedure for three groups of synthetic data. For this calculation, all generated 1,000 samples are used from each channel.<table-wrap position="float" id="tbl0007"><label>Table 7</label><caption><p>Sample sizes for GAN training.</p></caption><alt-text id="alt0010">Table 7</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Sample</th><th valign="top">Empirical Sample Size</th><th valign="top">Averaged Synthetic Sample Size</th><th valign="top">Augmented Sample Size</th></tr></thead><tbody><tr><td valign="top">Male</td><td valign="top">1020</td><td valign="top">300</td><td valign="top">1320</td></tr><tr><td valign="top">Female</td><td valign="top">300</td><td valign="top">300</td><td valign="top">600</td></tr><tr><td valign="top">Mixed</td><td valign="top">1320</td><td valign="top">300</td><td valign="top">1620</td></tr></tbody></table></table-wrap></p></sec><sec id="sec0012"><label>3.2.3</label><title>Results and Conclusions</title><p id="para0053">We qualitatively evaluated empirical and synthetically generated EEG samples visually shown in <xref rid="fig0004" ref-type="fig">Fig.&#x000a0;4</xref>. Further, we found that empirical and synthetic EEGs&#x02019; visuals indicated indistinguishable resemblances.<fig id="fig0004"><label>Fig. 4</label><caption><p>Empirical and synthetically generated EEG data.</p></caption><alt-text id="alt0011">Fig 4</alt-text><graphic xlink:href="gr4" id="celink0004"/></fig></p><p id="para0054">As for classification performances, a random forest classifier is employed to classify valence and arousal emotions. Generated synthetic data for male, female and mixed data samples are evaluated for empirical and augmented data separately. Overall, valence class performances overcome to arousal results. The augmented EEG had the highest mark with female data reaching up to 4.44% for valence, while it reaches 6.39% for arousal. Results show the correlation between sample size and accuracy is inverse, while sample size diminishes accuracy increases. This results supports the argument of inverse correlation proposed by Williams et&#x000a0;al. (2023) <xref rid="bib0008" ref-type="bibr">[8]</xref>. Results for sample groups with arousal and valence classes are depicted in <xref rid="tbl0008" ref-type="table">Table&#x000a0;8</xref>.<table-wrap position="float" id="tbl0008"><label>Table 8</label><caption><p>Classification results for empirical and augmented data of three sample sizes.</p></caption><alt-text id="alt0012">Table 8</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" valign="top">Method</th><th colspan="3" align="left" valign="top">Valence (%)<hr/></th><th valign="top"/><th valign="top"/><th colspan="3" align="left" valign="top">Arousal (%)<hr/></th></tr><tr><th valign="top">Accuracy</th><th valign="top">Precision</th><th valign="top">Recall</th><th valign="top">F1-score</th><th valign="top">Accuracy</th><th valign="top">Precision</th><th valign="top">Recall</th><th valign="top">F1-score</th></tr></thead><tbody><tr><td valign="top">Male Empirical</td><td valign="top">60.56</td><td valign="top">63.94</td><td valign="top">90.17</td><td valign="top">74.82</td><td valign="top">41.11</td><td valign="top">56.41</td><td valign="top">19.82</td><td valign="top">29.33</td></tr><tr><td valign="top">Male Augmented</td><td valign="top">60.28</td><td valign="top">63.66</td><td valign="top">90.6</td><td valign="top">74.78</td><td valign="top">40.83</td><td valign="top">55.7</td><td valign="top">19.82</td><td valign="top">29.24</td></tr><tr><td valign="top">Mixed Empirical</td><td valign="top">61.94</td><td valign="top">64.22</td><td valign="top">93.59</td><td valign="top">76.17</td><td valign="top">40.28</td><td valign="top">52.59</td><td valign="top">31.98</td><td valign="top">39.78</td></tr><tr><td valign="top">Mixed Augmented</td><td valign="top">63.33</td><td valign="top">65</td><td valign="top">94.44</td><td valign="top">77</td><td valign="top">42.22</td><td valign="top">56.25</td><td valign="top">28.38</td><td valign="top">37.72</td></tr><tr><td valign="top">Female Empirical</td><td valign="top">60.56</td><td valign="top">64.11</td><td valign="top">89.32</td><td valign="top">74.64</td><td valign="top">52.5</td><td valign="top">60</td><td valign="top">68.92</td><td valign="top">64.15</td></tr><tr><td valign="top">Female Augmented</td><td valign="top">65</td><td valign="top">65.79</td><td valign="top">96.15</td><td valign="top">78.12</td><td valign="top">58.89</td><td valign="top">63.03</td><td valign="top">80.63</td><td valign="top">70.75</td></tr></tbody></table></table-wrap></p><p id="para0055">Furthermore, an analysis of synthetically generated data is made to compare classification outcomes for three groups. Utilizing synthetic sample groups that are generated, the validation of synthetic data is assessed. In the context of classification, synthetic data is split into 80% training data and 20% test data for each group. The validation results and performance metrics are presented in <xref rid="tbl0009" ref-type="table">Table&#x000a0;9</xref>. The outcomes show synthetic data as a separate dataset shows superior performances for male and female group while mixed group gives almost near results with empirical data.<table-wrap position="float" id="tbl0009"><label>Table 9</label><caption><p>Classification results for synthetic data of three sample sizes.</p></caption><alt-text id="alt0013">Table 9</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="left" valign="top">Method</th><th colspan="4" align="left" valign="top">Valence (%)<hr/></th><th colspan="4" align="left" valign="top">Arousal (%)<hr/></th></tr><tr><th valign="top">Accuracy</th><th valign="top">Precision</th><th valign="top">Recall</th><th valign="top">F1-score</th><th valign="top">Accuracy</th><th valign="top">Precision</th><th valign="top">Recall</th><th valign="top">F1-score</th></tr></thead><tbody><tr><td valign="top">Male Synthetic</td><td valign="top">65.5</td><td valign="top">68.19</td><td valign="top">60.85</td><td valign="top">64.31</td><td valign="top">60.17</td><td valign="top">62.21</td><td valign="top">56.12</td><td valign="top">59.01</td></tr><tr><td valign="top">Female Synthetic</td><td valign="top">70.33</td><td valign="top">73.93</td><td valign="top">64.76</td><td valign="top">69.04</td><td valign="top">71.08</td><td valign="top">75</td><td valign="top">65.09</td><td valign="top">69.69</td></tr><tr><td valign="top">Mixed Synthetic</td><td valign="top">58.64</td><td valign="top">59.6</td><td valign="top">54.61</td><td valign="top">57</td><td valign="top">59.42</td><td valign="top">60.22</td><td valign="top">56.45</td><td valign="top">58.27</td></tr></tbody></table></table-wrap></p><p id="para0056">Additionally, a further validation is provided for EEGs generated synthetically by comparing the medians of empirical and synthetic EEG clusters via Euclidean distance. Initially, the dimensionality of empirical and synthetic EEG data is reduced to two components through the implementation of principal component analysis (PCA). The medians of each data cluster are subsequently computed. The formula for calculating the Euclidean distance between medians involves taking the square root of the sum of the squared differences between the two vectors. The data distribution and distances between the three groups are depicted in <xref rid="fig0005" ref-type="fig">Fig.&#x000a0;5</xref>. Visually, synthetic EEGs exhibit similarities to empirical data concerning the outcomes of the three groups. The female group provides the shortest distance with 1.8, indicating a high degree of similarity between synthetic and empirical data, followed by the mixed group with 3.25, with the male group providing the greatest distance of 6.6.<fig id="fig0005"><label>Fig. 5</label><caption><p>Euclidean distances between medians of three sample groups.</p></caption><alt-text id="alt0014">Fig 5</alt-text><graphic xlink:href="gr5" id="celink0005"/></fig></p><p id="para0057">In conclusion, overall findings showed that GAN-augmented EEG can enhance classification performance with random forest classifier. Data augmentation for EEG signals are evaluated using GAN method. The outcomes inform that synthetically generated EEGs can show resemblance and help to increase data size for more promising evaluations. The classification performances can be enhanced with augmented EEGs which open a way to improve performances of small datasets.</p></sec></sec></sec><sec id="sec0013"><title>Limitations</title><p id="para0058">None.</p></sec><sec id="sec0014"><title>Ethics Statement</title><p id="para0059">The informed consent was obtained from participants<italic>.</italic> The data acquisition was carried out in accordance with the Declaration of Helsinki, and the project was approved by Faculty of Computing, Engineering and Media at De Montfort University (CEM ID No G421051).</p></sec><sec id="sec0014a"><title>CRediT authorship contribution statement</title><p id="para0059a"><bold>Omer Faruk Kucukler:</bold> Conceptualization, Methodology, Software, Data curation, Writing &#x02013; original draft, Visualization, Investigation, Software. <bold>Abbes Amira:</bold> Conceptualization, Supervision, Writing &#x02013; review &#x00026; editing. <bold>Hossein Malekmohamadi:</bold> Conceptualization, Supervision, Writing &#x02013; review &#x00026; editing.</p></sec></body><back><ref-list id="cebibl1"><title>References</title><ref id="bib0001"><label>1</label><element-citation publication-type="journal" id="sbref0001"><person-group person-group-type="author"><name><surname>Herrmann</surname><given-names>M.R.</given-names></name><name><surname>Brumby</surname><given-names>D.P.</given-names></name><name><surname>Oreszczyn</surname><given-names>T.</given-names></name><name><surname>Gilbert</surname><given-names>X.M.P.</given-names></name></person-group><article-title>Does data visualization affect users&#x02019; understanding of electricity consumption?</article-title><source>Build. Res. Inf.</source><volume>46</volume><year>2018</year><fpage>238</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1080/09613218.2017.1356164</pub-id></element-citation></ref><ref id="bib0002"><label>2</label><element-citation publication-type="journal" id="sbref0002"><person-group person-group-type="author"><name><surname>Herrmann</surname><given-names>M.R.</given-names></name><name><surname>Brumby</surname><given-names>D.P.</given-names></name><name><surname>Cheng</surname><given-names>L.</given-names></name><name><surname>Gilbert</surname><given-names>X.M.P.</given-names></name><name><surname>Oreszczyn</surname><given-names>T.</given-names></name></person-group><article-title>An empirical investigation of domestic energy data visualizations</article-title><source>Int. J. Hum. Comput. Stud.</source><volume>152</volume><year>2021</year><object-id pub-id-type="publisher-id">102660</object-id><pub-id pub-id-type="doi">10.1016/j.ijhcs.2021.102660</pub-id></element-citation></ref><ref id="bib0003"><label>3</label><element-citation publication-type="book" id="sbref0003"><person-group person-group-type="author"><name><surname>Al-Kababji</surname><given-names>A.</given-names></name><name><surname>Alsalemi</surname><given-names>A.</given-names></name><name><surname>Himeur</surname><given-names>Y.</given-names></name><name><surname>Bensaali</surname><given-names>F.</given-names></name><name><surname>Amira</surname><given-names>A.</given-names></name><name><surname>Fernandez</surname><given-names>R.</given-names></name><name><surname>Fetais</surname><given-names>N.</given-names></name></person-group><part-title>Energy data visualizations on smartphones for triggering behavioral change: novel vs. conventional</part-title><source>2020 2nd Global Power, Energy and Communication Conference (GPECOM)</source><year>2020</year><fpage>312</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1109/GPECOM49333.2020.9247901</pub-id></element-citation></ref><ref id="bib0004"><label>4</label><element-citation publication-type="book" id="sbref0004"><person-group person-group-type="author"><name><surname>Spangher</surname><given-names>L.</given-names></name><name><surname>Tawade</surname><given-names>A.</given-names></name><name><surname>Devonport</surname><given-names>A.</given-names></name><name><surname>Spanos</surname><given-names>C.</given-names></name></person-group><part-title>Engineering vs. Ambient type visualizations: quantifying effects of different data visualizations on energy consumption</part-title><source>Proceedings of the 1st ACM International Workshop on Urban Building Energy Sensing, Controls, Big Data Analysis, and Visualization</source><year>2019</year><publisher-name>ACM</publisher-name><publisher-loc>New York NY USA</publisher-loc><fpage>14</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1145/3363459.3363527</pub-id></element-citation></ref><ref id="bib0005"><label>5</label><element-citation publication-type="journal" id="sbref0005"><person-group person-group-type="author"><name><surname>Athanasiadis</surname><given-names>C.</given-names></name><name><surname>Doukas</surname><given-names>D.</given-names></name><name><surname>Papadopoulos</surname><given-names>T.</given-names></name><name><surname>Chrysopoulos</surname><given-names>A.</given-names></name></person-group><article-title>A scalable real-time non-intrusive load monitoring system for the estimation of household appliance power consumption</article-title><source>Energies</source><volume>14</volume><year>2021</year><fpage>767</fpage><pub-id pub-id-type="doi">10.3390/en14030767</pub-id></element-citation></ref><ref id="bib0006"><label>6</label><element-citation publication-type="journal" id="sbref0006"><person-group person-group-type="author"><name><surname>Izidio</surname><given-names>D.M.F.</given-names></name><name><surname>de Mattos Neto</surname><given-names>P.S.G.</given-names></name><name><surname>Barbosa</surname><given-names>L.</given-names></name><name><surname>de Oliveira</surname><given-names>J.F.L.</given-names></name><name><surname>da N. Marinho</surname><given-names>M.H.</given-names></name><name><surname>Rissi</surname><given-names>G.F.</given-names></name></person-group><article-title>Evolutionary hybrid system for energy consumption forecasting for smart meters</article-title><source>Energies</source><volume>14</volume><year>2021</year><fpage>1794</fpage><pub-id pub-id-type="doi">10.3390/en14071794</pub-id></element-citation></ref><ref id="bib0007"><label>7</label><element-citation publication-type="journal" id="sbref0007"><person-group person-group-type="author"><name><surname>Fahimi</surname><given-names>F.</given-names></name><name><surname>Dosen</surname><given-names>S.</given-names></name><name><surname>Ang</surname><given-names>K.K.</given-names></name><name><surname>Mrachacz-Kersting</surname><given-names>N.</given-names></name><name><surname>Guan</surname><given-names>C.</given-names></name></person-group><article-title>Generative adversarial networks-based data augmentation for brain&#x02013;computer interface</article-title><source>IEEE Trans. Neural Networks Learn. Syst.</source><volume>32</volume><year>2021</year><fpage>4039</fpage><lpage>4051</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2020.3016666</pub-id></element-citation></ref><ref id="bib0008"><label>8</label><element-citation publication-type="book" id="sbref0008"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>C.C.</given-names></name><name><surname>Weinhardt</surname><given-names>D.</given-names></name><name><surname>Wirzberger</surname><given-names>M.</given-names></name><name><surname>Musslick</surname><given-names>S.</given-names></name></person-group><part-title>Augmenting EEG with generative adversarial networks enhances brain decoding across classifiers and sample sizes</part-title><source>Proceedings of the Annual Meeting of the Cognitive Science Society</source><year>2023</year><fpage>45</fpage><ext-link ext-link-type="uri" xlink:href="https://escholarship.org/uc/item/9gz8g908" id="interref0004">https://escholarship.org/uc/item/9gz8g908</ext-link><comment>(accessed August 10, 2023)</comment></element-citation></ref><ref id="bib0009"><label>9</label><element-citation publication-type="journal" id="sbref0009"><person-group person-group-type="author"><name><surname>Kober</surname><given-names>H.</given-names></name><name><surname>Barrett</surname><given-names>L.F.</given-names></name><name><surname>Joseph</surname><given-names>J.</given-names></name><name><surname>Bliss-Moreau</surname><given-names>E.</given-names></name><name><surname>Lindquist</surname><given-names>K.</given-names></name><name><surname>Wager</surname><given-names>T.D.</given-names></name></person-group><article-title>Functional grouping and cortical&#x02013;subcortical interactions in emotion: a meta-analysis of neuroimaging studies</article-title><source>Neuroimage</source><volume>42</volume><year>2008</year><fpage>998</fpage><lpage>1031</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.059</pub-id><pub-id pub-id-type="pmid">18579414</pub-id>
</element-citation></ref><ref id="bib0010"><label>10</label><element-citation publication-type="journal" id="sbref0010"><person-group person-group-type="author"><name><surname>Dennis</surname><given-names>T.A.</given-names></name><name><surname>Solomon</surname><given-names>B.</given-names></name></person-group><article-title>Frontal EEG and emotion regulation: electrocortical activity in response to emotional film clips is associated with reduced mood induction and attention interference effects</article-title><source>Biol. Psychol.</source><volume>85</volume><year>2010</year><fpage>456</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2010.09.008</pub-id><pub-id pub-id-type="pmid">20863872</pub-id>
</element-citation></ref><ref id="bib0011"><label>11</label><element-citation publication-type="journal" id="sbref0011"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z.-M.</given-names></name><name><surname>Hu</surname><given-names>S.-Y.</given-names></name><name><surname>Song</surname><given-names>H.</given-names></name></person-group><article-title>Channel selection method for EEG emotion recognition using normalized mutual information</article-title><source>IEEE Access</source><volume>7</volume><year>2019</year><fpage>143303</fpage><lpage>143311</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2944273</pub-id></element-citation></ref><ref id="bib0012"><label>12</label><element-citation publication-type="journal" id="sbref0012"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>J.D.</given-names></name></person-group><article-title>Observations: SAM: the self-assessment manikin: an efficient cross-cultural measurement of emotional response</article-title><source>J. Advertising Res.</source><volume>35</volume><year>1995</year><fpage>63</fpage><lpage>68</lpage></element-citation></ref><ref id="bib0013"><label>13</label><element-citation publication-type="journal" id="sbref0013"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>J.</given-names></name><name><surname>Knottenbelt</surname><given-names>W.</given-names></name></person-group><article-title>The UK-DALE dataset, domestic appliance-level electricity demand and whole-house demand from five UK homes</article-title><source>Sci. Data</source><volume>2</volume><year>2015</year><object-id pub-id-type="publisher-id">150007</object-id><pub-id pub-id-type="doi">10.1038/sdata.2015.7</pub-id></element-citation></ref><ref id="bib0014"><label>14</label><element-citation publication-type="journal" id="sbref0014"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>J.</given-names></name><name><surname>Gray</surname><given-names>J.R.</given-names></name><name><surname>Simpson</surname><given-names>S.</given-names></name><name><surname>MacAskill</surname><given-names>M.</given-names></name><name><surname>H&#x000f6;chenberger</surname><given-names>R.</given-names></name><name><surname>Sogo</surname><given-names>H.</given-names></name><name><surname>Kastman</surname><given-names>E.</given-names></name><name><surname>Lindel&#x000f8;v</surname><given-names>J.K.</given-names></name></person-group><article-title>PsychoPy2: experiments in behavior made easy</article-title><source>Behav. Res.</source><volume>51</volume><year>2019</year><fpage>195</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id></element-citation></ref><ref id="bib0015"><label>15</label><element-citation publication-type="book" id="sbref0015"><person-group person-group-type="author"><name><surname>Kucukler</surname><given-names>O.F.</given-names></name><name><surname>Amira</surname><given-names>A.</given-names></name><name><surname>Malekmohamadi</surname><given-names>H.</given-names></name></person-group><part-title>Statistical analysis of electroencephalographic signals in the stimulation of energy data visualizations</part-title><person-group person-group-type="editor"><name><surname>Arai</surname><given-names>K.</given-names></name></person-group><source>Intelligent Computing</source><year>2022</year><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc><fpage>504</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1007/978-3-031-10464-0_34</pub-id></element-citation></ref><ref id="bib0016"><label>16</label><mixed-citation publication-type="other" id="sbref0016">EMOTIV EPOC Flex Gel Kit, (n.d.). <ext-link ext-link-type="uri" xlink:href="https://emotiv.gitbook.io/epoc-flex-user-manual/" id="interref0005">https://emotiv.gitbook.io/epoc-flex-user-manual/</ext-link> (accessed July 7, 2022).</mixed-citation></ref><ref id="bib0017"><label>17</label><element-citation publication-type="journal" id="sbref0017"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>W.-L.</given-names></name><name><surname>Lu</surname><given-names>B.-L.</given-names></name></person-group><article-title>Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks</article-title><source>IEEE Trans. Auton. Ment. Dev.</source><volume>7</volume><year>2015</year><fpage>162</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1109/TAMD.2015.2431497</pub-id></element-citation></ref><ref id="bib0018"><label>18</label><element-citation publication-type="journal" id="sbref0018"><person-group person-group-type="author"><name><surname>Larson</surname><given-names>M.J.</given-names></name><name><surname>Carbine</surname><given-names>K.A.</given-names></name></person-group><article-title>Sample size calculations in human electrophysiology (EEG and ERP) studies: a systematic review and recommendations for increased rigor</article-title><source>Int. J. Psychophysiol.</source><volume>111</volume><year>2017</year><fpage>33</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.ijpsycho.2016.06.015</pub-id><pub-id pub-id-type="pmid">27373837</pub-id>
</element-citation></ref><ref id="bib0019"><label>19</label><element-citation publication-type="journal" id="sbref0019"><person-group person-group-type="author"><name><surname>Koelstra</surname><given-names>S.</given-names></name><name><surname>Muhl</surname><given-names>C.</given-names></name><name><surname>Soleymani</surname><given-names>M.</given-names></name><name><surname>Lee</surname><given-names>J.-S.</given-names></name><name><surname>Yazdani</surname><given-names>A.</given-names></name><name><surname>Ebrahimi</surname><given-names>T.</given-names></name><name><surname>Pun</surname><given-names>T.</given-names></name><name><surname>Nijholt</surname><given-names>A.</given-names></name><name><surname>Patras</surname><given-names>I.</given-names></name></person-group><article-title>DEAP: a database for emotion analysis&#x000a0;;using physiological signals</article-title><source>IEEE Trans. Affect. Comput.</source><volume>3</volume><year>2012</year><fpage>18</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1109/T-AFFC.2011.15</pub-id></element-citation></ref><ref id="bib0020"><label>20</label><element-citation publication-type="journal" id="sbref0020"><person-group person-group-type="author"><name><surname>Katsigiannis</surname><given-names>S.</given-names></name><name><surname>Ramzan</surname><given-names>N.</given-names></name></person-group><article-title>DREAMER: a database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices</article-title><source>IEEE J. Biomed. Health Inf.</source><volume>22</volume><year>2018</year><fpage>98</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2017.2688239</pub-id></element-citation></ref><ref id="bib0021"><label>21</label><element-citation publication-type="journal" id="sbref0021"><person-group person-group-type="author"><name><surname>Soleymani</surname><given-names>M.</given-names></name><name><surname>Lichtenauer</surname><given-names>J.</given-names></name><name><surname>Pun</surname><given-names>T.</given-names></name><name><surname>Pantic</surname><given-names>M.</given-names></name></person-group><article-title>A multimodal database for affect recognition and implicit tagging</article-title><source>IEEE Trans. Affect. Comput.</source><volume>3</volume><year>2012</year><fpage>42</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1109/T-AFFC.2011.25</pub-id></element-citation></ref><ref id="bib0022"><label>22</label><element-citation publication-type="journal" id="sbref0022"><person-group person-group-type="author"><name><surname>Lotte</surname><given-names>F.</given-names></name><name><surname>Bougrain</surname><given-names>L.</given-names></name><name><surname>Cichocki</surname><given-names>A.</given-names></name><name><surname>Clerc</surname><given-names>M.</given-names></name><name><surname>Congedo</surname><given-names>M.</given-names></name><name><surname>Rakotomamonjy</surname><given-names>A.</given-names></name><name><surname>Yger</surname><given-names>F.</given-names></name></person-group><article-title>A review of classification algorithms for EEG-based brain&#x02013;computer interfaces: a 10 year update</article-title><source>J. Neural Eng.</source><volume>15</volume><year>2018</year><object-id pub-id-type="publisher-id">031005</object-id><pub-id pub-id-type="doi">10.1088/1741-2552/aab2f2</pub-id></element-citation></ref><ref id="bib0023"><label>23</label><element-citation publication-type="book" id="sbref0023"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I.</given-names></name><name><surname>Pouget-Abadie</surname><given-names>J.</given-names></name><name><surname>Mirza</surname><given-names>M.</given-names></name><name><surname>Xu</surname><given-names>B.</given-names></name><name><surname>Warde-Farley</surname><given-names>D.</given-names></name><name><surname>Ozair</surname><given-names>S.</given-names></name><name><surname>Courville</surname><given-names>A.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><part-title>Generative adversarial nets</part-title><source>Advances in Neural Information Processing Systems</source><year>2014</year><publisher-name>Curran Associates, Inc.</publisher-name><ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html" id="interref0006">https://proceedings.neurips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html</ext-link><comment>(accessed September 28, 2023)</comment></element-citation></ref><ref id="bib0024"><label>24</label><element-citation publication-type="book" id="sbref0024"><person-group person-group-type="author"><name><surname>Arjovsky</surname><given-names>M.</given-names></name><name><surname>Chintala</surname><given-names>S.</given-names></name><name><surname>Bottou</surname><given-names>L.</given-names></name></person-group><part-title>Wasserstein generative adversarial networks</part-title><source>Proceedings of the 34th International Conference on Machine Learning, PMLR</source><year>2017</year><fpage>214</fpage><lpage>223</lpage><ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v70/arjovsky17a.html" id="interref0007">https://proceedings.mlr.press/v70/arjovsky17a.html</ext-link><comment>(accessed September 28, 2023)</comment></element-citation></ref><ref id="bib0025"><label>25</label><element-citation publication-type="book" id="sbref0025"><person-group person-group-type="author"><name><surname>Gulrajani</surname><given-names>I.</given-names></name><name><surname>Ahmed</surname><given-names>F.</given-names></name><name><surname>Arjovsky</surname><given-names>M.</given-names></name><name><surname>Dumoulin</surname><given-names>V.</given-names></name><name><surname>Courville</surname><given-names>A.C.</given-names></name></person-group><part-title>Improved training of Wasserstein GANs</part-title><source>Advances in Neural Information Processing Systems</source><year>2017</year><publisher-name>Curran Associates, Inc.</publisher-name><ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper_files/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html" id="interref0008">https://proceedings.neurips.cc/paper_files/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html</ext-link><comment>(accessed September 28, 2023)</comment></element-citation></ref><ref id="bib0026"><label>26</label><mixed-citation publication-type="other" id="sbref0026">X. Li, V. Metsis, H. Wang, A.H.H. Ngu, TTS-GAN: A Transformer-based time-series generative adversarial network, (2022). <pub-id pub-id-type="doi">10.48550/arXiv.2202.02691</pub-id>.</mixed-citation></ref><ref id="bib0027"><label>27</label><mixed-citation publication-type="other" id="sbref0027">M. Mirza, S. Osindero, Conditional generative adversarial nets, (2014). <pub-id pub-id-type="doi">10.48550/arXiv.1411.1784</pub-id>.</mixed-citation></ref></ref-list><sec sec-type="data-availability" id="refdata001"><title>Data Availability</title><p id="para9001">
<list list-type="simple" id="dacelist0001"><list-item id="rdlistitem0001"><p id="para9002"><ext-link ext-link-type="uri" xlink:href="https://data.mendeley.com/datasets/w9nk4mvgbb/3" id="interref0002">EEG Dataset Collected During Energy Data Visualization Stimuli Presentation (EDAVIS) (Original data)</ext-link> (Mendeley Data).</p></list-item></list>
</p></sec><ack id="ack0001"><title>Acknowledgements</title><p id="para0061">Authors would like to thank all the participants who contributed to the collection of data. This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.</p><sec id="coi0001"><title>Declaration of Competing Interest</title><p id="para0060">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></sec></ack></back></article>
