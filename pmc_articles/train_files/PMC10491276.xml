<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">medRxiv</journal-id><journal-id journal-id-type="publisher-id">MEDRXIV</journal-id><journal-title-group><journal-title>medRxiv</journal-title></journal-title-group><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">37693571</article-id><article-id pub-id-type="pmc">PMC10491276</article-id>
<article-id pub-id-type="doi">10.1101/2023.08.25.23294636</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Patient Phenotyping for Atopic Dermatitis with Transformers and Machine Learning</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0009-0003-1310-0107</contrib-id><name><surname>Wang</surname><given-names>Andrew</given-names></name><degrees>BS</degrees><xref rid="A1" ref-type="aff">1</xref><xref rid="FN1" ref-type="author-notes">+</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-9229-7259</contrib-id><name><surname>Fulton</surname><given-names>Rachel</given-names></name><degrees>MD</degrees><xref rid="A2" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3851-9521</contrib-id><name><surname>Hwang</surname><given-names>Sy</given-names></name><degrees>MS MS</degrees><xref rid="A3" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0506-8085</contrib-id><name><surname>Margolis</surname><given-names>David J.</given-names></name><degrees>MD PhD</degrees><xref rid="A4" ref-type="aff">4</xref><xref rid="A5" ref-type="aff">5</xref><xref rid="FN2" ref-type="author-notes">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3802-4457</contrib-id><name><surname>Mowery</surname><given-names>Danielle L.</given-names></name><degrees>PhD MS MS FAMIA</degrees><xref rid="A3" ref-type="aff">3</xref><xref rid="A4" ref-type="aff">4</xref><xref rid="FN2" ref-type="author-notes">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Computer and Information Science, School of Engineering and Applied Sciences, University of Pennsylvania, Philadelphia, PA</aff><aff id="A2"><label>2</label>Lankenau Medical Center, Dermatology Services, Wynnewood, PA</aff><aff id="A3"><label>3</label>Institute for Biomedical Informatics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA</aff><aff id="A4"><label>4</label>Department of Dermatology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA</aff><aff id="A5"><label>5</label>Department of Biostatistics, Epidemiology &#x00026; Informatics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA</aff><author-notes><fn id="FN1"><label>+</label><p id="P1">First author</p></fn><fn id="FN2"><label>*</label><p id="P2">Co-last authors</p></fn><corresp id="CR1"><bold>Corresponding Author:</bold> Danielle L. Mowery PhD MS MS, University of Pennsylvania, A206 Richards Building, 3700 Hamilton Walk, Philadelphia, PA 19104, <email>dlmowery@pennmedicine.upenn.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>04</day><month>12</month><year>2023</year></pub-date><elocation-id>2023.08.25.23294636</elocation-id><permissions><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</ext-link>, which allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator. The license allows for commercial use.</license-p></license></permissions><self-uri content-type="pdf">nihpp-2023.08.25.23294636.pdf</self-uri><abstract id="ABS1"><sec id="S1"><title>Background:</title><p id="P3">Atopic dermatitis (AD) is a chronic skin condition that millions of people around the world live with each day. Performing research studies into identifying the causes and treatment for this disease has great potential to provide benefit for these individuals. However, AD clinical trial recruitment is a non-trivial task due to variance in diagnostic precision and phenotypic definitions leveraged by different clinicians as well as time spent finding, recruiting, and enrolling patients by clinicians to become study subjects. Thus, there is a need for automatic and effective patient phenotyping for cohort recruitment.</p></sec><sec id="S2"><title>Objective:</title><p id="P4">Our study aims to present an approach for identifying patients whose electronic health records suggest that they may have AD.</p></sec><sec id="S3"><title>Methods:</title><p id="P5">We created a vectorized representation of each patient and trained various supervised machine learning methods to classify when a patient has AD. Each patient is represented by a vector of either probabilities or binary values where each value indicates whether they meet a different criteria for AD diagnosis.</p></sec><sec id="S4"><title>Results:</title><p id="P6">The most accurate AD classifier performed with a class-balanced accuracy of 0.8036, a precision of 0.8400, and a recall of 0.7500 when using XGBoost (Extreme Gradient Boosting).</p></sec><sec id="S5"><title>Conclusions:</title><p id="P7">Creating an automated approach for identifying patient cohorts has the potential to accelerate, standardize, and automate the process of patient recruitment for AD studies; therefore, reducing clinician burden and informing knowledge discovery of better treatment options for AD.</p></sec></abstract><kwd-group><kwd>patient phenotyping</kwd><kwd>atopic dermatitis</kwd><kwd>machine learning</kwd><kwd>natural language processing</kwd></kwd-group></article-meta></front><body><sec id="S6"><title>Introduction</title><sec id="S7"><title>Background</title><p id="P8">Atopic dermatitis (AD) is a common skin disease with a population prevalence of approximately 30% [<xref rid="R1" ref-type="bibr">1</xref>]. It is often diagnosed in early childhood, but onset can occur at any age [<xref rid="R2" ref-type="bibr">2</xref>&#x02013;<xref rid="R5" ref-type="bibr">5</xref>]. Symptoms of AD include inflamed, red, irritated, and itchy skin and can cause significant physical and emotional distress. AD is often associated with other allergic illnesses including asthma, seasonal allergies, and food allergies [<xref rid="R2" ref-type="bibr">2</xref>,<xref rid="R3" ref-type="bibr">3</xref>,<xref rid="R5" ref-type="bibr">5</xref>&#x02013;<xref rid="R7" ref-type="bibr">7</xref>].</p><p id="P9">AD is thought to be associated with skin barrier dysfunction and immune dysregulation [<xref rid="R5" ref-type="bibr">5</xref>]. AD has also been associated with genetic variation as well as environmental factors [<xref rid="R5" ref-type="bibr">5</xref>]. Classic treatment for AD has included the use of moisturizers, topical steroids, and other topical anti-inflammatory agents [<xref rid="R8" ref-type="bibr">8</xref>]. However, in the past few years, there have been significant treatment advances, which include systemic agents that alter immune function such as dupilumab. Therefore, due to the widespread nature of AD, the need for improved knowledge of the natural history of AD, the need to understand the efficacy of new treatments, and the need to develop new treatments, there is an urgent need to understand the clinical course of individuals with AD. However, identifying appropriate cohorts of patients for medical studies can be difficult and time consuming. Because AD is so common as well as diagnosed and managed by many different clinicians in varying healthcare settings, a potential source population would be patients from a health system&#x02019;s electronic health records (EHRs) [<xref rid="R9" ref-type="bibr">9</xref>]. Investigators often ascertain a patient&#x02019;s illness using International Classification of Disease (ICD) hospital billing codes as recorded during routine office visits. However, it has been previously demonstrated that reliance on ICD codes is not an accurate method for the ascertainment of AD study cohorts [<xref rid="R9" ref-type="bibr">9</xref>,<xref rid="R10" ref-type="bibr">10</xref>]. Furthermore, epidemiologic studies have used different methods and algorithms including the UK Working Party (UKWP) diagnostic criteria and the Hanifin and Rajka (HR) criteria [<xref rid="R11" ref-type="bibr">11</xref>,<xref rid="R12" ref-type="bibr">12</xref>]. Investigators attempting to conduct clinical trials and observational studies have also relied on manual, large-scale chart review, a process that is inefficient, slow, and tedious [<xref rid="R9" ref-type="bibr">9</xref>]. This motivates the need for a standard method to accurately, automatically, and efficiently identify potential patient cohorts from their text medical records via natural language processing (NLP) and machine learning (ML) techniques.</p></sec><sec id="S8"><title>Prior Work</title><p id="P10">Previously, researchers have aimed to phenotype patients with AD using EHR data. In particular, Gustafson et al. trained a logistic regression model with lasso regularization to identify cases of AD from the Northwestern Medical Enterprise Data Warehouse (NMEDW) which contained both structured data (ICD-9/ICD-10 codes, medication prescriptions, and lab results), as well as unstructured data (clinician notes from patient encounters) [<xref rid="R10" ref-type="bibr">10</xref>]. A gold standard diagnosis was assigned to each patient in their dataset by two rheumatologists following a chart review when using the UK Working Party (UKWP) criteria, and (alternatively) when using the Hanifin and Rajka (HR) criteria.</p><p id="P11">Although similar, our work differs in the following ways: 1) we survey a wide range of supervised machine learning algorithms as opposed to only using lasso regularized logistic regression, 2) we use transformer embeddings of sentences to represent information in each patient&#x02019;s records and aggregate these embeddings with MLP networks to create a patient vector representation for patient phenotyping, and 3) we performed an ablation study of processing methods to compare the impact on performance in using a probability-based vs binary label of whether each patient meets various AD diagnostic criteria when creating a vector to represent each patient for input to our final AD patient phenotyping algorithms.</p></sec><sec id="S9"><title>Contributions</title><p id="P12">The primary contributions of our paper are as follows:</p><list list-type="bullet" id="L1"><list-item><p id="P13">We introduce and validate a rules-based approach for aggregating information from patient electronic health record (EHR) data to generate binary-valued patient vectors that are used with standard ML algorithms for patient phenotyping</p></list-item><list-item><p id="P14">We introduce and validate a transformer-based approach for aggregating information and patient phenotyping by using BERT models (BERT Base Uncased and BioClinical BERT) to generate patient vectors of probabilities, which are used with standard ML algorithms for patient phenotyping.</p></list-item><list-item><p id="P15">We compare the aforementioned approaches to 1) discern whether a transformer model pretrained on clinical text can provide performance benefits over a transformer model not pretrained on clinical text, and to 2) discern whether a transformer-based approach for aggregating information could outperform a rules-based approach for aggregating information.</p></list-item><list-item><p id="P16">We demonstrate that multi-layer perceptron (MLP) networks can be used with BERT sentence embeddings to identify which sentences in patient records are relevant to the diagnosis of atopic dermatitis. These MLP networks can then be used during clinician chart review to highlight sentences that are relevant to diagnosis and therefore accelerate the process of chart review during clinical trial recruitment.</p></list-item></list></sec></sec><sec id="S10"><title>Methods</title><sec id="S11"><title>Overview</title><p id="P17">To predict whether a patient may qualify as a subject for an AD study based on their electronic health record, we first assigned patients in our dataset to either the training or testing sets. Then, for each patient, we aggregated the text from their EHR and constructed a vector representation of clinical features indicative of AD according to the UKWP criteria. Lastly, we leveraged our vectorized patient representations to train several machine learning classifiers to predict whether each patient has AD. In the following sections, we detail this process.</p></sec><sec id="S12"><title>Dataset Creation</title><p id="P18">We initially sampled 2,000 patients and their clinical records from Epic Clarity, Penn Medicine&#x02019;s EHR database. We selected Penn Medicine patients who were diagnosed with a subset of AD-related ICD codes [<xref rid="R9" ref-type="bibr">9</xref>]. Of the 2,000 sampled patients, we identified 1,926 patients who had clinical notes for processing. We then de-identified these patient records according to the Safe Harbor method using PHIlter [<xref rid="R13" ref-type="bibr">13</xref>]. Each patient in the dataset was also manually reviewed and labeled according to the UK Working Party (UKWP) diagnostic criteria for AD. According to the UKWP criteria, in order to qualify as having AD, a patient must have an itchy skin condition along with 3 or more of the following: a history of flexural involvement, a history of asthma or hay fever, a history of dry skin, an onset of rash under the age of 2 years, or a visible flexural dermatitis. Our dataset was validated by two clinicians (a board-certified dermatologist (DJM) and a medical fellow (RF)), resulting in 137 patients with AD and 1,789 patients without AD.</p></sec><sec id="S13"><title>Training and Testing Split</title><p id="P19">We first created our training set. Due to the heavy class imbalance in our dataset, we decided to create a balanced training set to prevent biasing the model towards either AD or non-AD patients. In particular, we created the training set by assigning 80% of the 137 patients with AD to our training set, and under sampling from the non-AD patients to match the number of patients with AD. The remaining 20% of the 137 patients were assigned to both of our testing sets. This resulted in a training set that had 109 patients with AD, and 109 patients without AD.</p><p id="P20">Next, we created two testing sets. The first testing set was class-balanced and was intended to show how our patient classification model can generalize to unseen samples if the class distribution was kept the same. The second testing set was class-imbalanced (30% of patients with AD and 70% of patients without AD), and was intended to show how our patient classification model can perform when the class-distribution of the dataset matches the prevalence of AD in the United States.</p><p id="P21">We created the first (balanced) testing set by including the 20% (previously reserved for testing) of the 137 patients with AD, and combining them with an equal number of patients without AD who have not been used during training. This resulted in a (balanced) testing set that had 28 patients with AD and 28 patients without AD.</p><p id="P22">Furthermore, we created the second (unbalanced) testing set by including the same 20% of the 137 patients with AD, but instead combining them with a greater number of patients without AD to match the 30% prevalence rate of AD found in the US [<xref rid="R1" ref-type="bibr">1</xref>]. This resulted in a (unbalanced) testing set with 28 patients who have AD and 63 patients who don&#x02019;t have AD.</p><p id="P23">We chose not to create a separate hyperparameter tuning set and instead applied cross validation for hyperparameter tuning on the training set due to the data scarce setting of our experiments.</p></sec><sec id="S14"><title>Vector Representation for AD Classification</title><p id="P24">Next, we created a vector representation for each patient. We performed 3 experiments to compare different methods of creating each patient&#x02019;s vector representation (<xref rid="F2" ref-type="fig">Figure 2</xref>).</p><sec id="S15"><title>Description of Patient Vector Representation</title><p id="P25">Each patient&#x02019;s vector representation is 8 elements long, where each element of the vector is representative of whether the patient fulfills a different AD diagnosis criteria based on the UKWP criteria as well as clinician feedback (<xref rid="T1" ref-type="table">Table 1</xref>). Across all three experiments, each element in the patient vector corresponds to a distinct classification task; however in experiments 1 and 2 each element is a probability, and in experiment 3 each element is a binary value.</p><p id="P26">In experiments 1 and 2, elements 1 through 8 of each patient&#x02019;s vector represent the highest probability that any sentence in the patient&#x02019;s EHR mentions 1) AD or synonyms of AD, 2) keywords that suggest hay fever allergies, 3) keywords that suggest atopic allergies, 4) keywords that suggest eczema or rashes, 5) keywords that indicate dry or itchy skin, 6) keywords denoting non-asthma medications, 7) keywords suggesting the presence of asthma, and 8) keywords indicating the use of asthma medications.</p><p id="P27">In experiment 3, instead of each element representing a probability, each element represents a binary value of whether there was at least 1 sentence in the corresponding patient record suggesting the presence of the corresponding AD indicator.</p><p id="P28">In the first two experiments, each patient&#x02019;s vector elements represent probabilities (ranging from 0 to 1). Each probability value is derived from a distinct MLP classifier. Experiments 1 and 2 were performed to compare the use of two BERT models (BERT Base Uncased [<xref rid="R14" ref-type="bibr">14</xref>,<xref rid="R15" ref-type="bibr">15</xref>] in experiment 1, and BioClinical BERT [<xref rid="R16" ref-type="bibr">16</xref>,<xref rid="R17" ref-type="bibr">17</xref>] in experiment 2) for creating sentence embeddings used to train multi-layer perceptron (MLP) networks (or alternatively, sentence classifiers). A separate MLP network is trained for each element of the patient vector. Each MLP network is trained to distinguish sentences in one of the 8 AD indicator categories from sentences in all other categories. Furthermore, medSpacy was used to split documents into sentences and label each sentence with different categories. After each sentence classifier is trained, embeddings of all sentences in each patient&#x02019;s full EHR are passed through each sentence classifier and an aggregation function (max operator) is used to assign a value to each element of each patient&#x02019;s vector. Our goal in experiments 1 and 2, was to test the hypothesis that a BERT model pretrained on clinical text (BioClinical BERT) could outperform a BERT model trained on non-clinical text (BERT Base Uncased).</p><p id="P29">In experiment 3, each patient&#x02019;s vector elements are binary (either 0 or 1). Each element corresponds to a diagnostic criteria and represents whether medSpacy was able to identify at least 1 sentence in the patient&#x02019;s record with a keyword and affirming context that suggests the patient meets the corresponding diagnostic criteria. Our goal was to conduct an ablation study to test the hypothesis that an AD patient classifier leveraging BERT embeddings to create the patient vector representation will better discern whether a patient has AD than an AD patient classifier without BERT embeddings.</p></sec><sec id="S16"><title>Preprocessing for Experiments 1&#x02013;3</title><p id="P30">Before each experiment, we applied the same preprocessing steps to assign one or more labels to each sentence in our corpus of documents in both our training and testing sets. Each sentence can be labeled as applying to one, multiple, or none of the 8 AD indicators previously defined.</p><p id="P31">For each of the 8 diagnostic criteria, we first created a list of keywords and phrases (for each vector element) that suggested the presence of the corresponding diagnostic criteria. Next, we used medSpacy with the ConText algorithm to split each document into sentences and categorize each sentence [<xref rid="R18" ref-type="bibr">18</xref>]. Using medSpacy allows us to obtain sentences that suggest the presence of each of the 8 diagnostic criteria due to medSpacy&#x02019;s use of regex and rules-based keyword matching. Furthermore, medSpacy&#x02019;s implementation of the ConText algorithm allows us to discern between sentences that affirm from negated assertions. We define negated sentences for each AD indicator as sentences where the indicator is ruled out, sentences where the indicator is experienced by someone other than the patient, and sentences where the existence of the indicator is hypothetical [<xref rid="R19" ref-type="bibr">19</xref>&#x02013;<xref rid="R22" ref-type="bibr">22</xref>].</p><p id="P32">After assigning one or more categorical labels to each sentence with medSpacy, we then performed 3 different experiments to create a vectorized representation of each patient.</p><p id="P33">In <xref rid="T2" ref-type="table">tables 2</xref> and <xref rid="T3" ref-type="table">3</xref> below, we include some statistics on the dataset obtained after preprocessing.</p><p id="P34">As shown in <xref rid="T2" ref-type="table">table 2</xref>, AD patients have approximately twice as many sentences as non-AD patients. The average number of documents and sentences are the same (within patients who have AD, and similarly within non-AD patients) between BERT Base Uncased and BioClinical BERT experiments because these values are only dependent on medSpacy&#x02019;s preprocessing of documents. Furthermore, using BioClinical BERT to tokenize sentences tends to yield more tokens (on average) per patient and per document. We hypothesize this is because the BioClinical BERT tokenizer is able to recognize more clinical terms and therefore yields more tokens for the same sentence than using the tokenizer from BERT Base Uncased.</p><p id="P35">As shown in <xref rid="T3" ref-type="table">table 3</xref>, sentences in category 5 (relating to dry or itchy skin) tend to have the most tokens, whereas sentences in category 6 (relating to the use of non-asthma medications related to treating AD) tend to have the least number of tokens. We hypothesize that this is because categories where the average number of tokens per sentence is greater tend to correspond to more general categories where many terms and sentences could apply, whereas categories where the average number of tokens per sentence is lower tend to correspond to more specific categories thus yielding a lower average number of tokens per sentence. Additionally, similarly to before, we can see that using BioClinical BERT tends to result in a greater number of tokens per sentence than using BERT Base Uncased for the same sentence.</p></sec><sec id="S17"><title>Experiments 1 and 2 &#x02013; Patient Vector Construction with BERT Embeddings</title><p id="P36">In experiments 1 and 2, we first used the sentences medSpacy identified in each category to create class-balanced training and testing sets for each MLP network classifier, as shown in <xref rid="T4" ref-type="table">table 4</xref>. The same training and testing set was used for both experiment 1 (BioClinical BERT) and experiment 2 (BERT Base Uncased).</p><p id="P37">Next, we used pretrained BERT models to generate embeddings of the sentences in each classifier&#x02019;s training and testing set. We incorporated pretrained BERT models because these models have been trained on a much larger corpus than our existing dataset, and BERT provides a context sensitive embedding of text which other techniques such as bag of words don&#x02019;t provide. Furthermore, we used BERT Base Uncased in experiment 1, and Alsentzer et. al&#x02019;s BioClinical BERT in experiment 2 because we wanted to quantify how much of a difference in performance that using a model pretrained on clinical text can provide over a model that has not been pretrained on clinical text.</p><p id="P38">Using these embeddings, we trained a multi-layer perceptron (MLP) network to distinguish sentence embeddings in each category from sentence embeddings that aren&#x02019;t in the corresponding category. Each of our MLP&#x02019;s were trained with the following architecture: a fully connected input layer of shape 768 by 100, followed by a ReLU (Rectified Linear Unit) activation, further followed by a fully connected output layer of shape 100 by 2. We trained each of our MLP&#x02019;s for 10 epochs with the cross-entropy loss function, the stochastic gradient descent (SGD) optimizer, a learning rate of 0.001, and a momentum value of 0.9. The final layer of each MLP can then be used to obtain the probability that any given sentence embedding comes from the category for which the MLP is being trained by passing the logits of the final layer to the softmax function.</p><p id="P39">We used the Rectified Linear Unit activation function as defined below, where <inline-formula><mml:math id="M1" display="inline"><mml:mi mathvariant="normal">x</mml:mi></mml:math></inline-formula> is the input to the <inline-formula><mml:math id="M2" display="inline"><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">U</mml:mi></mml:math></inline-formula> function:
<disp-formula id="FD1">
<mml:math id="M3" display="block"><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math>
</disp-formula></p><p id="P40">We also used the softmax function as defined below, where e is the standard exponential function, <inline-formula><mml:math id="M4" display="inline"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is element at index <inline-formula><mml:math id="M5" display="inline"><mml:mi mathvariant="normal">i</mml:mi></mml:math></inline-formula> of the <inline-formula><mml:math id="M6" display="inline"><mml:mi mathvariant="normal">K</mml:mi></mml:math></inline-formula> element long input vector <inline-formula><mml:math id="M7" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:math></inline-formula>.</p><disp-formula id="FD2">
<mml:math id="M8" display="block"><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math>
</disp-formula><p id="P41">We chose to embed our sentences once with pretrained BERT models, and then feed these saved embeddings to our MLP networks as opposed to adding a classification head (a linear layer) to the end of our pretrained BERT models. Although doing so only allows us to fine tune the weights in our MLP network (as opposed to also fine tuning the weights BERT uses to embed the sentences), doing so allows us to iterate over different experiments more quickly and with less computational power. In particular, we are able to 1) avoid the large computational expense of gradient calculations during backpropagation for all 12 layers of transformers used by BERT when fine tuning the model, 2) avoid the computational expense of repeatedly generating the same embeddings from BERT multiple times (if we chose to freeze the weights of BERT and only fine tune an added classification head/linear layer), and 3) iterate more efficiently over different hyperparameter combinations across different experiments with our MLP networks.</p><p id="P42">After training a separate MLP network for each of the 8 categories, we generated a vector representation for each patient where each of the 8 vector elements represents the highest probability that any given sentence in the patient record affirms the presence of the corresponding AD indicator. We accomplished this by iterating through all sentences in each patient&#x02019;s full EHR and passing the sentence embedding through each of our 8 trained MLP networks to obtain 8 probabilities for each sentence corresponding to the probability that the sentence affirms each of the 8 AD indicators we previously selected. Then, for each patient and for each AD indicator, we kept the highest probability that any given sentence in the patient&#x02019;s record affirms the presence of the AD indicator.</p></sec><sec id="S18"><title>Experiment 3 &#x02013; Patient Vector Construction without BERT Embeddings</title><p id="P43">In experiment 3, we generated each patient&#x02019;s vector representation by assigning a 1 to each element of the patient vector if medSpacy with the ConText algorithm identified at least 1 sentence in the patient&#x02019;s record that affirms or suggests the presence of the AD indicator for which the vector element corresponds to. Experiment 3 was conducted as an ablation study to quantify the performance benefit (if at all) of using contextual BERT text embeddings to generate probability scores that the patient meets various AD indicators.</p></sec></sec><sec id="S19"><title>AD Phenotyping with Vector Representations</title><p id="P44">In all three experiments, after generating a vector representation for each patient, we collated each patient vector representation with the corresponding label our clinicians assigned the patient when validating the dataset. Then, we fed the vector patient representation and corresponding patient label through a variety of classification algorithms. These include logistic regression, support vector machines (SVM), decision trees, random forests, K nearest neighbors (KNN), Extreme Gradient Boosting (XGBoost), and Adaptive Boosting (AdaBoost). During training for each of the previously mentioned classifiers, we used 5-fold cross validation to determine the best set of hyperparameters to use (as opposed to creating a separate validation set) due to the data scarce setting of our experiments. We then used the selected hyperparameters to train each algorithm on the entire training set and evaluated performance on the unbalanced and balanced testing sets. In addition to using the previously mentioned classifiers, we also used the stacking algorithm provided by scikit-learn to obtain an ensemble prediction from the different classifiers [<xref rid="R23" ref-type="bibr">23</xref>]. To quantify performance, we calculated the accuracy, precision, recall, F1-score, negative predictive value (NPV), and specificity of each algorithm on both testing sets.</p><p id="P45">We define accuracy, precision, and recall as follows, where <inline-formula><mml:math id="M9" display="inline"><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:math></inline-formula> is the number of true positives, <inline-formula><mml:math id="M10" display="inline"><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:math></inline-formula> is the number of true negatives, <inline-formula><mml:math id="M11" display="inline"><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:math></inline-formula> is the number of false positives, and <inline-formula><mml:math id="M12" display="inline"><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:math></inline-formula> is the number of false negatives:
<disp-formula id="FD3">
<mml:math id="M13" display="block"><mml:mi mathvariant="italic">Accuracy</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math>
</disp-formula>
<disp-formula id="FD4">
<mml:math id="M14" display="block"><mml:mi mathvariant="italic">Precision</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math>
</disp-formula>
<disp-formula id="FD5">
<mml:math id="M15" display="block"><mml:mi mathvariant="italic">Recall</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math>
</disp-formula></p><p id="P46">Additionally, we define the <inline-formula><mml:math id="M16" display="inline"><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn></mml:math></inline-formula>-score, <inline-formula><mml:math id="M17" display="inline"><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:math></inline-formula>, and specificity as follows:
<disp-formula id="FD6">
<mml:math id="M18" display="block"><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">*</mml:mi><mml:mi mathvariant="italic">Precision</mml:mi><mml:mi mathvariant="normal">*</mml:mi><mml:mi mathvariant="italic">Recall</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Precision</mml:mi><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="italic">Recall</mml:mi></mml:mrow></mml:mfrac></mml:math>
</disp-formula>
<disp-formula id="FD7">
<mml:math id="M19" display="block"><mml:mi>N</mml:mi><mml:mi>P</mml:mi><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math>
</disp-formula>
<disp-formula id="FD8">
<mml:math id="M20" display="block"><mml:mi mathvariant="italic">Specificity</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math>
</disp-formula></p></sec></sec><sec id="S20"><title>Results</title><sec id="S21"><title>Performance of MLP Networks</title><p id="P47">In this section, we compare the performance of several MLP classifiers in distinguishing sentences relevant to diagnosis of AD. This corresponds to the &#x0201c;Train separate MLP network (sentence classifier)&#x0201d; box in <xref rid="F2" ref-type="fig">Figure 2</xref>.</p><p id="P48">As part of our AD Phenotyping pipeline, we trained various MLP networks to classify when a given sentence embedding indicates the presence of an AD indicator, and we compared performance of BioClinical BERT embeddings to BERT Base Uncased embeddings when training these MLP networks. In both cases, the classifier with the highest accuracy was the classifier for category 1 (sentences with direct mentions of AD). The classifiers with the two lowest accuracies were either the classifier for category 5 (sentences with mentions of dry or itchy skin) or the classifier for category 7 (sentences with mentions of asthma) for both the use of BioClinical BERT embeddings and the use of BERT Base Uncased embeddings. However, the accuracy in classifier 7 was lower when using BERT Base Uncased embeddings than when using BioClinical BERT embeddings.</p><p id="P49">In experiment 1, the accuracies across AD indicator classifiers ranged from 0.7373 (classifier 5) to 0.9002 (classifier 1) as shown in <xref rid="T5" ref-type="table">table 5</xref> below.</p><p id="P50">In experiment 2, the accuracies across AD indicator classifiers ranged from 0.7269 (classifier 7) to 0.9153 (classifier 1) as shown in <xref rid="T6" ref-type="table">table 6</xref> below.</p></sec><sec id="S22"><title>AD Phenotyping with Patient Vector Representations</title><p id="P51">In this section, we compare performance in patient classification when using different methods for creating patient vector representations. This encompasses all three experiments and corresponds to the &#x0201c;Use vector patient representations to classify whether patient has AD&#x0201d; box in <xref rid="F2" ref-type="fig">Figure 2</xref>.</p><p id="P52">In experiment 1, we leveraged BioClinical BERT sentence embeddings to train various MLP networks to discern sentence embeddings in different AD indicator categories. Then, we applied these trained MLP networks (sentence classifiers) along with an aggregation function (max operator) to assign values to each element of each patient&#x02019;s vector representation. Lastly, we used each patient&#x02019;s vector representation with their validated label to train various ML algorithms. We evaluated on both a balanced and unbalanced testing set.</p><p id="P53">As shown in <xref rid="T7" ref-type="table">table 7</xref>, the accuracy on the balanced testing set ranges from 0.5893 (Decision Tree) to 0.7321 (Logistic Regression and SVM).</p><p id="P54">As shown in <xref rid="T8" ref-type="table">table 8</xref>, the range of accuracies on the unbalanced testing set is slightly lower, ranging from 0.5824 (Decision Tree) to 0.7253 (Stacking Classifier).</p><p id="P55">In experiment 2, we followed the same process as in experiment 1; however, we used BERT Base Uncased instead of BioClinical BERT. As shown in <xref rid="T9" ref-type="table">table 9</xref>, the accuracy of our AD patient classifiers on the balanced testing set ranges from 0.5179 (AdaBoost) to 0.6250 (Random Forest).</p><p id="P56">As shown in <xref rid="T10" ref-type="table">table 10</xref>, the range of accuracies of our AD patient classifiers on the unbalanced testing set is slightly higher, ranging from 0.5714 (Logistic Regression and SVM) to 0.6703 (Random Forest).</p><p id="P57">In experiment 3, we performed an ablation study and assigned binary labels to the elements of each patient&#x02019;s vector based on whether medSpacy was able to identify at least one sentence in each of the AD indicator categories that each vector element corresponds to. As shown in <xref rid="T11" ref-type="table">table 11</xref>, the accuracy across our AD patient classifiers on the balanced testing set ranges from 0.6964 (KNN) to 0.8036 (XGBoost).</p><p id="P58">As shown in <xref rid="T12" ref-type="table">table 12</xref>, the lower bound of the range of accuracies across our AD patient classifiers on the unbalanced testing set is higher and the upper bound of the accuracies is lower. The accuracies on the unbalanced testing set ranges from 0.7143 (Stacking Classifier) to 0.7582 (Random Forest and Stacking Classifier).</p></sec></sec><sec id="S23"><title>Discussion</title><sec id="S24"><title>Sentence Classification Results</title><p id="P59">We hypothesized that using BioClinical BERT sentence embeddings to train sentence classifiers would provide better performance than using BERT Base Uncased sentence embeddings due to the clinical setting of our data. Given the results in <xref rid="T5" ref-type="table">tables 5</xref> and <xref rid="T6" ref-type="table">6</xref>, we observed that this was most often true in the context of sentence classification because we were able to achieve better performance in the majority (5 out of 8) of the sentence classification tasks when using BioClinical BERT embeddings as opposed to BERT Base Uncased embeddings.</p><p id="P60">Using BioClinical BERT sentence embeddings yielded stronger performance when distinguishing sentences in 5 of the 8 sentence categories &#x02013; category 2 (mentions of hay fever allergies), category 3 (mentions of atopic allergies), category 5 (mentions of dry or itchy skin), category 6 (mentions of non-asthma medications), and category 7 (mentions of asthma). More specifically, we observed higher accuracies when using BioClinical BERT sentence embeddings for classifiers 2 (0.8954), 3 (0.8214), 5 (0.7373), 6 (0.8204), and 7 (0.7712) than their corresponding counterparts when using BERT Base Uncased embeddings for classifiers 2 (0.7730), 3 (0.7976), 5 (0.7288), 6 (0.8096), and 7 (0.7269). We observed that the differences in performance between using BioClinical BERT embeddings and BERT Base Uncased embeddings are most pronounced for classifiers 2 and 7 which correspond to mentions of hay fever allergies and asthma mentions. We hypothesize this is because hay fever allergies and asthma (and their synonyms) may be very common terms in clinical notes; therefore, models trained on clinical data (BioClinical BERT) may be able to provide stronger performance than models trained on non-clinical text (BERT Base Uncased) which may not have as many mentions of hay fever allergies or asthma.</p><p id="P61">Conversely, using BERT Base Uncased embeddings yielded stronger performance when distinguishing sentences in the other 3 of 8 sentence categories &#x02013; category 1 (direct mentions of AD), 4 (mentions of eczema or rashes), and 8 (mentions of asthma medications). More specifically, we observed higher accuracies when using BERT Base Uncased sentence embeddings for classifiers 1 (0.9153), 4 (0.8439), and 8 (0.8738) than their corresponding counterparts when using BioClinical BERT embeddings for classifiers 1 (0.9002), 4 (0.8284), and 8 (0.8299). We observed differences in performance between using BERT Base Uncased embeddings and BioClinical BERT embeddings are most evident for classifier 8 which corresponds to mentions of asthma medications. Although this is counterintuitive at first (we would expect that a classifier using embeddings generated from BioClinical BERT to be able to better recognize allergy medicines), we believe that the performance benefit from using BERT Base Uncased can be attributed to the list of terms we give to medSpacy when asking it to identify sentences in category 8. Many of the asthma medications in category 8 sentences are either monoclonal antibody medications ending in -mab (Ex: benralizumab, mepolizumab, omalizumab, etc) or hydrofluoroalkanes (Ex: atrovent hfa, flovent hfa, xopenex hfa, etc). Because monoclonal antibodies are very specialized types of medication, they may not occur as frequently as other terms in the corpus used to train BioClinical BERT so a more general model such as BERT Base Uncased may provide more robust performance. Additionally, because the hydrofluoroalkane allergy medications in category 8 sentences are often abbreviated with &#x0201c;hfa&#x0201d; which can have alternate medical meanings such as high-functioning autism or health facility administrator, the BioClinical BERT embeddings might not be representative of the presence of allergy medications in the sentence, so a more general model such as BERT Base Uncased may be able to provide better performance.</p><p id="P62">More broadly looking at the results in <xref rid="T5" ref-type="table">table 5</xref> and <xref rid="T6" ref-type="table">6</xref>, we can see that the least accurate classifier has an accuracy of 0.7288, while the most accurate classifier is able to achieve an accuracy of 0.9153. Furthermore, when aggregating the most accurate classifiers from both tables we can see that we are able to achieve accuracies of 0.9153 (classifier 1) for identifying sentences that directly suggest the patient has AD, 0.8954 (classifier 2) for identifying sentences that mention hay fever allergies, 0.8214 (classifier 3) for identifying sentences that mention atopic allergies, 0.8439 (classifier 4) for identifying sentences that mention eczema or skin rashes, 0.7373 (classifier 5) for identifying sentences that mention dry or itchy skin, 0.8204 (classifier 6) for identifying sentences that mention non-asthma medications related to diagnosis of AD, 0.7712 (classifier 7) for identifying sentences that mention asthma, and 0.8738 (classifier 8) for identifying sentences that mention asthma medications. Because our training and testing sets were both class-balanced and the majority (6 of the 8) of the most accurate classifiers previously mentioned achieve accuracies between 0.8204 and 0.9153, we believe these results are promising and indicate that our sentence classifiers could potentially be used to save time in a clinical setting during chart review by identifying (and highlighting for review) sentences relevant to diagnosis of AD when recruiting for clinical trials.</p></sec><sec id="S25"><title>AD Phenotyping Results</title><p id="P63">Per <xref rid="T7" ref-type="table">tables 7</xref> through <xref rid="T10" ref-type="table">10</xref>, our earlier hypothesis holds - using clinical embeddings (BioClinical BERT) to generate the patient vector representation does provide better performance in patient phenotyping than using non-clinical embeddings (BERT Base Uncased). Comparing evaluation on the balanced testing set in <xref rid="T7" ref-type="table">tables 7</xref> and <xref rid="T9" ref-type="table">9</xref>, we observe that using BioClinical BERT embeddings provides higher accuracy in almost all models, with the exception of Decision Trees where BERT Base Uncased provides better performance (accuracy of 0.6071) as compared to BioClinical BERT (accuracy of 0.5893). Comparing evaluation on the unbalanced testing set in <xref rid="T8" ref-type="table">tables 8</xref> and <xref rid="T10" ref-type="table">10</xref>, we observed that the same trend follows &#x02013; using BioClinical BERT embeddings provides higher accuracy in almost all models, with the exception of Decision Trees and XGBoost where using BERT Base Uncased embeddings provides better performance (accuracy of 0.6484 for Decision Trees and 0.6374 for XGBoost) as compared to their counterparts with BioClinical BERT embeddings (accuracy of 0.5824 for Decision Trees and 0.6264 for XGBoost).</p><p id="P64">As part of our experimental design, we included an ablation study in experiment 3 so we could compare the difference in performance during patient phenotyping when removing the use of BERT models to create each patient&#x02019;s vector representations. On the class-balanced testing set, we observed that accuracies range from 0.6071 to 0.7321 when using BioClinical BERT embeddings in <xref rid="T7" ref-type="table">table 7</xref>, accuracies range from 0.5179 to 0.6250 when using BERT Base Uncased embeddings in <xref rid="T9" ref-type="table">table 9</xref>, and accuracies range from 0.6964 to 0.8036 when removing the use of BERT models in <xref rid="T11" ref-type="table">table 11</xref> (experiment 3). On the unbalanced testing set, we observed that accuracies range from 0.5824 to 0.7253 when using BioClinical BERT embeddings in <xref rid="T8" ref-type="table">table 8</xref>, accuracies range from 0.5714 to 0.6703 when using BERT Base Uncased embeddings in <xref rid="T10" ref-type="table">table 10</xref>, and accuracies range from 0.7143 to 0.7582 when removing the use of BERT models in <xref rid="T12" ref-type="table">table 12</xref> (experiment 3).</p><p id="P65">In both cases (evaluation on the balanced testing set, and evaluation on the unbalanced testing set), we found that models in experiment 3 (our ablation study) generally outperform (or are as good as) their corresponding counterparts in experiments 1 and 2 (our BERT experiments) across all metrics (accuracy, precision, recall, F1, NPV, and specificity), with the exception that the stacking classifier in experiment 1 (BioClinical BERT) has marginally stronger accuracy and precision than the stacking classifier in experiment 3. This shows that traditional rules-based approaches (experiment 3) can outperform BERT-based approaches for generating a patient vector representation for downstream patient phenotyping.</p><p id="P66">We hypothesize that models in experiments 1 and 2 showed lower performance because errors from our sentence classifiers in earlier stages of the pipeline could have propagated to later stages of the pipeline during patient phenotyping. Because we leveraged the max operator to aggregate probabilities that any given sentence in the patient record applies to each category, more sentences in each patient record would lead to a greater chance that an erroneous prediction with a high probability would lead to a false positive error in the creation of each patient&#x02019;s vector representation in experiments 1 and 2.</p><p id="P67">Although there is a wide range in performance for our AD patient phenotyping algorithms, we believe that we have reached our goal of developing a system capable of AD patient phenotyping for clinical trial recruitment because <xref rid="T11" ref-type="table">tables 11</xref> and <xref rid="T12" ref-type="table">12</xref> show promising results. Furthermore, our system can be used as a first step during AD clinical trial recruitment to filter out most patients who may not qualify for AD trials and therefore save valuable clinician time. We believe our pipeline is important and valuable because unlike other diseases such as influenza, COVID-19, and cancer, there is no gold-standard test result that can be used to determine when a patient has atopic dermatitis. Instead, clinicians must spend large amounts of time undergoing chart review to individually determine whether each patient has atopic dermatitis.</p></sec><sec id="S26"><title>Limitations</title><p id="P68">One limitation of our study was the small size of our dataset. Although we had a total of 1,926 patients in our dataset, only 137 of them were validated as having AD. During training, we leveraged 109 of the 137 AD patients, and sampled another 109 non-AD patients to create a class balanced training set. The small size of the training set could lead to overfitting and therefore result in reduced performance on the testing set. Future work could involve obtaining more data from patients with AD, as well as exploring the use of an imbalanced dataset but using a class-weighted loss function to counteract the class-imbalance.</p><p id="P69">A second limitation of our study was the input limit size of the large language models that were used. Both BERT Base Uncased and BioClinical BERT had an input limit of 512 tokens. This meant that any input text that was longer than 512 tokens would be ignored when training BERT. Consequently, we couldn&#x02019;t simply directly concatenate all documents from each patient&#x02019;s EHR and feed the tokenized documents of each patient into BERT with an added classification head for training as well as direct prediction of whether the patient has AD. Instead, we designed a pipeline around distilling information from all documents in each patient&#x02019;s EHR into a patient vector representation and then using this patient vector representation to train various classical ML algorithms for phenotyping the patient. Future work could involve exploring the use of other LLM&#x02019;s that are suited for long inputs such as Longformer or Doc2Vec for predicting when a patient should be labeled as having AD.</p><p id="P70">A third limitation of our study was the list of AD indicators we selected. We didn&#x02019;t consider additional AD indicators and we also did not consider the use of different combinations (or subsets) of the AD indicators selected. This is particularly relevant in considering that 1) our pipeline is intended to be used for identifying patients with AD, and 2) that one of our AD indicators (category 1) directly targets whether there is any given sentence in the patient&#x02019;s record that mentions AD which could be in the context of a family history of AD, a potential (but not confirmed) diagnosis of AD, as well as a confirmed diagnosis of AD, among other possibilities. If this AD indicator is removed, then one interesting research question could be whether our pipeline is still able to maintain performance similarly to what it is currently able to achieve. Future work could involve assessing the performance impact from removing (or adding) the use of various AD indicators. We could then determine if our pipeline is relying too much on or overfitting to one or more indicators. Furthermore, we could also re-design our patient vector and separate the feature for category 1 (any sentence that mentions AD) into 3 separate indicators, whether there is 1) a family history of AD, 2) an affirmed diagnosis that the patient has AD, and 3) uncertainty of whether the patient has AD. Doing so could potentially improve precision.</p></sec><sec id="S27"><title>Potential Applications</title><p id="P71">Given the aforementioned results, we believe our AD classifier could be operationalized to facilitate reliable and efficient EHR chart review. For example, sentence classifiers could visually indicate AD indicators inline text, therefore reducing information foraging efforts by clinicians. Additionally, AD phenotyping classifiers could indicate the strength of a patient match to UKWP criteria, exact or partial, based on AD indicator sentence classifications. Furthermore, ranking patient cases by match strength could reduce the number of cases reviewed to generate both case and matched controls.</p></sec><sec id="S28"><title>Conclusions</title><p id="P72">In conclusion, we present and validate a promising pipeline for phenotyping patients with AD during clinical trial recruitment. To do so, we compare a rules-based and transformer-based approach for creating a vector representation of each patient, and compare downstream performance in patient phenotyping with various standard ML algorithms. We find that a traditional rules-based approach outperforms using a transformer-based approach (Experiment 3). We hope that our pipeline can be deployed in hospital settings during clinical trial recruitment as an initial first step to automatically filter candidates before manual review. Additionally, we show that multi-layer perceptron networks can identify whether sentences are relevant to AD diagnosis. These multi-layer perceptron networks can later be deployed in clinical settings to highlight which sentences relevant for physicians during manual chart review, therefore reducing physician burden. Future work can involve extending our patient phenotyping pipeline to other datasets and other diseases.</p></sec></sec></body><back><ack id="S29"><title>Acknowledgements</title><p id="P73">AW designed the experiments, wrote the code, performed the experiments, wrote the first draft of the manuscript, and revised the manuscript.</p><p id="P74">DJM conceptualized and implemented the chart abstraction study, annotated the dataset, interpreted results, and revised the manuscript.</p><p id="P75">RF annotated the dataset and revised the manuscript.</p><p id="P76">SH queried and de-identified the dataset as well as revised the manuscript.</p><p id="P77">DLM conceptualized the study and experiment design, interpreted results, wrote and revised the manuscript, provided secure storage and compute resources.</p><p id="P78">This study was partially funded by the National Institutes of Health (NIH) National Institute of Arthritis and Musculoskeletal and Skin Diseases (NIAMS) P30-AR069589 as part of the Penn Skin Biology and Diseases Resource-Based Center (Core: DJM, DLM).</p></ack><fn-group><fn fn-type="COI-statement" id="FN3"><p id="P79">Conflicts of Interest</p><p id="P80">DJM is or recently has been a consultant for Pfizer, Leo, and Sanofi with respect to studies of atopic dermatitis and served on an advisory board for the National Eczema Association.</p></fn><fn id="FN4"><p id="P81">Ethics Statement</p><p id="P82">This research protocol was reviewed and approved by the University of Pennsylvania Institute Review Board and determined as exempt (IRB#843922).</p></fn></fn-group><glossary><title>Abbreviations</title><def-list><def-item><term>AD</term><def><p id="P83">atopic dermatitis</p></def></def-item><def-item><term>BERT</term><def><p id="P84">Bidirectional Encoder Representations from Transformers</p></def></def-item><def-item><term>EHR</term><def><p id="P85">Electronic Health Records</p></def></def-item><def-item><term>ICD</term><def><p id="P86">International Classification of Disease</p></def></def-item><def-item><term>UKWP</term><def><p id="P87">United Kingdom Working Party</p></def></def-item><def-item><term>HR</term><def><p id="P88">Hanifin and Rajka</p></def></def-item><def-item><term>AI</term><def><p id="P89">Artificial Intelligence</p></def></def-item><def-item><term>NLP</term><def><p id="P90">Natural Language Processing</p></def></def-item><def-item><term>ML</term><def><p id="P91">Machine Learning</p></def></def-item><def-item><term>MLP</term><def><p id="P92">Multi-layer Perceptron</p></def></def-item><def-item><term>ReLU</term><def><p id="P93">Rectified Linear Unit</p></def></def-item><def-item><term>SGD</term><def><p id="P94">Stochastic Gradient Descent</p></def></def-item><def-item><term>KNN</term><def><p id="P95">K-Nearest Neighbors</p></def></def-item><def-item><term>XGBoost</term><def><p id="P96">Extreme Gradient Boosting</p></def></def-item><def-item><term>AdaBoost</term><def><p id="P97">Adaptive Boosting</p></def></def-item><def-item><term>SVM</term><def><p id="P98">Support Vector Machines</p></def></def-item><def-item><term>TP</term><def><p id="P99">True Positive</p></def></def-item><def-item><term>TN</term><def><p id="P100">True Negative</p></def></def-item><def-item><term>FP</term><def><p id="P101">False Positive</p></def></def-item><def-item><term>FN</term><def><p id="P102">False Negative</p></def></def-item><def-item><term>NPV</term><def><p id="P103">Negative Predictive Value</p></def></def-item></def-list></glossary><ref-list><title>References:</title><ref id="R1"><label>1.</label><mixed-citation publication-type="book"><collab>Eczema (ATOPic DERMATITIS)</collab>. <source>Paediatric Allergy and Clinical Immunology (As Applied to Atopic Disease)</source>
<publisher-loc>Toronto</publisher-loc>: <publisher-name>University of Toronto Press</publisher-name>; <year>1973</year>. p. <fpage>32</fpage>&#x02013;<lpage>37</lpage>.</mixed-citation></ref><ref id="R2"><label>2.</label><mixed-citation publication-type="journal"><name><surname>Lyons</surname><given-names>JJ</given-names></name>, <name><surname>Milner</surname><given-names>JD</given-names></name>, <name><surname>Stone</surname><given-names>KD</given-names></name>. <article-title>Atopic dermatitis in children: clinical features, pathophysiology, and treatment</article-title>. <source>Immunol Allergy Clin North Am</source>
<year>2015</year>
<month>Feb</month>;<volume>35</volume>(<issue>1</issue>):<fpage>161</fpage>&#x02013;<lpage>183</lpage>.<pub-id pub-id-type="pmid">25459583</pub-id>
</mixed-citation></ref><ref id="R3"><label>3.</label><mixed-citation publication-type="journal"><name><surname>Eichenfield</surname><given-names>LF</given-names></name>, <name><surname>Tom</surname><given-names>WL</given-names></name>, <name><surname>Chamlin</surname><given-names>SL</given-names></name>, <name><surname>Feldman</surname><given-names>SR</given-names></name>, <name><surname>Hanifin</surname><given-names>JM</given-names></name>, <name><surname>Simpson</surname><given-names>EL</given-names></name>, <name><surname>Berger</surname><given-names>TG</given-names></name>, <name><surname>Bergman</surname><given-names>JN</given-names></name>, <name><surname>Cohen</surname><given-names>DE</given-names></name>, <name><surname>Cooper</surname><given-names>KD</given-names></name>, <name><surname>Cordoro</surname><given-names>KM</given-names></name>, <name><surname>Davis</surname><given-names>DM</given-names></name>, <name><surname>Krol</surname><given-names>A</given-names></name>, <name><surname>Margolis</surname><given-names>DJ</given-names></name>, <name><surname>Paller</surname><given-names>AS</given-names></name>, <name><surname>Schwarzenberger</surname><given-names>K</given-names></name>, <name><surname>Silverman</surname><given-names>RA</given-names></name>, <name><surname>Williams</surname><given-names>HC</given-names></name>, <name><surname>Elmets</surname><given-names>CA</given-names></name>, <name><surname>Block</surname><given-names>J</given-names></name>, <name><surname>Harrod</surname><given-names>CG</given-names></name>, <name><surname>Smith Begolka</surname><given-names>W</given-names></name>, <name><surname>Sidbury</surname><given-names>R</given-names></name>. <article-title>Guidelines of care for the management of atopic dermatitis: section 1. Diagnosis and assessment of atopic dermatitis</article-title>. <source>J Am Acad Dermatol</source>
<year>2014</year>
<month>Feb</month>;<volume>70</volume>(<issue>2</issue>):<fpage>338</fpage>&#x02013;<lpage>351</lpage>.<pub-id pub-id-type="pmid">24290431</pub-id>
</mixed-citation></ref><ref id="R4"><label>4.</label><mixed-citation publication-type="journal"><name><surname>Abramovits</surname><given-names>W.</given-names></name>
<article-title>Atopic dermatitis</article-title>. <source>J Am Acad Dermatol</source>
<year>2005</year>
<month>Jul</month>;<volume>53</volume>(<issue>1 Suppl 1</issue>):<fpage>S86</fpage>&#x02013;<lpage>93</lpage>.<pub-id pub-id-type="pmid">15968268</pub-id>
</mixed-citation></ref><ref id="R5"><label>5.</label><mixed-citation publication-type="journal"><name><surname>Weidinger</surname><given-names>S</given-names></name>, <name><surname>Beck</surname><given-names>LA</given-names></name>, <name><surname>Bieber</surname><given-names>T</given-names></name>, <name><surname>Kabashima</surname><given-names>K</given-names></name>, <name><surname>Irvine</surname><given-names>AD</given-names></name>. <article-title>Atopic dermatitis</article-title>. <source>Nat Rev Dis Primers</source>
<year>2018</year>
<month>Jun</month>
<day>21</day>;<volume>4</volume>(<issue>1</issue>):<fpage>1</fpage>.<pub-id pub-id-type="pmid">29930242</pub-id>
</mixed-citation></ref><ref id="R6"><label>6.</label><mixed-citation publication-type="journal"><name><surname>Schneider</surname><given-names>L</given-names></name>, <name><surname>Hanifin</surname><given-names>J</given-names></name>, <name><surname>Boguniewicz</surname><given-names>M</given-names></name>, <name><surname>Eichenfield</surname><given-names>LF</given-names></name>, <name><surname>Spergel</surname><given-names>JM</given-names></name>, <name><surname>Dakovic</surname><given-names>R</given-names></name>, <name><surname>Paller</surname><given-names>AS</given-names></name>. <article-title>Study of the Atopic March: Development of Atopic Comorbidities</article-title>. <source>Pediatr Dermatol</source>
<year>2016</year>
<month>Jul</month>;<volume>33</volume>(<issue>4</issue>):<fpage>388</fpage>&#x02013;<lpage>398</lpage>.<pub-id pub-id-type="pmid">27273433</pub-id>
</mixed-citation></ref><ref id="R7"><label>7.</label><mixed-citation publication-type="journal"><name><surname>Del Pozo</surname><given-names>DV</given-names></name>, <name><surname>Zhu</surname><given-names>Y</given-names></name>, <name><surname>Mitra</surname><given-names>N</given-names></name>, <name><surname>Hoffstad</surname><given-names>OJ</given-names></name>, <name><surname>Margolis</surname><given-names>DJ</given-names></name>. <article-title>The risk of atopic comorbidities and atopic march progression among Black and White children with mild-to-moderate atopic dermatitis: A cross-sectional study</article-title>. <source>J Am Acad Dermatol</source>
<year>2022</year>
<month>Nov</month>;<volume>87</volume>(<issue>5</issue>):<fpage>1145</fpage>&#x02013;<lpage>1147</lpage>.<pub-id pub-id-type="pmid">35192898</pub-id>
</mixed-citation></ref><ref id="R8"><label>8.</label><mixed-citation publication-type="journal"><name><surname>Eichenfield</surname><given-names>LF</given-names></name>, <name><surname>Tom</surname><given-names>WL</given-names></name>, <name><surname>Berger</surname><given-names>TG</given-names></name>, <name><surname>Krol</surname><given-names>A</given-names></name>, <name><surname>Paller</surname><given-names>AS</given-names></name>, <name><surname>Schwarzenberger</surname><given-names>K</given-names></name>, <name><surname>Bergman</surname><given-names>JN</given-names></name>, <name><surname>Chamlin</surname><given-names>SL</given-names></name>, <name><surname>Cohen</surname><given-names>DE</given-names></name>, <name><surname>Cooper</surname><given-names>KD</given-names></name>, <name><surname>Cordoro</surname><given-names>KM</given-names></name>, <name><surname>Davis</surname><given-names>DM</given-names></name>, <name><surname>Feldman</surname><given-names>SR</given-names></name>, <name><surname>Hanifin</surname><given-names>JM</given-names></name>, <name><surname>Margolis</surname><given-names>DJ</given-names></name>, <name><surname>Silverman</surname><given-names>RA</given-names></name>, <name><surname>Simpson</surname><given-names>EL</given-names></name>, <name><surname>Williams</surname><given-names>HC</given-names></name>, <name><surname>Elmets</surname><given-names>CA</given-names></name>, <name><surname>Block</surname><given-names>J</given-names></name>, <name><surname>Harrod</surname><given-names>CG</given-names></name>, <name><surname>Smith Begolka</surname><given-names>W</given-names></name>, <name><surname>Sidbury</surname><given-names>R</given-names></name>. <article-title>Guidelines of care for the management of atopic dermatitis: section 2. Management and treatment of atopic dermatitis with topical therapies</article-title>. <source>J Am Acad Dermatol</source>
<year>2014</year>
<month>Jul</month>;<volume>71</volume>(<issue>1</issue>):<fpage>116</fpage>&#x02013;<lpage>132</lpage>.<pub-id pub-id-type="pmid">24813302</pub-id>
</mixed-citation></ref><ref id="R9"><label>9.</label><mixed-citation publication-type="journal"><name><surname>Fulton</surname><given-names>RL</given-names></name>, <name><surname>Mitra</surname><given-names>N</given-names></name>, <name><surname>Chiesa-Fuxench</surname><given-names>Z</given-names></name>, <name><surname>Sockler</surname><given-names>PG</given-names></name>, <name><surname>Margolis</surname><given-names>DJ</given-names></name>. <article-title>Untapping the potential of utilizing electronic medical records to identify patients with atopic dermatitis: an algorithm using ICD-10 codes</article-title>. <source>Arch Dermatol Res</source>
<year>2022</year>
<month>Jul</month>;<volume>314</volume>(<issue>5</issue>):<fpage>439</fpage>&#x02013;<lpage>444</lpage>.<pub-id pub-id-type="pmid">34081192</pub-id>
</mixed-citation></ref><ref id="R10"><label>10.</label><mixed-citation publication-type="journal"><name><surname>Gustafson</surname><given-names>E</given-names></name>, <name><surname>Pacheco</surname><given-names>J</given-names></name>, <name><surname>Wehbe</surname><given-names>F</given-names></name>, <name><surname>Silverberg</surname><given-names>J</given-names></name>, <name><surname>Thompson</surname><given-names>W</given-names></name>. <article-title>A Machine Learning Algorithm for Identifying Atopic Dermatitis in Adults from Electronic Health Records</article-title>. <source>IEEE Int Conf Healthc Inform</source>
<year>2017</year>
<month>Aug</month>;<volume>2017</volume>:<fpage>83</fpage>&#x02013;<lpage>90</lpage>.<pub-id pub-id-type="pmid">29104964</pub-id>
</mixed-citation></ref><ref id="R11"><label>11.</label><mixed-citation publication-type="journal"><name><surname>Hanifin</surname><given-names>JM</given-names></name>, <name><surname>Rajka</surname><given-names>G</given-names></name>. <article-title>Diagnostic features of atopic dermatitis</article-title>. <source>Acta Derm Venereol Medical Journals Sweden AB</source>; <year>1980</year>
<month>Nov</month>
<day>11</day>;<volume>60</volume>:<fpage>44</fpage>&#x02013;<lpage>47</lpage>.</mixed-citation></ref><ref id="R12"><label>12.</label><mixed-citation publication-type="journal"><name><surname>Williams</surname><given-names>HC</given-names></name>, <name><surname>Burney</surname><given-names>PG</given-names></name>, <name><surname>Hay</surname><given-names>RJ</given-names></name>, <name><surname>Archer</surname><given-names>CB</given-names></name>, <name><surname>Shipley</surname><given-names>MJ</given-names></name>, <name><surname>Hunter</surname><given-names>JJ</given-names></name>, <name><surname>Bingham</surname><given-names>EA</given-names></name>, <name><surname>Finlay</surname><given-names>AY</given-names></name>, <name><surname>Pembroke</surname><given-names>AC</given-names></name>, <name><surname>Graham-Brown</surname><given-names>RA</given-names></name>. <article-title>The U.K. Working Party&#x02019;s Diagnostic Criteria for Atopic Dermatitis. I. Derivation of a minimum set of discriminators for atopic dermatitis</article-title>. <source>Br J Dermatol</source>
<year>1994</year>
<month>Sep</month>;<volume>131</volume>(<issue>3</issue>):<fpage>383</fpage>&#x02013;<lpage>396</lpage>.<pub-id pub-id-type="pmid">7918015</pub-id>
</mixed-citation></ref><ref id="R13"><label>13.</label><mixed-citation publication-type="journal"><name><surname>Norgeot</surname><given-names>B</given-names></name>, <name><surname>Muenzen</surname><given-names>K</given-names></name>, <name><surname>Peterson</surname><given-names>TA</given-names></name>, <name><surname>Fan</surname><given-names>X</given-names></name>, <name><surname>Glicksberg</surname><given-names>BS</given-names></name>, <name><surname>Schenk</surname><given-names>G</given-names></name>, <name><surname>Rutenberg</surname><given-names>E</given-names></name>, <name><surname>Oskotsky</surname><given-names>B</given-names></name>, <name><surname>Sirota</surname><given-names>M</given-names></name>, <name><surname>Yazdany</surname><given-names>J</given-names></name>, <name><surname>Schmajuk</surname><given-names>G</given-names></name>, <name><surname>Ludwig</surname><given-names>D</given-names></name>, <name><surname>Goldstein</surname><given-names>T</given-names></name>, <name><surname>Butte</surname><given-names>AJ</given-names></name>. <article-title>Protected Health Information filter (Philter): accurately and securely de-identifying free-text clinical notes</article-title>. <source>NPJ Digit Med</source>
<year>2020</year>
<month>Apr</month>
<day>14</day>;<volume>3</volume>:<fpage>57</fpage>.<pub-id pub-id-type="pmid">32337372</pub-id>
</mixed-citation></ref><ref id="R14"><label>14.</label><mixed-citation publication-type="webpage"><collab>Bert-base-uncased &#x000b7; hugging face</collab>. <comment>Available from</comment>: <comment><ext-link xlink:href="https://huggingface.co/bert-base-uncased" ext-link-type="uri">https://huggingface.co/bert-base-uncased</ext-link></comment> [<date-in-citation>accessed Nov 29, 2023</date-in-citation>]</mixed-citation></ref><ref id="R15"><label>15.</label><mixed-citation publication-type="journal"><name><surname>Devlin</surname><given-names>J</given-names></name>, <name><surname>Chang</surname><given-names>M-W</given-names></name>, <name><surname>Lee</surname><given-names>K</given-names></name>, <name><surname>Toutanova</surname><given-names>K</given-names></name>. <article-title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</article-title>. <source>arXiv [csCL]</source>. <year>2018</year>. <comment>Available from</comment>: <comment><ext-link xlink:href="http://arxiv.org/abs/1810.04805" ext-link-type="uri">http://arxiv.org/abs/1810.04805</ext-link></comment></mixed-citation></ref><ref id="R16"><label>16.</label><mixed-citation publication-type="journal"><name><surname>Alsentzer</surname><given-names>E</given-names></name>, <name><surname>Murphy</surname><given-names>JR</given-names></name>, <name><surname>Boag</surname><given-names>W</given-names></name>, <name><surname>Weng</surname><given-names>W-H</given-names></name>, <name><surname>Jin</surname><given-names>D</given-names></name>, <name><surname>Naumann</surname><given-names>T</given-names></name>, <name><surname>McDermott</surname><given-names>MBA</given-names></name>. <article-title>Publicly Available Clinical BERT Embeddings</article-title>. <source>arXiv [csCL]</source>. <year>2019</year>. <comment>Available from</comment>: <comment><ext-link xlink:href="http://arxiv.org/abs/1904.03323" ext-link-type="uri">http://arxiv.org/abs/1904.03323</ext-link></comment></mixed-citation></ref><ref id="R17"><label>17.</label><mixed-citation publication-type="webpage"><source>emilyalsentzer/Bio_ClinicalBERT &#x000b7; Hugging Face</source>. <comment>Available from</comment>: <comment><ext-link xlink:href="https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT" ext-link-type="uri">https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT</ext-link></comment> [<date-in-citation>accessed Nov 29, 2023</date-in-citation>]</mixed-citation></ref><ref id="R18"><label>18.</label><mixed-citation publication-type="journal"><name><surname>Eyre</surname><given-names>H</given-names></name>, <name><surname>Chapman</surname><given-names>AB</given-names></name>, <name><surname>Peterson</surname><given-names>KS</given-names></name>, <name><surname>Shi</surname><given-names>J</given-names></name>, <name><surname>Alba</surname><given-names>PR</given-names></name>, <name><surname>Jones</surname><given-names>MM</given-names></name>, <name><surname>Box</surname><given-names>TL</given-names></name>, <name><surname>DuVall</surname><given-names>SL</given-names></name>, <name><surname>Patterson</surname><given-names>OV</given-names></name>. <article-title>Launching into clinical space with medspaCy: a new clinical text processing toolkit in Python</article-title>. <source>AMIA Annu Symp Proc</source>
<year>2021</year>;<volume>2021</volume>:<fpage>438</fpage>&#x02013;<lpage>447</lpage>.<pub-id pub-id-type="pmid">35308962</pub-id>
</mixed-citation></ref><ref id="R19"><label>19.</label><mixed-citation publication-type="journal"><name><surname>Chapman</surname><given-names>BE</given-names></name>, <name><surname>Lee</surname><given-names>S</given-names></name>, <name><surname>Kang</surname><given-names>HP</given-names></name>, <name><surname>Chapman</surname><given-names>WW</given-names></name>. <article-title>Document-level classification of CT pulmonary angiography reports based on an extension of the ConText algorithm</article-title>. <source>J Biomed Inform</source>
<year>2011</year>
<month>Oct</month>;<volume>44</volume>(<issue>5</issue>):<fpage>728</fpage>&#x02013;<lpage>737</lpage>.<pub-id pub-id-type="pmid">21459155</pub-id>
</mixed-citation></ref><ref id="R20"><label>20.</label><mixed-citation publication-type="journal"><name><surname>Harkema</surname><given-names>H</given-names></name>, <name><surname>Dowling</surname><given-names>JN</given-names></name>, <name><surname>Thornblade</surname><given-names>T</given-names></name>, <name><surname>Chapman</surname><given-names>WW</given-names></name>. <article-title>ConText: an algorithm for determining negation, experiencer, and temporal status from clinical reports</article-title>. <source>J Biomed Inform</source>
<year>2009</year>
<month>Oct</month>;<volume>42</volume>(<issue>5</issue>):<fpage>839</fpage>&#x02013;<lpage>851</lpage>.<pub-id pub-id-type="pmid">19435614</pub-id>
</mixed-citation></ref><ref id="R21"><label>21.</label><mixed-citation publication-type="journal"><name><surname>Mowery</surname><given-names>DL</given-names></name>, <name><surname>Kawamoto</surname><given-names>K</given-names></name>, <name><surname>Bradshaw</surname><given-names>R</given-names></name>, <name><surname>Kohlmann</surname><given-names>W</given-names></name>, <name><surname>Schiffman</surname><given-names>JD</given-names></name>, <name><surname>Weir</surname><given-names>C</given-names></name>, <name><surname>Borbolla</surname><given-names>D</given-names></name>, <name><surname>Chapman</surname><given-names>WW</given-names></name>, <name><surname>Del Fiol</surname><given-names>G</given-names></name>. <article-title>Determining Onset for Familial Breast and Colorectal Cancer from Family History Comments in the Electronic Health Record</article-title>. <source>AMIA Jt Summits Transl Sci Proc</source>
<year>2019</year>
<month>May</month>
<day>6</day>;<volume>2019</volume>:<fpage>173</fpage>&#x02013;<lpage>181</lpage>.<pub-id pub-id-type="pmid">31258969</pub-id>
</mixed-citation></ref><ref id="R22"><label>22.</label><mixed-citation publication-type="webpage"><name><surname>Mowery</surname><given-names>DL</given-names></name>, <name><surname>Velupillai</surname><given-names>S</given-names></name>, <name><surname>Chapman</surname><given-names>W</given-names></name>. <article-title>Medical diagnosis lost in translation&#x02013;Analysis of uncertainty and negation expressions in English and Swedish clinical texts</article-title>. <source>BioNLP: Proceedings of the <ext-link xlink:href="http://aclweb.org" ext-link-type="uri">aclweb.org</ext-link></source>; <year>2012</year>; <comment>Available from</comment>: <comment><ext-link xlink:href="https://www.aclweb.org/anthology/W12-2407.pdf" ext-link-type="uri">https://www.aclweb.org/anthology/W12-2407.pdf</ext-link></comment></mixed-citation></ref><ref id="R23"><label>23.</label><mixed-citation publication-type="journal"><name><surname>Pedregosa</surname><given-names>F</given-names></name>, <name><surname>Varoquaux</surname><given-names>G</given-names></name>, <name><surname>Gramfort</surname><given-names>A</given-names></name>, <name><surname>Michel</surname><given-names>V</given-names></name>, <name><surname>Thirion</surname><given-names>B</given-names></name>, <name><surname>Grisel</surname><given-names>O</given-names></name>, <name><surname>Blondel</surname><given-names>M</given-names></name>, <name><surname>Prettenhofer</surname><given-names>P</given-names></name>, <name><surname>Weiss</surname><given-names>R</given-names></name>, <name><surname>Dubourg</surname><given-names>V</given-names></name>, <name><surname>Vanderplas</surname><given-names>J</given-names></name>, <name><surname>Passos</surname><given-names>A</given-names></name>, <name><surname>Cournapeau</surname><given-names>D</given-names></name>, <name><surname>Brucher</surname><given-names>M</given-names></name>, <name><surname>Perrot</surname><given-names>M</given-names></name>, <name><surname>Duchesnay</surname><given-names>&#x000c9;</given-names></name>. <article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>J Mach Learn Res</source>
<year>2011</year>;<volume>12</volume>(<issue>85</issue>):<fpage>2825</fpage>&#x02013;<lpage>2830</lpage>.</mixed-citation></ref></ref-list></back><floats-group><fig position="float" id="F1"><label>Figure 1.</label><caption><p id="P104">Waterfall diagram of cohort</p></caption><graphic xlink:href="nihpp-2023.08.25.23294636v2-f0001" position="float"/></fig><fig position="float" id="F2"><label>Figure 2.</label><caption><p id="P105">AD Phenotyping pipeline across all 3 experiments</p></caption><graphic xlink:href="nihpp-2023.08.25.23294636v2-f0002" position="float"/></fig><fig position="float" id="F3"><label>Figure 3.</label><caption><p id="P106">Patient vector representations of AD indicators in experiments 1 and 2</p></caption><graphic xlink:href="nihpp-2023.08.25.23294636v2-f0003" position="float"/></fig><fig position="float" id="F4"><label>Figure 4.</label><caption><p id="P107">Patient vector representations of AD indicators in experiment 3</p></caption><graphic xlink:href="nihpp-2023.08.25.23294636v2-f0004" position="float"/></fig><table-wrap position="float" id="T1"><label>Table 1.</label><caption><p id="P108">Meaning of each patient vector element</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Element</th><th align="center" valign="top" rowspan="1" colspan="1">AD Indicator (Diagnostic Criteria)</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">EHR directly mentions patient has AD</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">Patient has hay fever allergies</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">Patient has atopic allergies</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">Patient has eczema or rashes</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">Patient has dry or itchy skin</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">Patient uses non-asthma medications related to treating AD</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">Patient has asthma</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">Patient uses asthma medications</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T2"><label>Table 2.</label><caption><p id="P109">Differences in number of documents, sentences, and tokens between AD and non-AD patients</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" valign="top" rowspan="1" colspan="1"/><th align="left" valign="top" rowspan="1" colspan="1">AD Patient</th><th align="left" valign="top" rowspan="1" colspan="1">AD Patient</th><th align="left" valign="top" rowspan="1" colspan="1">Non-AD Patient</th><th align="left" valign="top" rowspan="1" colspan="1">Non-AD Patient</th></tr><tr><th align="left" valign="top" rowspan="1" colspan="1"/><th align="left" valign="top" rowspan="1" colspan="1">BERT Uncased</th><th align="left" valign="top" rowspan="1" colspan="1">BioClinical BERT</th><th align="left" valign="top" rowspan="1" colspan="1">BERT Uncased</th><th align="left" valign="top" rowspan="1" colspan="1">BioClinical BERT</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Avg # docs (per patient)</td><td align="left" valign="top" rowspan="1" colspan="1">23.44</td><td align="left" valign="top" rowspan="1" colspan="1">23.44</td><td align="left" valign="top" rowspan="1" colspan="1">7.99</td><td align="left" valign="top" rowspan="1" colspan="1">7.99</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Avg # sentences (per patient)</td><td align="left" valign="top" rowspan="1" colspan="1">392.99</td><td align="left" valign="top" rowspan="1" colspan="1">392.99</td><td align="left" valign="top" rowspan="1" colspan="1">193.69</td><td align="left" valign="top" rowspan="1" colspan="1">193.69</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Avg # tokens (per patient)</td><td align="left" valign="top" rowspan="1" colspan="1">16035.39</td><td align="left" valign="top" rowspan="1" colspan="1">17054.11</td><td align="left" valign="top" rowspan="1" colspan="1">7241.02</td><td align="left" valign="top" rowspan="1" colspan="1">7674.35</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Avg # sentences (per doc)</td><td align="left" valign="top" rowspan="1" colspan="1">16.77</td><td align="left" valign="top" rowspan="1" colspan="1">16.77</td><td align="left" valign="top" rowspan="1" colspan="1">24.25</td><td align="left" valign="top" rowspan="1" colspan="1">24.25</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Avg # tokens (per doc)</td><td align="left" valign="top" rowspan="1" colspan="1">684.16</td><td align="left" valign="top" rowspan="1" colspan="1">727.63</td><td align="left" valign="top" rowspan="1" colspan="1">906.45</td><td align="left" valign="top" rowspan="1" colspan="1">960.69</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Avg # tokens (per sentence)</td><td align="left" valign="top" rowspan="1" colspan="1">40.80</td><td align="left" valign="top" rowspan="1" colspan="1">43.40</td><td align="left" valign="top" rowspan="1" colspan="1">37.38</td><td align="left" valign="top" rowspan="1" colspan="1">39.62</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T3"><label>Table 3.</label><caption><p id="P110">Mean number of tokens for sentences identified in each category</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" valign="top" rowspan="1" colspan="1"/><th align="left" valign="top" rowspan="1" colspan="1">BERT Uncased (Mean # tokens per sentence)</th><th align="left" valign="top" rowspan="1" colspan="1">BioClinical BERT (Mean # tokens per sentence)</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Category 1</td><td align="left" valign="top" rowspan="1" colspan="1">99.49</td><td align="left" valign="top" rowspan="1" colspan="1">106.16</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Category 2</td><td align="left" valign="top" rowspan="1" colspan="1">81.18</td><td align="left" valign="top" rowspan="1" colspan="1">92.41</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Category 3</td><td align="left" valign="top" rowspan="1" colspan="1">79.20</td><td align="left" valign="top" rowspan="1" colspan="1">82.07</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Category 4</td><td align="left" valign="top" rowspan="1" colspan="1">83.74</td><td align="left" valign="top" rowspan="1" colspan="1">92.55</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Category 5</td><td align="left" valign="top" rowspan="1" colspan="1">106.64</td><td align="left" valign="top" rowspan="1" colspan="1">112.58</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Category 6</td><td align="left" valign="top" rowspan="1" colspan="1">74.93</td><td align="left" valign="top" rowspan="1" colspan="1">80.17</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Category 7</td><td align="left" valign="top" rowspan="1" colspan="1">92.85</td><td align="left" valign="top" rowspan="1" colspan="1">109.40</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Category 8</td><td align="left" valign="top" rowspan="1" colspan="1">76.13</td><td align="left" valign="top" rowspan="1" colspan="1">83.57</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T4"><label>Table 4.</label><caption><p id="P111">Training and testing dataset size for each classifier</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Classifier</th><th align="center" valign="top" rowspan="1" colspan="1"># of training samples</th><th align="center" valign="top" rowspan="1" colspan="1"># of testing samples</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">2766</td><td align="center" valign="top" rowspan="1" colspan="1">862</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">1302</td><td align="center" valign="top" rowspan="1" colspan="1">392</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">532</td><td align="center" valign="top" rowspan="1" colspan="1">168</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">9822</td><td align="center" valign="top" rowspan="1" colspan="1">2454</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">1466</td><td align="center" valign="top" rowspan="1" colspan="1">354</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">9114</td><td align="center" valign="top" rowspan="1" colspan="1">2316</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">1596</td><td align="center" valign="top" rowspan="1" colspan="1">520</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">4764</td><td align="center" valign="top" rowspan="1" colspan="1">1070</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T5"><label>Table 5.</label><caption><p id="P112">Accuracy of different multi-layer perceptron networks in discerning sentences by AD indicator categories using BioClinical BERT sentence embeddings</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Classifier</th><th align="center" valign="top" rowspan="1" colspan="1">AD Indicator</th><th align="center" valign="top" rowspan="1" colspan="1">Accuracy</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">Direct mention of AD</td><td align="center" valign="top" rowspan="1" colspan="1">0.9002</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of hay fever allergies</td><td align="center" valign="top" rowspan="1" colspan="1">0.8954</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of atopic allergies</td><td align="center" valign="top" rowspan="1" colspan="1">0.8214</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of eczema or rash</td><td align="center" valign="top" rowspan="1" colspan="1">0.8284</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of dry or itchy skin</td><td align="center" valign="top" rowspan="1" colspan="1">0.7373</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of non-asthma medications</td><td align="center" valign="top" rowspan="1" colspan="1">0.8204</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of asthma</td><td align="center" valign="top" rowspan="1" colspan="1">0.7712</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of asthma medications</td><td align="center" valign="top" rowspan="1" colspan="1">0.8299</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T6"><label>Table 6.</label><caption><p id="P113">Accuracy of different multi-layer perceptron networks in discerning sentences by AD indicator categories using BERT Base Uncased sentence embeddings</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Classifier</th><th align="center" valign="top" rowspan="1" colspan="1">AD Indicator</th><th align="center" valign="top" rowspan="1" colspan="1">Accuracy</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">Direct mention of AD</td><td align="center" valign="top" rowspan="1" colspan="1">0.9153</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of hay fever allergies</td><td align="center" valign="top" rowspan="1" colspan="1">0.7730</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of atopic allergies</td><td align="center" valign="top" rowspan="1" colspan="1">0.7976</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of eczema or rash</td><td align="center" valign="top" rowspan="1" colspan="1">0.8439</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of dry or itchy skin</td><td align="center" valign="top" rowspan="1" colspan="1">0.7288</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of non-asthma medications</td><td align="center" valign="top" rowspan="1" colspan="1">0.8096</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of asthma</td><td align="center" valign="top" rowspan="1" colspan="1">0.7269</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">Mention of asthma medications</td><td align="center" valign="top" rowspan="1" colspan="1">0.8738</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T7"><label>Table 7:</label><caption><p id="P114">AD Phenotyping Performance on Balanced Testing Set in Experiment 1 BioClinical BERT)</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Model</th><th align="center" valign="top" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="top" rowspan="1" colspan="1">Precision</th><th align="center" valign="top" rowspan="1" colspan="1">Recall</th><th align="center" valign="top" rowspan="1" colspan="1">F1</th><th align="center" valign="top" rowspan="1" colspan="1">NPV</th><th align="center" valign="top" rowspan="1" colspan="1">Specificity</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">Logistic Regression</td><td align="center" valign="top" rowspan="1" colspan="1">0.7321</td><td align="center" valign="top" rowspan="1" colspan="1">0.7241</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td><td align="center" valign="top" rowspan="1" colspan="1">0.7368</td><td align="center" valign="top" rowspan="1" colspan="1">0.7407</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">SVM</td><td align="center" valign="top" rowspan="1" colspan="1">0.7321</td><td align="center" valign="top" rowspan="1" colspan="1">0.7826</td><td align="center" valign="top" rowspan="1" colspan="1">0.6429</td><td align="center" valign="top" rowspan="1" colspan="1">0.7059</td><td align="center" valign="top" rowspan="1" colspan="1">0.6970</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Decision Tree</td><td align="center" valign="top" rowspan="1" colspan="1">0.5893</td><td align="center" valign="top" rowspan="1" colspan="1">0.6316</td><td align="center" valign="top" rowspan="1" colspan="1">0.4286</td><td align="center" valign="top" rowspan="1" colspan="1">0.5106</td><td align="center" valign="top" rowspan="1" colspan="1">0.5676</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Random Forest</td><td align="center" valign="top" rowspan="1" colspan="1">0.6964</td><td align="center" valign="top" rowspan="1" colspan="1">0.7037</td><td align="center" valign="top" rowspan="1" colspan="1">0.6786</td><td align="center" valign="top" rowspan="1" colspan="1">0.6909</td><td align="center" valign="top" rowspan="1" colspan="1">0.6897</td><td align="center" valign="top" rowspan="1" colspan="1">0.8214</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">KNN</td><td align="center" valign="top" rowspan="1" colspan="1">0.6786</td><td align="center" valign="top" rowspan="1" colspan="1">0.7273</td><td align="center" valign="top" rowspan="1" colspan="1">0.5714</td><td align="center" valign="top" rowspan="1" colspan="1">0.6400</td><td align="center" valign="top" rowspan="1" colspan="1">0.6471</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">XGBoost</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6154</td><td align="center" valign="top" rowspan="1" colspan="1">0.5714</td><td align="center" valign="top" rowspan="1" colspan="1">0.5926</td><td align="center" valign="top" rowspan="1" colspan="1">0.6000</td><td align="center" valign="top" rowspan="1" colspan="1">0.8571</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">AdaBoost</td><td align="center" valign="top" rowspan="1" colspan="1">0.6429</td><td align="center" valign="top" rowspan="1" colspan="1">0.6538</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6296</td><td align="center" valign="top" rowspan="1" colspan="1">0.6333</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Stacking Classifier</td><td align="center" valign="top" rowspan="1" colspan="1">0.6964</td><td align="center" valign="top" rowspan="1" colspan="1">0.7391</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6667</td><td align="center" valign="top" rowspan="1" colspan="1">0.6667</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T8"><label>Table 8:</label><caption><p id="P115">AD Phenotyping Performance on Unbalanced Testing Set in Experiment 1 (BioClinical BERT)</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Model</th><th align="center" valign="top" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="top" rowspan="1" colspan="1">Precision</th><th align="center" valign="top" rowspan="1" colspan="1">Recall</th><th align="center" valign="top" rowspan="1" colspan="1">F1</th><th align="center" valign="top" rowspan="1" colspan="1">NPV</th><th align="center" valign="top" rowspan="1" colspan="1">Specificity</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">Logistic Regression</td><td align="center" valign="top" rowspan="1" colspan="1">0.6813</td><td align="center" valign="top" rowspan="1" colspan="1">0.4884</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td><td align="center" valign="top" rowspan="1" colspan="1">0.5915</td><td align="center" valign="top" rowspan="1" colspan="1">0.8542</td><td align="center" valign="top" rowspan="1" colspan="1">0.6984</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">SVM</td><td align="center" valign="top" rowspan="1" colspan="1">0.6923</td><td align="center" valign="top" rowspan="1" colspan="1">0.5000</td><td align="center" valign="top" rowspan="1" colspan="1">0.6429</td><td align="center" valign="top" rowspan="1" colspan="1">0.5625</td><td align="center" valign="top" rowspan="1" colspan="1">0.8181</td><td align="center" valign="top" rowspan="1" colspan="1">0.7302</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Decision Tree</td><td align="center" valign="top" rowspan="1" colspan="1">0.5824</td><td align="center" valign="top" rowspan="1" colspan="1">0.3438</td><td align="center" valign="top" rowspan="1" colspan="1">0.3929</td><td align="center" valign="top" rowspan="1" colspan="1">0.3667</td><td align="center" valign="top" rowspan="1" colspan="1">0.7119</td><td align="center" valign="top" rowspan="1" colspan="1">0.7143</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Random Forest</td><td align="center" valign="top" rowspan="1" colspan="1">0.7143</td><td align="center" valign="top" rowspan="1" colspan="1">0.5313</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.5667</td><td align="center" valign="top" rowspan="1" colspan="1">0.6845</td><td align="center" valign="top" rowspan="1" colspan="1">0.7619</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">KNN</td><td align="center" valign="top" rowspan="1" colspan="1">0.6593</td><td align="center" valign="top" rowspan="1" colspan="1">0.4571</td><td align="center" valign="top" rowspan="1" colspan="1">0.5714</td><td align="center" valign="top" rowspan="1" colspan="1">0.5079</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7937</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">XGBoost</td><td align="center" valign="top" rowspan="1" colspan="1">0.6264</td><td align="center" valign="top" rowspan="1" colspan="1">0.4211</td><td align="center" valign="top" rowspan="1" colspan="1">0.5714</td><td align="center" valign="top" rowspan="1" colspan="1">0.4848</td><td align="center" valign="top" rowspan="1" colspan="1">0.7736</td><td align="center" valign="top" rowspan="1" colspan="1">0.7619</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">AdaBoost</td><td align="center" valign="top" rowspan="1" colspan="1">0.6044</td><td align="center" valign="top" rowspan="1" colspan="1">0.4048</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.4857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7755</td><td align="center" valign="top" rowspan="1" colspan="1">0.7302</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Stacking Classifier</td><td align="center" valign="top" rowspan="1" colspan="1">0.7253</td><td align="center" valign="top" rowspan="1" colspan="1">0.5429</td><td align="center" valign="top" rowspan="1" colspan="1">0.6786</td><td align="center" valign="top" rowspan="1" colspan="1">0.6032</td><td align="center" valign="top" rowspan="1" colspan="1">0.8393</td><td align="center" valign="top" rowspan="1" colspan="1">0.6984</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T9"><label>Table 9:</label><caption><p id="P116">AD Phenotyping Performance on Balanced Testing Set in Experiment 2 (BERT Base Uncased)</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Model</th><th align="center" valign="top" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="top" rowspan="1" colspan="1">Precision</th><th align="center" valign="top" rowspan="1" colspan="1">Recall</th><th align="center" valign="top" rowspan="1" colspan="1">F1</th><th align="center" valign="top" rowspan="1" colspan="1">NPV</th><th align="center" valign="top" rowspan="1" colspan="1">Specificity</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">Logistic Regression</td><td align="center" valign="top" rowspan="1" colspan="1">0.5893</td><td align="center" valign="top" rowspan="1" colspan="1">0.5758</td><td align="center" valign="top" rowspan="1" colspan="1">0.6786</td><td align="center" valign="top" rowspan="1" colspan="1">0.6230</td><td align="center" valign="top" rowspan="1" colspan="1">0.6087</td><td align="center" valign="top" rowspan="1" colspan="1">0.5000</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">SVM</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.5938</td><td align="center" valign="top" rowspan="1" colspan="1">0.6786</td><td align="center" valign="top" rowspan="1" colspan="1">0.6333</td><td align="center" valign="top" rowspan="1" colspan="1">0.6250</td><td align="center" valign="top" rowspan="1" colspan="1">0.5357</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Decision Tree</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Random Forest</td><td align="center" valign="top" rowspan="1" colspan="1">0.6250</td><td align="center" valign="top" rowspan="1" colspan="1">0.6522</td><td align="center" valign="top" rowspan="1" colspan="1">0.5357</td><td align="center" valign="top" rowspan="1" colspan="1">0.5882</td><td align="center" valign="top" rowspan="1" colspan="1">0.6061</td><td align="center" valign="top" rowspan="1" colspan="1">0.7143</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">KNN</td><td align="center" valign="top" rowspan="1" colspan="1">0.5536</td><td align="center" valign="top" rowspan="1" colspan="1">0.5714</td><td align="center" valign="top" rowspan="1" colspan="1">0.4286</td><td align="center" valign="top" rowspan="1" colspan="1">0.4898</td><td align="center" valign="top" rowspan="1" colspan="1">0.5429</td><td align="center" valign="top" rowspan="1" colspan="1">0.6786</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">XGBoost</td><td align="center" valign="top" rowspan="1" colspan="1">0.5536</td><td align="center" valign="top" rowspan="1" colspan="1">0.5556</td><td align="center" valign="top" rowspan="1" colspan="1">0.5357</td><td align="center" valign="top" rowspan="1" colspan="1">0.5455</td><td align="center" valign="top" rowspan="1" colspan="1">0.5517</td><td align="center" valign="top" rowspan="1" colspan="1">0.5714</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">AdaBoost</td><td align="center" valign="top" rowspan="1" colspan="1">0.5179</td><td align="center" valign="top" rowspan="1" colspan="1">0.5185</td><td align="center" valign="top" rowspan="1" colspan="1">0.5000</td><td align="center" valign="top" rowspan="1" colspan="1">0.5091</td><td align="center" valign="top" rowspan="1" colspan="1">0.5172</td><td align="center" valign="top" rowspan="1" colspan="1">0.5357</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Stacking Classifier</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T10"><label>Table 10:</label><caption><p id="P117">AD Phenotyping Performance on Unbalanced Testing Set in Experiment 2 (BERT Base Uncased)</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Model</th><th align="center" valign="top" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="top" rowspan="1" colspan="1">Precision</th><th align="center" valign="top" rowspan="1" colspan="1">Recall</th><th align="center" valign="top" rowspan="1" colspan="1">F1</th><th align="center" valign="top" rowspan="1" colspan="1">NPV</th><th align="center" valign="top" rowspan="1" colspan="1">Specificity</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">Logistic Regression</td><td align="center" valign="top" rowspan="1" colspan="1">0.5714</td><td align="center" valign="top" rowspan="1" colspan="1">0.3878</td><td align="center" valign="top" rowspan="1" colspan="1">0.6786</td><td align="center" valign="top" rowspan="1" colspan="1">0.4935</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.5238</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">SVM</td><td align="center" valign="top" rowspan="1" colspan="1">0.5714</td><td align="center" valign="top" rowspan="1" colspan="1">0.3878</td><td align="center" valign="top" rowspan="1" colspan="1">0.6786</td><td align="center" valign="top" rowspan="1" colspan="1">0.4935</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.5238</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Decision Tree</td><td align="center" valign="top" rowspan="1" colspan="1">0.6484</td><td align="center" valign="top" rowspan="1" colspan="1">0.4474</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.5152</td><td align="center" valign="top" rowspan="1" colspan="1">0.7925</td><td align="center" valign="top" rowspan="1" colspan="1">0.6667</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Random Forest</td><td align="center" valign="top" rowspan="1" colspan="1">0.6703</td><td align="center" valign="top" rowspan="1" colspan="1">0.4737</td><td align="center" valign="top" rowspan="1" colspan="1">0.6429</td><td align="center" valign="top" rowspan="1" colspan="1">0.5455</td><td align="center" valign="top" rowspan="1" colspan="1">0.8113</td><td align="center" valign="top" rowspan="1" colspan="1">0.6825</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">KNN</td><td align="center" valign="top" rowspan="1" colspan="1">0.6264</td><td align="center" valign="top" rowspan="1" colspan="1">0.4000</td><td align="center" valign="top" rowspan="1" colspan="1">0.4286</td><td align="center" valign="top" rowspan="1" colspan="1">0.4138</td><td align="center" valign="top" rowspan="1" colspan="1">0.7377</td><td align="center" valign="top" rowspan="1" colspan="1">0.7143</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">XGBoost</td><td align="center" valign="top" rowspan="1" colspan="1">0.6374</td><td align="center" valign="top" rowspan="1" colspan="1">0.4286</td><td align="center" valign="top" rowspan="1" colspan="1">0.5357</td><td align="center" valign="top" rowspan="1" colspan="1">0.4762</td><td align="center" valign="top" rowspan="1" colspan="1">0.7679</td><td align="center" valign="top" rowspan="1" colspan="1">0.6825</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">AdaBoost</td><td align="center" valign="top" rowspan="1" colspan="1">0.5934</td><td align="center" valign="top" rowspan="1" colspan="1">0.3784</td><td align="center" valign="top" rowspan="1" colspan="1">0.5000</td><td align="center" valign="top" rowspan="1" colspan="1">0.4308</td><td align="center" valign="top" rowspan="1" colspan="1">0.7407</td><td align="center" valign="top" rowspan="1" colspan="1">0.6349</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Stacking Classifier</td><td align="center" valign="top" rowspan="1" colspan="1">0.6484</td><td align="center" valign="top" rowspan="1" colspan="1">0.4474</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.5152</td><td align="center" valign="top" rowspan="1" colspan="1">0.7925</td><td align="center" valign="top" rowspan="1" colspan="1">0.6667</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T11"><label>Table 11:</label><caption><p id="P118">AD Phenotyping Performance on Balanced Testing Set in Experiment 3 Binary Vector Encoding)</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Model</th><th align="center" valign="top" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="top" rowspan="1" colspan="1">Precision</th><th align="center" valign="top" rowspan="1" colspan="1">Recall</th><th align="center" valign="top" rowspan="1" colspan="1">F1</th><th align="center" valign="top" rowspan="1" colspan="1">NPV</th><th align="center" valign="top" rowspan="1" colspan="1">Specificity</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">Logistic Regression</td><td align="center" valign="top" rowspan="1" colspan="1">0.7679</td><td align="center" valign="top" rowspan="1" colspan="1">0.7586</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7719</td><td align="center" valign="top" rowspan="1" colspan="1">0.7778</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">SVM</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Decision Tree</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7667</td><td align="center" valign="top" rowspan="1" colspan="1">0.8214</td><td align="center" valign="top" rowspan="1" colspan="1">0.7931</td><td align="center" valign="top" rowspan="1" colspan="1">0.8077</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Random Forest</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.8077</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td><td align="center" valign="top" rowspan="1" colspan="1">0.7778</td><td align="center" valign="top" rowspan="1" colspan="1">0.7667</td><td align="center" valign="top" rowspan="1" colspan="1">0.8214</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">KNN</td><td align="center" valign="top" rowspan="1" colspan="1">0.6964</td><td align="center" valign="top" rowspan="1" colspan="1">0.7391</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.6667</td><td align="center" valign="top" rowspan="1" colspan="1">0.6667</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">XGBoost</td><td align="center" valign="top" rowspan="1" colspan="1">0.8036</td><td align="center" valign="top" rowspan="1" colspan="1">0.8400</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td><td align="center" valign="top" rowspan="1" colspan="1">0.7925</td><td align="center" valign="top" rowspan="1" colspan="1">0.7742</td><td align="center" valign="top" rowspan="1" colspan="1">0.8571</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">AdaBoost</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Stacking Classifier</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T12"><label>Table 12:</label><caption><p id="P119">AD Phenotyping Performance on Unbalanced Testing Set in Experiment 3 Binary Vector Encoding)</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Model</th><th align="center" valign="top" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="top" rowspan="1" colspan="1">Precision</th><th align="center" valign="top" rowspan="1" colspan="1">Recall</th><th align="center" valign="top" rowspan="1" colspan="1">F1</th><th align="center" valign="top" rowspan="1" colspan="1">NPV</th><th align="center" valign="top" rowspan="1" colspan="1">Specificity</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">Logistic Regression</td><td align="center" valign="top" rowspan="1" colspan="1">0.7253</td><td align="center" valign="top" rowspan="1" colspan="1">0.5366</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.6377</td><td align="center" valign="top" rowspan="1" colspan="1">0.8800</td><td align="center" valign="top" rowspan="1" colspan="1">0.6984</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">SVM</td><td align="center" valign="top" rowspan="1" colspan="1">0.7473</td><td align="center" valign="top" rowspan="1" colspan="1">0.5641</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.6567</td><td align="center" valign="top" rowspan="1" colspan="1">0.8846</td><td align="center" valign="top" rowspan="1" colspan="1">0.7302</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Decision Tree</td><td align="center" valign="top" rowspan="1" colspan="1">0.7473</td><td align="center" valign="top" rowspan="1" colspan="1">0.5610</td><td align="center" valign="top" rowspan="1" colspan="1">0.8214</td><td align="center" valign="top" rowspan="1" colspan="1">0.6667</td><td align="center" valign="top" rowspan="1" colspan="1">0.9000</td><td align="center" valign="top" rowspan="1" colspan="1">0.7143</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Random Forest</td><td align="center" valign="top" rowspan="1" colspan="1">0.7582</td><td align="center" valign="top" rowspan="1" colspan="1">0.5833</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td><td align="center" valign="top" rowspan="1" colspan="1">0.6563</td><td align="center" valign="top" rowspan="1" colspan="1">0.8727</td><td align="center" valign="top" rowspan="1" colspan="1">0.7619</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">KNN</td><td align="center" valign="top" rowspan="1" colspan="1">0.7363</td><td align="center" valign="top" rowspan="1" colspan="1">0.5667</td><td align="center" valign="top" rowspan="1" colspan="1">0.6071</td><td align="center" valign="top" rowspan="1" colspan="1">0.5862</td><td align="center" valign="top" rowspan="1" colspan="1">0.8197</td><td align="center" valign="top" rowspan="1" colspan="1">0.7937</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">XGBoost</td><td align="center" valign="top" rowspan="1" colspan="1">0.7582</td><td align="center" valign="top" rowspan="1" colspan="1">0.5833</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td><td align="center" valign="top" rowspan="1" colspan="1">0.6563</td><td align="center" valign="top" rowspan="1" colspan="1">0.8727</td><td align="center" valign="top" rowspan="1" colspan="1">0.7619</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">AdaBoost</td><td align="center" valign="top" rowspan="1" colspan="1">0.7473</td><td align="center" valign="top" rowspan="1" colspan="1">0.5641</td><td align="center" valign="top" rowspan="1" colspan="1">0.7857</td><td align="center" valign="top" rowspan="1" colspan="1">0.6567</td><td align="center" valign="top" rowspan="1" colspan="1">0.8846</td><td align="center" valign="top" rowspan="1" colspan="1">0.7302</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Stacking Classifier</td><td align="center" valign="top" rowspan="1" colspan="1">0.7143</td><td align="center" valign="top" rowspan="1" colspan="1">0.5250</td><td align="center" valign="top" rowspan="1" colspan="1">0.7500</td><td align="center" valign="top" rowspan="1" colspan="1">0.6176</td><td align="center" valign="top" rowspan="1" colspan="1">0.8627</td><td align="center" valign="top" rowspan="1" colspan="1">0.6984</td></tr></tbody></table></table-wrap></floats-group></article>
