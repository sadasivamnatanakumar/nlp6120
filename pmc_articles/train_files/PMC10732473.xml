<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><?properties manuscript?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-journal-id">9918227365206676</journal-id><journal-id journal-id-type="pubmed-jr-id">50752</journal-id><journal-id journal-id-type="nlm-ta">Neuroimage Rep</journal-id><journal-id journal-id-type="iso-abbrev">Neuroimage Rep</journal-id><journal-title-group><journal-title>Neuroimage. Reports</journal-title></journal-title-group><issn pub-type="epub">2666-9560</issn></journal-meta><article-meta><article-id pub-id-type="pmid">38125823</article-id><article-id pub-id-type="pmc">PMC10732473</article-id><article-id pub-id-type="doi">10.1016/j.ynirp.2023.100191</article-id><article-id pub-id-type="manuscript">NIHMS1951039</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>ImageNomer: Description of a functional connectivity and omics analysis tool and case study identifying a race confound</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Orlichenko</surname><given-names>Anton</given-names></name><xref rid="A1" ref-type="aff">a</xref><xref rid="CR1" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Daly</surname><given-names>Grant</given-names></name><xref rid="A2" ref-type="aff">b</xref></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Ziyu</given-names></name><xref rid="A1" ref-type="aff">a</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Anqi</given-names></name><xref rid="A3" ref-type="aff">c</xref></contrib><contrib contrib-type="author"><name><surname>Shen</surname><given-names>Hui</given-names></name><xref rid="A3" ref-type="aff">c</xref></contrib><contrib contrib-type="author"><name><surname>Deng</surname><given-names>Hong-Wen</given-names></name><xref rid="A3" ref-type="aff">c</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Yu-Ping</given-names></name><xref rid="A1" ref-type="aff">a</xref></contrib></contrib-group><aff id="A1"><label>a</label>Department of Biomedical Engineering, Tulane University, New Orleans, LA, USA</aff><aff id="A2"><label>b</label>College of Medicine, University of South Alabama, Mobile, AL, USA</aff><aff id="A3"><label>c</label>School of Medicine, Tulane University, New Orleans, LA, USA</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author. <email>aorlichenko@tulane.edu</email> (A. Orlichenko).</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>13</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="ppub"><month>12</month><year>2023</year></pub-date><pub-date pub-type="epub"><day>7</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>12</month><year>2023</year></pub-date><volume>3</volume><issue>4</issue><elocation-id>100191</elocation-id><permissions><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article under the CC BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xlink:href="https://www.sciencedirect.com/science/article/pii/S2666956023000363?via%3Dihub"/><abstract id="ABS1"><p id="P1">Most packages for the analysis of fMRI-based functional connectivity (FC) and genomic data are used with a programming language interface, lacking an easy-to-navigate GUI frontend. This exacerbates two problems found in these types of data: demographic confounds and quality control in the face of high dimensionality of features. The reason is that it is too slow and cumbersome to use a programming interface to create all the necessary visualizations required to identify all correlations, confounding effects, or quality control problems in a dataset. FC in particular usually contains tens of thousands of features per subject, and can only be summarized and efficiently explored using visualizations. To remedy this situation, we have developed ImageNomer, a data visualization and analysis tool that allows inspection of both subject-level and cohort-level demographic, genomic, and imaging features. The software is Python-based, runs in a self-contained Docker image, and contains a browser-based GUI frontend. We demonstrate the usefulness of ImageNomer by identifying an unexpected race confound when predicting achievement scores in the Philadelphia Neurodevelopmental Cohort (PNC) dataset, which contains multitask fMRI and single nucleotide polymorphism (SNP) data of healthy adolescents. In the past, many studies have attempted to use FC to identify achievement-related features in fMRI. Using ImageNomer to visualize trends in achievement scores between races, we find a clear potential for confounding effects if race can be predicted using FC. Using correlation analysis in the ImageNomer software, we show that FCs correlated with Wide Range Achievement Test (WRAT) score are in fact more highly correlated with race. Investigating further, we find that whereas both FC and SNP (genomic) features can account for 10&#x02013;15% of WRAT score variation, this predictive ability disappears when controlling for race. We also use ImageNomer to investigate race-FC correlation in the Bipolar and Schizophrenia Network for Intermediate Phenotypes (BSNIP) dataset. In this work, we demonstrate the advantage of our ImageNomer GUI tool in data exploration and confound detection. Additionally, this work identifies race as a strong confound in FC data and casts doubt on the possibility of finding unbiased achievement-related features in fMRI and SNP data of healthy adolescents.</p></abstract><kwd-group><kwd>fMRI</kwd><kwd>Functional connectivity</kwd><kwd>Software</kwd><kwd>Achievement score</kwd><kwd>Race confound</kwd><kwd>PNC dataset</kwd></kwd-group></article-meta></front><body><sec id="S1"><label>1.</label><title>Introduction</title><p id="P2">Functional magnetic resonance imaging (fMRI) uses the blood oxygen level-dependent (BOLD) signal to identify regions of increased neural activity (<xref rid="R4" ref-type="bibr">Belliveau et al., 1991</xref>). Functional connectivity (FC) is an fMRI-derived measure that quantifies the synchronization between BOLD signal in different regions of the brain (<xref rid="R12" ref-type="bibr">Greicius et al., 2003</xref>). It and similar measures have been used to predict age, (<xref rid="R21" ref-type="bibr">Orlichenko et al., 2022</xref>) sex, (<xref rid="R38" ref-type="bibr">Zhang et al., 2020</xref>) intelligence, (<xref rid="R28" ref-type="bibr">Qu et al., 2021</xref>) and disease status (<xref rid="R8" ref-type="bibr">Du et al., 2018</xref>; <xref rid="R20" ref-type="bibr">Millar et al., 2022</xref>). FC-like measures derived from magnetoencephalography (MEG) have also been used for predicting age (<xref rid="R37" ref-type="bibr">Xifra-Porxas et al., 2021</xref>) and sex (<xref rid="R22" ref-type="bibr">Ott et al., 2021</xref>). Genomics such as single nucleotide polymorphisms (SNPs) can be used to make predictions that are much more accurate than those based on fMRI (<xref rid="R15" ref-type="bibr">Krishnan et al., 2016</xref>; <xref rid="R17" ref-type="bibr">Liu et al., 2020</xref>). Use of genomics and fMRI together may give superior results (<xref rid="R14" ref-type="bibr">Hu et al., 2021</xref>).</p><p id="P3">Existing software packages for analysis of fMRI, FC, and FC-like measures such as partial correlation connectivity are either mostly text-based (programmatic interface) or have incomplete feature sets for identifying correlations in phenotypes (see <xref rid="F1" ref-type="fig">Fig. 1</xref>). For example, numpy (<xref rid="R13" ref-type="bibr">Harris et al., 2020</xref>), PyTorch, (<xref rid="R23" ref-type="bibr">Paszke et al., 2019</xref>) scikit-learn, (<xref rid="R24" ref-type="bibr">Pedregosa et al., 2011</xref>) nilearn, (<xref rid="R1" ref-type="bibr">Abraham et al., 2014</xref>) and nipype (<xref rid="R11" ref-type="bibr">Gorgolewski et al., 2016</xref>) are all powerful and popular Python-based toolkits that can be used to conduct neuroimaging research. In fact, we use some of these packages as components in our ImageNomer software, but they all lack a graphical user interface that can speed up exploration of new datasets. Classic packages such as the Matlab-based BrainNet viewer (<xref rid="R36" ref-type="bibr">Xia et al., 2013</xref>) or GIFT toolbox, (<xref rid="R6" ref-type="bibr">Calhoun et al., 2001</xref>) although they have a GUI frontend, do not allow for analysis of correlations between phenotypes as well as between phenotypes and imaging features/SNPs. Additionally, a Matlab-based toolchain ties one&#x02019;s product to a proprietary and non-free dependency. Even more modern tools like COINSTAC (<xref rid="R26" ref-type="bibr">Plis et al., 2016</xref>) fall short because of a complicated user interface, lack of support for extremely high dimensional features, and a focus on federated learning which most neuroscientists do not need in their research. In contrast, ImageNomer focuses on data exploration by allowing correlation analysis of imaging, demographic, and genomic features and the creation of demographic-based subgroups. An overview of the ImageNomer architecture is shown in <xref rid="F2" ref-type="fig">Fig. 2</xref>.</p><p id="P4">Two problems with creating good, easy-to-use tools for analysis of fMRI-derived FC data are the high dimensionality of imaging features and the small effect sizes being measured. For example, Bennet et al. found that many effects found as marginally significant by standard analysis techniques are simply due to noise (<xref rid="R5" ref-type="bibr">Bennett and Miller, 2010</xref>; <xref rid="R18" ref-type="bibr">Lyon, 2017</xref>). For many recent fMRI studies, high dimensionality of the data and small effect size is exacerbated by small cohort sizes, (<xref rid="R31" ref-type="bibr">Szucs and Ioannidis, 2020</xref>) with the average reproducible cohort size for a fMRI result being 36 subjects (<xref rid="R33" ref-type="bibr">Turner et al., 2018</xref>). Our ImageNomer software addresses these points by treating visualized FC matrices (see <xref rid="F3" ref-type="fig">Fig. 3</xref>) and FC/phenotype correlation maps (see, e.g., <xref rid="F8" ref-type="fig">Fig. 8</xref>) as the primary outcome of the analysis, allowing quick visual inspection of what would take a long time through a programming interface. Cognizant of the high dimensionality of imaging features, we also perform Bonferroni-type multiple comparison correction in all FC-phenotype and SNP-phenotype correlation analysis. This does a lot to avoid the dead-salmon effect found by Bennet et al. (<xref rid="R5" ref-type="bibr">Bennett and Miller, 2010</xref>).</p><p id="P5">To demonstrate the utility of our developed ImageNomer tool, we use its data visualization and correlation abilities to quickly and easily identify a race confound in FC data. Specifically, we find that the high correlation between FC and race and the unequal distribution of achievement scores among races makes it appear that FC can predict achievement score, when our work shows it is more likely due to a confound. Many studies have used FC features to predict scholastic achievement, as measured by, e.g., WRAT score, (<xref rid="R30" ref-type="bibr">Sayegh et al., 2014</xref>) explaining 10% of the variance in a population (<xref rid="R21" ref-type="bibr">Orlichenko et al., 2022</xref>) or achieving a small correlation with ground truth of &#x003c1; &#x02248; 0.3 (<xref rid="R25" ref-type="bibr">Pervaiz et al., 2020</xref>). We show, however, that the FC feature to WRAT score correlation is probably due to a confounding effect of race on FC. Indeed, previous studies have shown that AI models can sometimes trivially detect and be confounded by race, (<xref rid="R9" ref-type="bibr">Gichoya et al., 2022</xref>) and recent work has suggested that race can confound FC-based prediction of behavior <xref rid="R16" ref-type="bibr">(Li et al., 2022</xref>). In this work, we use ImageNomer to identify a confound in FC, and find that this race confound is primarily responsible for any ability to predict WRAT score from FC. The utility of a tool like ImageNomer is validated by speed with which we find the race confound using a GUI toolkit, while numerous groups continue to search for achievement-based features, presumably using programmatic interfaces (<xref rid="R25" ref-type="bibr">Pervaiz et al., 2020</xref>; <xref rid="R28" ref-type="bibr">Qu et al., 2021</xref>).</p><p id="P6">In summary, correlation analysis can give a quick overview of the data, and subject-level or cohort-level views can be instrumental for quality control. This is the reason we have developed ImageNomer, a visualization and analysis tool for connectivity-based fMRI and omics studies. The tool enables rapid correlation analysis as well as the comparison of features from outside models in a convenient browser-based user interface. Additionally, we include the ability to analyze distribution of phenotypes. Indeed, we find that correlation analysis is sufficient to quickly and clearly identify the confounding effect of race on WRAT score found in our study. Our code, as well as a Docker image and a live on-line demo, has been released and are available via links on our GitHub page (Available online at <ext-link xlink:href="https://github.com/TulaneMBB/ImageNomer" ext-link-type="uri">https://github.com/TulaneMBB/ImageNomer</ext-link>).</p></sec><sec id="S2"><label>2.</label><title>Methods</title><sec id="S3"><label>2.1.</label><title>Architecture</title><p id="P7">ImageNomer is made up of three components (see <xref rid="F2" ref-type="fig">Fig. 2</xref>).</p><list list-type="bullet" id="L1"><list-item><p id="P8">a Python backend which integrates with available libraries such matplotlib, scikit-learn, and nilearn</p></list-item><list-item><p id="P9">a Flask server that handles requests from the browser-based UI to the backend</p></list-item><list-item><p id="P10">a Vue frontend which provides an interactive user experience from within the browser</p></list-item></list><p id="P11">A web-based user interface allows quick navigation around a cohort as well as the creation of summary graphs and correlation analyses. The main FC view is shown in <xref rid="F3" ref-type="fig">Fig. 3</xref>. The data being explored is stored locally in the server component, while the Python backend allows integration with standard libraries such as nilearn, scipy, numpy, and matplotlib. The matplotlib backend is used to generate all graphs on the backend, which are sent to the frontend as images. The Vue frontend allows for modularity of UI components, provides a library of pre-built widgets via Vuetify, and enables easy-to-code interactivity.</p></sec><sec id="S4"><label>2.2.</label><title>2.2 Software features</title><p id="P12">ImageNomer has the following capabilities.</p><list list-type="bullet" id="L2"><list-item><p id="P13">Examine individual subject FC and partial correlation-based (PC) connectivity</p></list-item><list-item><p id="P14">Display distributions of phenotypes</p></list-item><list-item><p id="P15">Correlate phenotypes with phenotypes, FC/PC features with phenotypes, and SNPs with phenotypes</p></list-item><list-item><p id="P16">Display p-value maps for correlations</p></list-item><list-item><p id="P17">Perform math on images</p></list-item><list-item><p id="P18">Display components for FC decompositions (such as PCA)</p></list-item><list-item><p id="P19">Correlate decomposition components with phenotypes or SNPs</p></list-item><list-item><p id="P20">Display weights from machine learning models</p></list-item><list-item><p id="P21">Summarize and average weights from multiple models</p></list-item></list><p id="P22">Future work with fMRI will likely require summarizing connectivity patterns into discrete network contributions (<xref rid="R34" ref-type="bibr">Wang et al., 2021</xref>). In the future, we plan to expand ImageNomer&#x02019;s capabilities for summary measures and dictionary learning.</p></sec><sec id="S5"><label>2.3.</label><title>Live web demo, docker images, and tutorial</title><p id="P23">We have created a live on-line demo (on-line demo available at <ext-link xlink:href="https://aorliche.github.io/ImageNomer/live/" ext-link-type="uri">https://aorliche.github.io/ImageNomer/live/</ext-link>) and a Docker image containing an example open-access dataset of Fibromyalgia patients. This dataset is available as accession number ds004144 from OpenNeuro (<xref rid="R3" ref-type="bibr">Balducci et al., 2022</xref>; <xref rid="R19" ref-type="bibr">Markiewicz et al., 2021</xref>). Instructions for using the Docker images, as well as a tutorial based around the Fibromyalgia dataset, can be found online (documentation and tutorial link: <ext-link xlink:href="https://imagenomer.readthedocs.io/en/latest/index.html" ext-link-type="uri">https://imagenomer.readthedocs.io/en/latest/index.html</ext-link>). The tutorial goes through step-by-step each of the major functions of ImageNomer, with instructions and screenshots of the expected output. Unfortunately, NIH data access policy precludes us from making the PNC or BSNIP data available publicly. If you are an approved researcher, we would be happy to work with you regarding functions, e.g., SNPs, which are not found in the Fibromyalgia dataset.</p><p id="P24">The easiest way to use ImageNomer is by mapping a directory on your local machine containing a &#x0201c;demographics.pkl&#x0201d; file and a &#x0201c;fc&#x0201d; subdirectory into the Docker image when starting the container (download and use ImageNomer: <ext-link xlink:href="https://github.com/TulaneMBB/ImageNomer" ext-link-type="uri">https://github.com/TulaneMBB/ImageNomer</ext-link>). We provide a second preprocessed OpenNeuro dataset ds004775 (<xref rid="R35" ref-type="bibr">Weber et al., 2023</xref>) dealing with Vicarious Punishment in our GitHub along with a tutorial on how to map it to the Docker container, similar to what one would do for one&#x02019;s own dataset. Our GitHub repository also contains a notebooks folder with Jupyter notebooks that shows step-by-step how the Fibromyalgia and Punishment data were preprocessed in manner suitable for ImageNomer, starting from a CSV/TSV file and BOLD timeseries.</p><p id="P25">Alternatively, ImageNomer can be used by cloning our GitHub project and installing the Python requirements via pip. However, the use of Docker images, via instructions found on our GitHub (<ext-link xlink:href="https://github.com/TulaneMBB/ImageNomer" ext-link-type="uri">https://github.com/TulaneMBB/ImageNomer</ext-link>) is the easiest method. Docker images have been built for the amd64 and arm64 architectures; check the documentation for how to use the right version for you. If one is interested in editing the code, it is split into the &#x0201c;backend&#x0201d; and &#x0201c;frontend&#x0201d; directories. The &#x0201c;backend&#x0201d; directory contains Python modules and does the heavy lifting with respect to data loading, image generation, and correlation analysis. The &#x0201c;frontend&#x0201d; directory contains a Vue javascript project that handles the browser-based interface and keeps track of most session state. Individual parts of the web-interface are built as Vue components.</p></sec><sec id="S6"><label>2.4.</label><title>Case study on ethnicity confound in FC and its impact on achievement score prediction</title><p id="P26">As a demonstration of the power of ImageNomer&#x02019;s GUI in quickly identifying trends, confounds, and correlations in data, we give a case study of using ImageNomer to identify an under-reported race confound present in FC. As discussed in the Introduction, many groups have tried to predict achievement score or similar metrics from FC. Using ImageNomer, we find that there is a large difference between achievement scores among races. This can potentially lead to a confounding effect if race can be predicted from FC. We also find a high correlation between race and certain FC regions, making us suspect that race-from-FC prediction is possible. We then verify the presence of this confound by performing regression on the whole cohort vs. within-ethnicity subsets. We learn that quick and dirty data exploration may save a lot of time trying to look for FC correlates of cognition that may or may not be there.</p></sec><sec id="S7"><label>2.5.</label><title>PNC dataset</title><p id="P27">We tested ImageNomer by using it to examine the large Philadelphia Neurodevelopmental Cohort (PNC) dataset (<xref rid="R10" ref-type="bibr">Glessner et al., 2010</xref>; <xref rid="R29" ref-type="bibr">Satterthwaite et al., 2014</xref>). The PNC dataset contains fMRI scans, SNP information, cognitive batteries, questionnaires, and phenotype data from healthy adolescents between 8 and 23 years old. The dataset is enriched for European Ancestry (EA) and African Ancestry (AA) races. It contains fMRI scans for 1445 healthy adolescents and SPN data for more than 9267. We chose an 830-subject subset of the data which included subjects with SNP information as well as resting state (rest), working memory (nback), and emotion identification (emoid) scanner task fMRI scans. Scholastic achievement and problem-solving ability was measured by Wide Range Achievement Test (WRAT) score (<xref rid="R30" ref-type="bibr">Sayegh et al., 2014</xref>) with the effects of age regressed out. A total of three fMRI tasks were acquired: resting state, working memory, and emotion identification. An example of parcellation along with mean FC in the PNC dataset is shown in <xref rid="F4" ref-type="fig">Fig. 4</xref>. The acquisition parameters for fMRI and FC preprocessing have been described elsewhere (<xref rid="R21" ref-type="bibr">Orlichenko et al., 2022</xref>).</p><p id="P28">SNPs were collected using one of eight different platforms, with the largest set containing 1,185,051 SNPs (<xref rid="R10" ref-type="bibr">Glessner et al., 2010</xref>). We selected a subset of 10,433 SNPs that were found in at least 100 subjects in the cohort for our analysis. SNPs were categorized by haplotype as homozygous minor variant, heterozygous, and homozygous major variant. Missing values for subjects were set to zero for all haplotypes.</p></sec><sec id="S8"><label>2.6.</label><title>BSNIP dataset</title><p id="P29">Robustness of race prediction from FC was tested by using an independent dataset to validate models trained on PNC. The dataset used was the Bipolar and Schizophrenia Network for Intermediate Phenotypes (BSNIP) dataset of 933 patients, 1059 relatives, and 459 healthy controls (<xref rid="R32" ref-type="bibr">Tamminga et al., 2014</xref>). fMRI scans were acquired over 6 different sites, and acquisition and preprocessing are described elsewhere (<xref rid="R2" ref-type="bibr">Abrol et al., 2019</xref>). For validation of race prediction we chose a subset of 387 African Americans (AA) and 778 Caucasians (CA), both patients and healthy controls, for whom we had fMRI scans.</p></sec></sec><sec id="S9"><label>3.</label><title>Results and Discussion</title><p id="P30">We first present our exploration of the potential race confound in achievement score prediction from FC using ImageNomer. Based on analysis with ImageNomer, we hypothesize that due to the high correlation of FC with race and the obvious difference in achievement scores between races, prediction of achievement score from FC is solely due to a race confound. We then corroborate our hypothesis by using whole cohort and within-ethnic group regression models. Note that all Figures presented in this section are screenshots from the ImageNomer program.</p><sec id="S10"><label>3.1.</label><title>Data exploration of possible race confound with ImageNomer</title><p id="P31">We first confirm that age and sex are not possible confounding factors with respect to achievement score prediction. This is illustrated in <xref rid="F5" ref-type="fig">Figs. 5</xref> and <xref rid="F6" ref-type="fig">6</xref>, where we see equal distributions of WRAT score among males and females and no correlation with age (raw WRAT score has been corrected for age).</p><p id="P32">Next, we use the group creation capabilities of ImageNomer to create two groups: European Ancestry (EA) and African Ancestry (AA) groups. We then compare the WRAT score distribution between the two groups, illustrated in <xref rid="F7" ref-type="fig">Fig. 7</xref>. We find that here there is a clear difference in achievement score distribution, leading to the possibility of a confounding effect if there is a race signal present in FC.</p><p id="P33">Using the FC-to-phenotype correlation feature of ImageNomer, we explore whether there is correlation between race and FC. In <xref rid="F8" ref-type="fig">Fig. 8</xref>, we find that there is a large and significant correlation between race and FC. Furthermore, in the same Figure, we show that the smaller correlation between WRAT score and FC is actually a subset of the race-FC correlation.</p><p id="P34">We perform the same analysis for SNPs, with the caveat that sex and race can be perfectly predicted using SNPs, and that SNP data does not contain any age-related signal. Nevertheless, we still attempted to find whether there was a suggestive overlap between SNPs correlated with race and SNPs correlated with high or low WRAT score. Our results are shown in <xref rid="F9" ref-type="fig">Fig. 9</xref>. From a total of 10,433 SNPs found in 100 or more subjects, we identified the top 20 SNPs correlated with race and achievement score. Of these top 20, six appeared in both the highly WRAT-correlated and highly race-correlated batches.</p><p id="P35">In summary, we use ImageNomer to form the hypothesis that race may bias FC-based prediction of achievement score if there is a race signal present in FC. We then find, using ImageNomer, that the race signal present in FC is in fact stronger than the signal for achievement score, and that WRAT score to FC correlation is a subset of race-FC correlation. Finally, we draw the same conclusion in SNP to race and SNP to achievement score correlation. In the next section, we describe the use of regression models to validate our hypothesis.</p></sec><sec id="S11"><label>3.2.</label><title>Validation with regression models</title><p id="P36">We validate the qualitative results from ImageNomer&#x02019;s data visualization and exploration capabilities with train/test regression models. We use regularized Ridge and Logistic Regression models with an 80/20 train-test split and 20 bootstrapping repetitions to predict age, sex, race, and WRAT score. Additionally, we predict WRAT score in whole cohort as well as within intra-ethnicity groups. The results are shown in <xref rid="T1" ref-type="table">Table 1</xref>.</p><p id="P37">The results are as follows: age, sex, and race can all be modestly well predicted using FC. WRAT score can be predicted, although at a barely significant level, using the whole cohort, with both FC and/or SNPs as input. However, any ability to predict WRAT score disappears in race-controlled (within ethnicity) groups. This validates our hypothesis, formulated with ImageNomer via data exploration, that FC features used to predict achievement score are actually predicting ethnicity instead.</p><p id="P38">Next, we confirm the stability of race signal in FC by using both ImageNomer and transfer learning of regression models to find that race signal is at least somewhat conserved between the PNC and BSNIP datasets. Finally, we consider the effect of socioeconomic status (SES) as another potential confound besides race in predicting achievement score from FC.</p><sec id="S12"><label>3.2.1.</label><title>Transfer of race prediction models between PNC and BSNIP datasets</title><p id="P39">We show screenshots of ImageNomer-based data exploration for FC correlation with race in the PNC and BSNIP datasets in <xref rid="F10" ref-type="fig">Fig. 10</xref>. This figure highlights the fact that both datasets have similar correlations between specific FCs and race. We confirm the ImageNomer-based hypothesis with results for transfer of race prediction models between the PNC and BSNIP datasets, shown in <xref rid="T2" ref-type="table">Table 2</xref>. A Logistic Regression model trained on the PNC dataset was able to predict race in the BSNIP dataset with an average accuracy of 68%. When trained on BSNIP and evaluated on PNC, the average prediction accuracy was 66%. We find that the prediction is less good than within-dataset prediction, although still better than chance. It should be taken into account that the PNC dataset is made up of healthy adolescents, while the BSNIP dataset contains schizophrenia and bipolar patients, relatives of patients, and healthy controls. <xref rid="F10" ref-type="fig">Fig. 10</xref> shows a comparison between race correlation and FC in the PNC and BSNIP datasets, created using ImageNomer.</p></sec></sec><sec id="S13"><label>3.3.</label><title>Effect of socioeconomic status (SES) explored with ImageNomer and regression models</title><p id="P40">We consider socioeconomic status (SES) as another confounding factor when predicting scholastic achievement based on a standardized test, with the majority of analysis again carried out using ImageNomer. Predictive models were only used to validate the conclusions made using data exploration in ImageNomer. A problem is that SES was not directly measured in the PNC study, in that the income of family groups was not known. However, previous studies have used parental education levels as a proxy for SES, (<xref rid="R7" ref-type="bibr">Chen et al., 2018</xref>) and this information was included in the PNC dataset. Indeed, as seen in <xref rid="F11" ref-type="fig">Fig. 11</xref>, we find that SES, race, and WRAT score are all inter-related. We see that non-EA ethnicity tend to have lower SES as measured by mother education level. The correlation of father education level with FC was similar to mother education, although less significant. It should be noted that many children had missing values for father education level.</p><p id="P41">We see in <xref rid="F11" ref-type="fig">Fig. 11</xref> that SES, ethnicity, and WRAT score correlate to similar regions on the FC map. The p-values associated with FC-SES correlation (as measured by mother education) are somewhat lower than those associated with FC-WRAT or FC-ethnicity correlation, but still significant. Additionally, performing regression analysis for WRAT score based on FC in low SES (mother education &#x02264;12 years) and high SES (mother education &#x02265;14 years) groups, we find a barely significant ability to predict achievement in the low SES group but not a significant ability in the high SES group. The results are shown in <xref rid="T3" ref-type="table">Table 3</xref>. The WRAT prediction accuracy in the low SES group is worse than in the cohort as a whole (compare 13.6 RMSE vs 14.1 RMSE). We conclude that SES is a confounding factor in WRAT score prediction, though not as severe as ethnicity in this dataset. Finally, we reject the idea that race is a causal factor in achievement or WRAT score, but only point out the potential confounding effects if race or SES is not taken into account in studies seeking to find markers of high or low achievement, and the ease with which such confounds were found using ImageNomer.</p></sec></sec><sec id="S14"><label>4.</label><title>Conclusion</title><p id="P42">We present ImageNomer, a new fMRI and omics visualization and analysis tool. We note that most of the figures shown in this manuscript were created as screenshots of the working tool. We use this tool to examine the large PNC dataset and discover features important for phenotype prediction. As validation for ImageNomer-based correlation analysis, we find that age, sex, and race can be moderately well predicted by FC features, with 10 FC features giving up to 72% race prediction accuracy, compared with 85% for the full model.</p><p id="P43">We find both FC features and SNPs can somewhat predict scholastic knowledge and problem-solving ability, as measured by WRAT score, but that this is probably due to a race confound. When controlling for race, FC-based achievement score prediction drops to the same accuracy as the null model and the SNP-based prediction becomes statistically insignificant. We conclude that, on average, the effect of either SNPs or FC features on scholastic achievement in normal children is very small, if one exists at all. Additionally, we find that race prediction from FC is at least somewhat robust between different datasets. Using ImageNomer, this work quickly and easily identifies race as an important confounding factor in FC and casts doubt on the ability to predict achievement-related features from both FC and SNP data.</p><p id="P44">Finally, we note that it is very easy to add additional datasets to explore into the ImageNomer program. To do so, follow the links given in <xref rid="S5" ref-type="sec">Section 2.3</xref> and read the corresponding instructions. Doing so requires following a Jupyter notebook, but once data is loaded into ImageNomer, it can be explored without writing any additional code. We find the ability to quickly visualize trends, correlations, and potential confounding effects provided by the ImageNomer software is invaluable to the ability to perform good and careful research. This is demonstrated by the rapid identification of a race confound on FC-based prediction of achievement using ImageNomer, despite the fact that many studies have attempted to predict achievement using FC with little or no mention of this effect (<xref rid="R25" ref-type="bibr">Pervaiz et al., 2020</xref>; <xref rid="R28" ref-type="bibr">Qu et al., 2021</xref>; <xref rid="R30" ref-type="bibr">Sayegh et al., 2014</xref>).</p></sec></body><back><ack id="S15"><title>Acknowledgments</title><p id="P45">The authors would like acknowledge the NIH (grants R01 GM109068, R01 MH104680, R01 MH107354, P20 GM103472, R01 EB020407, R01 EB006841, R56 MH124925, 5U19 AG055373), NSF (#1539067), and American Heart Association (#830166) for partial funding support.</p><p id="P46">PNC fMRI, SNP, and phenotype data came from the Neurodevelopmental Genomics: Trajectories of Complex Phenotypes database of genotypes and phenotypes repository, dbGaP Study Accession ID phs000607. v3. p2. BSNIP data came from the National Institutes of Mental Health (NIMH) Data Archive (NDA).</p><p id="P47">Part of the work on ImageNomer was conducted at the UBRITE Multiomics Hackathon (<ext-link xlink:href="https://hackathon.ubrite.org/" ext-link-type="uri">https://hackathon.ubrite.org/</ext-link>) sponsored by the University of Alabama at Birmingham. The authors would like to acknowledge the organizers and mentors at UAB for their help in fostering teamwork and innovation in the bioinformatics community.</p></ack><fn-group><fn fn-type="COI-statement" id="FN1"><p id="P49">Declaration of competing interest</p><p id="P50">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></fn></fn-group><sec sec-type="data-availability" id="S30"><title>Data availability</title><p id="P48">The authors do not have permission to share data.</p></sec><ref-list><title>References</title><ref id="R1"><mixed-citation publication-type="journal"><name><surname>Abraham</surname><given-names>A</given-names></name>, <name><surname>Pedregosa</surname><given-names>F</given-names></name>, <name><surname>Eickenberg</surname><given-names>M</given-names></name>, <name><surname>Gervais</surname><given-names>P</given-names></name>, <name><surname>Mueller</surname><given-names>A</given-names></name>, <name><surname>Kossaifi</surname><given-names>J</given-names></name>, <name><surname>Gramfort</surname><given-names>A</given-names></name>, <name><surname>Thirion</surname><given-names>B</given-names></name>, <name><surname>Varoquaux</surname><given-names>G</given-names></name>, <year>2014</year>. <article-title>Machine learning for neuroimaging with scikit-learn</article-title>. <source>Front. Neuroinf</source>
<volume>8</volume>.</mixed-citation></ref><ref id="R2"><mixed-citation publication-type="confproc"><name><surname>Abrol</surname><given-names>A</given-names></name>, <name><surname>Rokham</surname><given-names>H</given-names></name>, <name><surname>Calhoun</surname><given-names>VD</given-names></name>, <year>2019</year>. <source>Diagnostic and prognostic classification of brain disorders using residual learning on structural mri data</source>. In: <conf-name>2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society</conf-name>. <publisher-name>EMBC</publisher-name>, pp. <fpage>4084</fpage>&#x02013;<lpage>4088</lpage>.</mixed-citation></ref><ref id="R3"><mixed-citation publication-type="journal"><name><surname>Balducci</surname><given-names>T</given-names></name>, <name><surname>Rasgado-Toledo</surname><given-names>J</given-names></name>, <name><surname>Valencia</surname><given-names>A</given-names></name>, <name><surname>van Tol</surname><given-names>M-J</given-names></name>, <name><surname>Aleman</surname><given-names>A</given-names></name>, <name><surname>Garza-Villarreal</surname><given-names>EA</given-names></name>, <year>2022</year>. <source>A Behavioral, Clinical and Brain Imaging Dataset with Focus on Emotion Regulation of Females with Fibromyal- Gia</source>.</mixed-citation></ref><ref id="R4"><mixed-citation publication-type="journal"><name><surname>Belliveau</surname><given-names>JW</given-names></name>, <name><surname>Kennedy</surname><given-names>DN</given-names></name>, <name><surname>McKinstry</surname><given-names>RC</given-names></name>, <name><surname>Buchbinder</surname><given-names>BR</given-names></name>, <name><surname>Weisskoff</surname><given-names>RM</given-names></name>, <name><surname>Cohen</surname><given-names>MS</given-names></name>, <name><surname>Vevea</surname><given-names>JM</given-names></name>, <name><surname>Brady</surname><given-names>TJ</given-names></name>, <name><surname>Rosen</surname><given-names>BR</given-names></name>, <year>1991</year>. <article-title>Functional mapping of the human visual cortex by magnetic resonance imaging</article-title>. <source>Science</source>
<volume>254</volume>, <fpage>716</fpage>&#x02013;<lpage>719</lpage>, <comment>5032.</comment><pub-id pub-id-type="pmid">1948051</pub-id>
</mixed-citation></ref><ref id="R5"><mixed-citation publication-type="journal"><name><surname>Bennett</surname><given-names>CM</given-names></name>, <name><surname>Miller</surname><given-names>MB</given-names></name>, <year>2010</year>. <article-title>How reliable are the results from functional magnetic resonance imag- ing?</article-title>
<source>Ann. N. Y. Acad. Sci</source>
<volume>1191</volume>, <fpage>133</fpage>&#x02013;<lpage>155</lpage>. <comment>Mar.</comment><pub-id pub-id-type="pmid">20392279</pub-id>
</mixed-citation></ref><ref id="R6"><mixed-citation publication-type="journal"><name><surname>Calhoun</surname><given-names>V</given-names></name>, <name><surname>Adali</surname><given-names>T</given-names></name>, <name><surname>Pearlson</surname><given-names>G</given-names></name>, <name><surname>Pekar</surname><given-names>J</given-names></name>, <year>2001</year>. <article-title>A method for making group inferences from functional MRI data using independent component analysis</article-title>. <source>Hum. Brain Mapp</source>
<volume>14</volume> (<issue>3</issue>), <fpage>140</fpage>&#x02013;<lpage>151</lpage>.<pub-id pub-id-type="pmid">11559959</pub-id>
</mixed-citation></ref><ref id="R7"><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>Q</given-names></name>, <name><surname>Kong</surname><given-names>Y</given-names></name>, <name><surname>Gao</surname><given-names>W</given-names></name>, <name><surname>Mo</surname><given-names>L</given-names></name>, <year>2018</year>. <article-title>Effects of socioeconomic status, parent&#x02013;child relationship, and learning motivation on reading ability</article-title>. <source>Front. Psychol</source>
<volume>9</volume>.</mixed-citation></ref><ref id="R8"><mixed-citation publication-type="journal"><name><surname>Du</surname><given-names>Y</given-names></name>, <name><surname>Fu</surname><given-names>Z</given-names></name>, <name><surname>Calhoun</surname><given-names>VD</given-names></name>, <year>2018</year>. <article-title>Classification and prediction of brain disorders using functional connectivity: promising but challenging</article-title>. <source>Front. Neurosci</source>
<volume>12</volume>.</mixed-citation></ref><ref id="R9"><mixed-citation publication-type="journal"><name><surname>Gichoya</surname><given-names>JW</given-names></name>, <name><surname>Banerjee</surname><given-names>I</given-names></name>, <name><surname>Bhimireddy</surname><given-names>AR</given-names></name>, <name><surname>Burns</surname><given-names>JL</given-names></name>, <name><surname>Celi</surname><given-names>LA</given-names></name>, <name><surname>Chen</surname><given-names>L-C</given-names></name>, <name><surname>Correa</surname><given-names>R</given-names></name>, <name><surname>Dullerud</surname><given-names>N</given-names></name>, <name><surname>Ghassemi</surname><given-names>M</given-names></name>, <name><surname>Huang</surname><given-names>S-C</given-names></name>, <name><surname>Kuo</surname><given-names>P-C</given-names></name>, <name><surname>Lungren</surname><given-names>MP</given-names></name>, <name><surname>Palmer</surname><given-names>LJ</given-names></name>, <name><surname>Price</surname><given-names>BJ</given-names></name>, <name><surname>Purkayastha</surname><given-names>S</given-names></name>, <name><surname>Pyrros</surname><given-names>AT</given-names></name>, <name><surname>Oakden-Rayner</surname><given-names>L</given-names></name>, <name><surname>Okechukwu</surname><given-names>C</given-names></name>, <name><surname>Seyyed-Kalantari</surname><given-names>L</given-names></name>, <name><surname>Trivedi</surname><given-names>H</given-names></name>, <name><surname>Wang</surname><given-names>R</given-names></name>, <name><surname>Zaiman</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <year>2022</year>. <article-title>AI recognition of patient race in medical imaging: a modelling study</article-title>. <source>Lancet Digit Health</source>
<volume>4</volume>, <fpage>e406</fpage>&#x02013;<lpage>e414</lpage>.<pub-id pub-id-type="pmid">35568690</pub-id>
</mixed-citation></ref><ref id="R10"><mixed-citation publication-type="journal"><name><surname>Glessner</surname><given-names>JT</given-names></name>, <name><surname>Reilly</surname><given-names>MP</given-names></name>, <name><surname>Kim</surname><given-names>CE</given-names></name>, <name><surname>Takahashi</surname><given-names>N</given-names></name>, <name><surname>Albano</surname><given-names>A</given-names></name>, <name><surname>Hou</surname><given-names>C</given-names></name>, <name><surname>Bradfield</surname><given-names>JP</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Sleiman</surname><given-names>PMA</given-names></name>, <name><surname>Flory</surname><given-names>JH</given-names></name>, <name><surname>Imielinski</surname><given-names>M</given-names></name>, <name><surname>Frackelton</surname><given-names>EC</given-names></name>, <name><surname>Chiavacci</surname><given-names>R</given-names></name>, <name><surname>Thomas</surname><given-names>KA</given-names></name>, <name><surname>Garris</surname><given-names>M</given-names></name>, <name><surname>Otieno</surname><given-names>FG</given-names></name>, <name><surname>Davidson</surname><given-names>M</given-names></name>, <name><surname>Weiser</surname><given-names>M</given-names></name>, <name><surname>Reichenberg</surname><given-names>A</given-names></name>, <name><surname>Davis</surname><given-names>KL</given-names></name>, <name><surname>Friedman</surname><given-names>JI</given-names></name>, <name><surname>Cappola</surname><given-names>TP</given-names></name>, <name><surname>Margulies</surname><given-names>KB</given-names></name>, <name><surname>Rader</surname><given-names>DJ</given-names></name>, <name><surname>Grant</surname><given-names>SFA</given-names></name>, <name><surname>Buxbaum</surname><given-names>JD</given-names></name>, <name><surname>Gur</surname><given-names>RE</given-names></name>, <name><surname>Hakonarson</surname><given-names>H</given-names></name>, <year>2010</year>. <article-title>Strong synaptic transmission impact by copy number variations in schizophrenia</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>
<volume>107</volume>, <fpage>10584</fpage>&#x02013;<lpage>10589</lpage>.<pub-id pub-id-type="pmid">20489179</pub-id>
</mixed-citation></ref><ref id="R11"><mixed-citation publication-type="journal"><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name>, <name><surname>Esteban</surname><given-names>O</given-names></name>, <name><surname>Burns</surname><given-names>C</given-names></name>, <name><surname>Ziegler</surname><given-names>E</given-names></name>, <name><surname>Pinsard</surname><given-names>B</given-names></name>, <name><surname>Madison</surname><given-names>C</given-names></name>, <name><surname>Waskom</surname><given-names>M</given-names></name>, <name><surname>Ellis</surname><given-names>DG</given-names></name>, <name><surname>Clark</surname><given-names>D</given-names></name>, <name><surname>Dayan</surname><given-names>M</given-names></name>, <name><surname>Manh&#x000e3;es-Savio</surname><given-names>A</given-names></name>, <name><surname>Notter</surname><given-names>MP</given-names></name>, <name><surname>Johnson</surname><given-names>H</given-names></name>, <name><surname>Dewey</surname><given-names>BE</given-names></name>, <name><surname>Halchenko</surname><given-names>YO</given-names></name>, <name><surname>Hamalainen</surname><given-names>C</given-names></name>, <name><surname>Keshavan</surname><given-names>A</given-names></name>, <name><surname>Clark</surname><given-names>D</given-names></name>, <name><surname>Huntenburg</surname><given-names>JM</given-names></name>, <name><surname>Hanke</surname><given-names>M</given-names></name>, <name><surname>Nichols</surname><given-names>BN</given-names></name>, <name><surname>Wassermann</surname><given-names>D</given-names></name>, <name><surname>Eshaghi</surname><given-names>A</given-names></name>, <name><surname>Markiewicz</surname><given-names>C</given-names></name>, <name><surname>Varoquaux</surname><given-names>G</given-names></name>, <name><surname>Acland</surname><given-names>B</given-names></name>, <name><surname>Forbes</surname><given-names>J</given-names></name>, <name><surname>Rokem</surname><given-names>A</given-names></name>, <name><surname>Kong</surname><given-names>X-Z</given-names></name>, <name><surname>Gramfort</surname><given-names>A</given-names></name>, <name><surname>Kleesiek</surname><given-names>J</given-names></name>, <name><surname>Schaefer</surname><given-names>A</given-names></name>, <name><surname>Sikka</surname><given-names>S</given-names></name>, <name><surname>Perez-Guevara</surname><given-names>MF</given-names></name>, <name><surname>Glatard</surname><given-names>T</given-names></name>, <name><surname>Iqbal</surname><given-names>S</given-names></name>, <name><surname>Liu</surname><given-names>S</given-names></name>, <name><surname>Welch</surname><given-names>D</given-names></name>, <name><surname>Sharp</surname><given-names>P</given-names></name>, <name><surname>Warner</surname><given-names>J</given-names></name>, <name><surname>Kastman</surname><given-names>E</given-names></name>, <name><surname>Lampe</surname><given-names>L</given-names></name>, <name><surname>Perkins</surname><given-names>LN</given-names></name>, <name><surname>Craddock</surname><given-names>RC</given-names></name>, <name><surname>K&#x000fc;ttner</surname><given-names>R</given-names></name>, <name><surname>Bielievtsov</surname><given-names>D</given-names></name>, <name><surname>Geisler</surname><given-names>D</given-names></name>, <name><surname>Gerhard</surname><given-names>S</given-names></name>, <name><surname>Liem</surname><given-names>F</given-names></name>, <name><surname>Linkersd&#x000f6;rfer</surname><given-names>J</given-names></name>, <name><surname>Margulies</surname><given-names>DS</given-names></name>, <name><surname>Andberg</surname><given-names>SK</given-names></name>, <name><surname>Stadler</surname><given-names>J</given-names></name>, <name><surname>Steele</surname><given-names>CJ</given-names></name>, <name><surname>Broderick</surname><given-names>W</given-names></name>, <name><surname>Cooper</surname><given-names>G</given-names></name>, <name><surname>Floren</surname><given-names>A</given-names></name>, <name><surname>Huang</surname><given-names>L</given-names></name>, <name><surname>Gonzalez</surname><given-names>I</given-names></name>, <name><surname>McNamee</surname><given-names>D</given-names></name>, <name><surname>Papadopoulos Orfanos</surname><given-names>D</given-names></name>, <name><surname>Pellman</surname><given-names>J</given-names></name>, <name><surname>Triplett</surname><given-names>W</given-names></name>, <name><surname>Ghosh</surname><given-names>S</given-names></name>, <year>2016</year>. <source>Nipype: a Flexible, Lightweight and Extensible Neuroimaging Data Processing Framework in Python. 0.12.0-rc1</source>,&#x0201d;.</mixed-citation></ref><ref id="R12"><mixed-citation publication-type="journal"><name><surname>Greicius</surname><given-names>MD</given-names></name>, <name><surname>Krasnow</surname><given-names>B</given-names></name>, <name><surname>Reiss</surname><given-names>AL</given-names></name>, <name><surname>Menon</surname><given-names>V</given-names></name>, <year>2003</year>. <article-title>Functional connectivity in the resting brain: a network analysis of the default mode hypothesis</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>
<volume>100</volume> (<issue>1</issue>), <fpage>253</fpage>&#x02013;<lpage>258</lpage>.<pub-id pub-id-type="pmid">12506194</pub-id>
</mixed-citation></ref><ref id="R13"><mixed-citation publication-type="journal"><name><surname>Harris</surname><given-names>CR</given-names></name>, <name><surname>Millman</surname><given-names>KJ</given-names></name>, <name><surname>van der Walt</surname><given-names>SJ</given-names></name>, <name><surname>Gommers</surname><given-names>R</given-names></name>, <name><surname>Virtanen</surname><given-names>P</given-names></name>, <name><surname>Cournapeau</surname><given-names>D</given-names></name>, <name><surname>Wieser</surname><given-names>E</given-names></name>, <name><surname>Taylor</surname><given-names>J</given-names></name>, <name><surname>Berg</surname><given-names>S</given-names></name>, <name><surname>Smith</surname><given-names>NJ</given-names></name>, <name><surname>Kern</surname><given-names>R</given-names></name>, <name><surname>Picus</surname><given-names>M</given-names></name>, <name><surname>Hoyer</surname><given-names>S</given-names></name>, <name><surname>van Kerkwijk</surname><given-names>MH</given-names></name>, <name><surname>Brett</surname><given-names>M</given-names></name>, <name><surname>Haldane</surname><given-names>A</given-names></name>, <name><surname>del R&#x02019;io</surname><given-names>JF</given-names></name>, <name><surname>Wiebe</surname><given-names>M</given-names></name>, <name><surname>Peterson</surname><given-names>P</given-names></name>, <name><surname>G&#x02019;erard-Marchant</surname><given-names>P</given-names></name>, <name><surname>Sheppard</surname><given-names>K</given-names></name>, <name><surname>Reddy</surname><given-names>T</given-names></name>, <name><surname>Weckesser</surname><given-names>W</given-names></name>, <name><surname>Abbasi</surname><given-names>H</given-names></name>, <name><surname>Gohlke</surname><given-names>C</given-names></name>, <name><surname>Oliphant</surname><given-names>TE</given-names></name>, <year>2020</year>. <article-title>Array programming with NumPy</article-title>. <source>Nature</source>
<volume>585</volume>, <fpage>357</fpage>&#x02013;<lpage>362</lpage>.<pub-id pub-id-type="pmid">32939066</pub-id>
</mixed-citation></ref><ref id="R14"><mixed-citation publication-type="journal"><name><surname>Hu</surname><given-names>W</given-names></name>, <name><surname>Meng</surname><given-names>X</given-names></name>, <name><surname>Bai</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>A</given-names></name>, <name><surname>Qu</surname><given-names>G</given-names></name>, <name><surname>Cai</surname><given-names>B</given-names></name>, <name><surname>Zhang</surname><given-names>G</given-names></name>, <name><surname>Wilson</surname><given-names>TW</given-names></name>, <name><surname>Stephen</surname><given-names>JM</given-names></name>, <name><surname>Calhoun</surname><given-names>VD</given-names></name>, <name><surname>Wang</surname><given-names>Y-P</given-names></name>, <year>2021</year>. <article-title>Interpretable multimodal fusion networks reveal mechanisms of brain cognition</article-title>. <source>IEEE Trans. Med. Imag</source>
<volume>40</volume>, <fpage>1474</fpage>&#x02013;<lpage>1483</lpage>.</mixed-citation></ref><ref id="R15"><mixed-citation publication-type="journal"><name><surname>Krishnan</surname><given-names>A</given-names></name>, <name><surname>Zhang</surname><given-names>R</given-names></name>, <name><surname>Yao</surname><given-names>V</given-names></name>, <name><surname>Theesfeld</surname><given-names>CL</given-names></name>, <name><surname>Wong</surname><given-names>AK</given-names></name>, <name><surname>Tadych</surname><given-names>A</given-names></name>, <name><surname>Volfovsky</surname><given-names>N</given-names></name>, <name><surname>Packer</surname><given-names>A</given-names></name>, <name><surname>Lash</surname><given-names>A</given-names></name>, <name><surname>Troyanskaya</surname><given-names>OG</given-names></name>, <year>2016</year>. <article-title>Genome-wide prediction and functional characterization of the genetic basis of autism spectrum disorder</article-title>. <source>Nat. Neurosci</source>
<volume>19</volume>, <fpage>1454</fpage>&#x02013;<lpage>1462</lpage>.<pub-id pub-id-type="pmid">27479844</pub-id>
</mixed-citation></ref><ref id="R16"><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>J</given-names></name>, <name><surname>Bzdok</surname><given-names>D</given-names></name>, <name><surname>Chen</surname><given-names>J</given-names></name>, <name><surname>Tam</surname><given-names>A</given-names></name>, <name><surname>Ooi</surname><given-names>LQR</given-names></name>, <name><surname>Holmes</surname><given-names>AJ</given-names></name>, <name><surname>Ge</surname><given-names>T</given-names></name>, <name><surname>Patil</surname><given-names>KR</given-names></name>, <name><surname>Jabbi</surname><given-names>M</given-names></name>, <name><surname>Eickhoff</surname><given-names>SB</given-names></name>, <name><surname>Yeo</surname><given-names>BTT</given-names></name>, <name><surname>Genon</surname><given-names>S</given-names></name>, <year>2022</year>. <article-title>Cross-ethnicity/race generalization failure of behavioral prediction from resting-state functional connectivity</article-title>. <source>Sci. Adv</source>
<volume>8</volume>, <fpage>eabj1812</fpage>.<pub-id pub-id-type="pmid">35294251</pub-id>
</mixed-citation></ref><ref id="R17"><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Xu</surname><given-names>L</given-names></name>, <name><surname>Li</surname><given-names>J</given-names></name>, <name><surname>Yu</surname><given-names>J</given-names></name>, <name><surname>Yu</surname><given-names>X</given-names></name>, <year>2020</year>. <article-title>Attentional connectivity-based prediction of autism using heterogeneous rs-fMRI data from CC200 atlas</article-title>. <source>Exp. Neurobiol</source>
<volume>29</volume>, <fpage>27</fpage>&#x02013;<lpage>37</lpage>.<pub-id pub-id-type="pmid">32122106</pub-id>
</mixed-citation></ref><ref id="R18"><mixed-citation publication-type="journal"><name><surname>Lyon</surname><given-names>L</given-names></name>, <year>2017</year>. <article-title>Dead salmon and voodoo correlations: should we be sceptical about functional MRI?</article-title>
<source>Brain</source>
<volume>140</volume>, <fpage>e53</fpage>.<pub-id pub-id-type="pmid">28899026</pub-id>
</mixed-citation></ref><ref id="R19"><mixed-citation publication-type="journal"><name><surname>Markiewicz</surname><given-names>CJ</given-names></name>, <name><surname>Gorgolewski</surname><given-names>KJ</given-names></name>, <name><surname>Feingold</surname><given-names>F</given-names></name>, <name><surname>Blair</surname><given-names>R</given-names></name>, <name><surname>Halchenko</surname><given-names>YO</given-names></name>, <name><surname>Miller</surname><given-names>E</given-names></name>, <name><surname>Hardcastle</surname><given-names>N</given-names></name>, <name><surname>Wexler</surname><given-names>J</given-names></name>, <name><surname>Esteban</surname><given-names>O</given-names></name>, <name><surname>Goncavles</surname><given-names>M</given-names></name>, <name><surname>Jwa</surname><given-names>A</given-names></name>, <name><surname>Poldrack</surname><given-names>R</given-names></name>, <year>2021</year>. <article-title>The OpenNeuro resource for sharing of neuroscience data</article-title>. <source>Elife</source>
<volume>10</volume>.</mixed-citation></ref><ref id="R20"><mixed-citation publication-type="journal"><name><surname>Millar</surname><given-names>PR</given-names></name>, <name><surname>Luckett</surname><given-names>PH</given-names></name>, <name><surname>Gordon</surname><given-names>BA</given-names></name>, <name><surname>Benzinger</surname><given-names>TL</given-names></name>, <year>2022</year>. <article-title>Predicting brain age from functional connectivity in symptomatic and preclinical alzheimer disease</article-title>. <source>Neuroimage</source>
<volume>256</volume>, <fpage>119228</fpage>.<pub-id pub-id-type="pmid">35452806</pub-id>
</mixed-citation></ref><ref id="R21"><mixed-citation publication-type="journal"><name><surname>Orlichenko</surname><given-names>A</given-names></name>, <name><surname>Qu</surname><given-names>G</given-names></name>, <name><surname>Zhang</surname><given-names>G</given-names></name>, <name><surname>Patel</surname><given-names>B</given-names></name>, <name><surname>Wilson</surname><given-names>TW</given-names></name>, <name><surname>Stephen</surname><given-names>JM</given-names></name>, <name><surname>Calhoun</surname><given-names>VD</given-names></name>, <name><surname>Wang</surname><given-names>Y-P</given-names></name>, <year>2022</year>. <article-title>Latent similarity identifies important functional connections for phenotype prediction</article-title>. <source>IEEE Trans. Biomed. Eng</source>
<fpage>1</fpage>&#x02013;<lpage>12</lpage>.</mixed-citation></ref><ref id="R22"><mixed-citation publication-type="journal"><name><surname>Ott</surname><given-names>LR</given-names></name>, <name><surname>Penhale</surname><given-names>SH</given-names></name>, <name><surname>Taylor</surname><given-names>BK</given-names></name>, <name><surname>Lew</surname><given-names>BJ</given-names></name>, <name><surname>Wang</surname><given-names>Y-P</given-names></name>, <name><surname>Calhoun</surname><given-names>VD</given-names></name>, <name><surname>Stephen</surname><given-names>JM</given-names></name>, <name><surname>Wilson</surname><given-names>TW</given-names></name>, <year>2021</year>. <article-title>Spontaneous cortical MEG activity undergoes unique age- and sex-related changes during the transition to adolescence</article-title>. <source>Neuroimage</source>
<volume>244</volume>, <fpage>118552</fpage>.<pub-id pub-id-type="pmid">34517128</pub-id>
</mixed-citation></ref><ref id="R23"><mixed-citation publication-type="journal"><name><surname>Paszke</surname><given-names>A</given-names></name>, <name><surname>Gross</surname><given-names>S</given-names></name>, <name><surname>Massa</surname><given-names>F</given-names></name>, <name><surname>Lerer</surname><given-names>A</given-names></name>, <name><surname>Bradbury</surname><given-names>J</given-names></name>, <name><surname>Chanan</surname><given-names>G</given-names></name>, <name><surname>Killeen</surname><given-names>T</given-names></name>, <name><surname>Lin</surname><given-names>Z</given-names></name>, <name><surname>Gimelshein</surname><given-names>N</given-names></name>, <name><surname>Antiga</surname><given-names>L</given-names></name>, <name><surname>Desmaison</surname><given-names>A</given-names></name>, <name><surname>Kopf</surname><given-names>A</given-names></name>, <name><surname>Yang</surname><given-names>E</given-names></name>, <name><surname>DeVito</surname><given-names>Z</given-names></name>, <name><surname>Raison</surname><given-names>M</given-names></name>, <name><surname>Tejani</surname><given-names>A</given-names></name>, <name><surname>Chilamkurthy</surname><given-names>S</given-names></name>, <name><surname>Steiner</surname><given-names>B</given-names></name>, <name><surname>Fang</surname><given-names>L</given-names></name>, <name><surname>Bai</surname><given-names>J</given-names></name>, <name><surname>Chintala</surname><given-names>S</given-names></name>, <year>2019</year>. <article-title>Pytorch: an imperative style, high-performance deep learning library</article-title>. <source>Adv. Neural Inf. Process. Syst</source>
<volume>32</volume>, <fpage>8024</fpage>&#x02013;<lpage>8035</lpage>. <comment>Curran Associates, Inc.</comment></mixed-citation></ref><ref id="R24"><mixed-citation publication-type="journal"><name><surname>Pedregosa</surname><given-names>F</given-names></name>, <name><surname>Varoquaux</surname><given-names>G</given-names></name>, <name><surname>Gramfort</surname><given-names>A</given-names></name>, <name><surname>Michel</surname><given-names>V</given-names></name>, <name><surname>Thirion</surname><given-names>B</given-names></name>, <name><surname>Grisel</surname><given-names>O</given-names></name>, <name><surname>Blondel</surname><given-names>M</given-names></name>, <name><surname>Prettenhofer</surname><given-names>P</given-names></name>, <name><surname>Weiss</surname><given-names>R</given-names></name>, <name><surname>Dubourg</surname><given-names>V</given-names></name>, <name><surname>Vanderplas</surname><given-names>J</given-names></name>, <name><surname>Passos</surname><given-names>A</given-names></name>, <name><surname>Cournapeau</surname><given-names>D</given-names></name>, <name><surname>Brucher</surname><given-names>M</given-names></name>, <name><surname>Perrot</surname><given-names>M</given-names></name>, <name><surname>Duchesnay</surname><given-names>E</given-names></name>, <year>2011</year>. <article-title>Scikit-learn: machine learning in Python</article-title>. <source>J. Mach. Learn. Res</source>
<volume>12</volume>, <fpage>2825</fpage>&#x02013;<lpage>2830</lpage>.</mixed-citation></ref><ref id="R25"><mixed-citation publication-type="journal"><name><surname>Pervaiz</surname><given-names>U</given-names></name>, <name><surname>Vidaurre</surname><given-names>D</given-names></name>, <name><surname>Woolrich</surname><given-names>MW</given-names></name>, <name><surname>Smith</surname><given-names>SM</given-names></name>, <year>2020</year>. <article-title>&#x0201c;Optimising network modelling methods for fMRI</article-title>. <source>Neuroimage</source>
<volume>211</volume>, <fpage>116604</fpage>.<pub-id pub-id-type="pmid">32062083</pub-id>
</mixed-citation></ref><ref id="R26"><mixed-citation publication-type="journal"><name><surname>Plis</surname><given-names>SM</given-names></name>, <name><surname>Sarwate</surname><given-names>AD</given-names></name>, <name><surname>Wood</surname><given-names>D</given-names></name>, <name><surname>Dieringer</surname><given-names>C</given-names></name>, <name><surname>Landis</surname><given-names>D</given-names></name>, <name><surname>Reed</surname><given-names>C</given-names></name>, <name><surname>Panta</surname><given-names>SR</given-names></name>, <name><surname>Turner</surname><given-names>JA</given-names></name>, <name><surname>Shoemaker</surname><given-names>JM</given-names></name>, <name><surname>Carter</surname><given-names>KW</given-names></name>, <name><surname>Thompson</surname><given-names>P</given-names></name>, <name><surname>Hutchison</surname><given-names>K</given-names></name>, <name><surname>Calhoun</surname><given-names>VD</given-names></name>, <year>2016</year>. &#x0201c;<article-title>COINSTAC: a privacy enabled model and prototype for leveraging and processing decentralized brain imaging data</article-title>,.&#x0201d;. <source>Front. Neurosci</source>
<volume>10</volume>.</mixed-citation></ref><ref id="R27"><mixed-citation publication-type="journal"><name><surname>Power</surname><given-names>JD</given-names></name>, <name><surname>Cohen</surname><given-names>AL</given-names></name>, <name><surname>Nelson</surname><given-names>SM</given-names></name>, <name><surname>Wig</surname><given-names>GS</given-names></name>, <name><surname>Barnes</surname><given-names>KA</given-names></name>, <name><surname>Church</surname><given-names>JA</given-names></name>, <name><surname>Vogel</surname><given-names>AC</given-names></name>, <name><surname>Laumann</surname><given-names>TO</given-names></name>, <name><surname>Miezin</surname><given-names>FM</given-names></name>, <name><surname>Schlaggar</surname><given-names>BL</given-names></name>, <name><surname>Petersen</surname><given-names>SE</given-names></name>, <year>2011</year>. <article-title>Functional network organization of the human brain</article-title>. <source>Neuron</source>
<volume>72</volume>, <fpage>665</fpage>&#x02013;<lpage>678</lpage>.<pub-id pub-id-type="pmid">22099467</pub-id>
</mixed-citation></ref><ref id="R28"><mixed-citation publication-type="journal"><name><surname>Qu</surname><given-names>G</given-names></name>, <name><surname>Xiao</surname><given-names>L</given-names></name>, <name><surname>Hu</surname><given-names>W</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>K</given-names></name>, <name><surname>Calhoun</surname><given-names>VD</given-names></name>, <name><surname>ping Wang</surname><given-names>Y</given-names></name>, <year>2021</year>. <article-title>Ensemble manifold regularized multi-modal graph convolutional network for cognitive ability prediction</article-title>. <source>IEEE (Inst. Electr. Electron. Eng.) Trans. Biomed. Eng</source>
<volume>68</volume>, <fpage>3564</fpage>&#x02013;<lpage>3573</lpage>.</mixed-citation></ref><ref id="R29"><mixed-citation publication-type="journal"><name><surname>Satterthwaite</surname><given-names>TD</given-names></name>, <name><surname>Elliott</surname><given-names>MA</given-names></name>, <name><surname>Ruparel</surname><given-names>K</given-names></name>, <name><surname>Loughead</surname><given-names>J</given-names></name>, <name><surname>Prabhakaran</surname><given-names>K</given-names></name>, <name><surname>Calkins</surname><given-names>ME</given-names></name>, <name><surname>Hop- son</surname><given-names>R</given-names></name>, <name><surname>Jackson</surname><given-names>C</given-names></name>, <name><surname>Keefe</surname><given-names>J</given-names></name>, <name><surname>Riley</surname><given-names>M</given-names></name>, <name><surname>Mentch</surname><given-names>FD</given-names></name>, <name><surname>Sleiman</surname><given-names>PMA</given-names></name>, <name><surname>Verma</surname><given-names>R</given-names></name>, <name><surname>Davatzikos</surname><given-names>C</given-names></name>, <name><surname>Hakonarson</surname><given-names>H</given-names></name>, <name><surname>Gur</surname><given-names>RC</given-names></name>, <name><surname>Gur</surname><given-names>RE</given-names></name>, <year>2014</year>. <article-title>Neuroimaging of the philadelphia neurodevelopmental cohort</article-title>. <source>Neuroimage</source>
<volume>86</volume>, <fpage>544</fpage>&#x02013;<lpage>553</lpage>.<pub-id pub-id-type="pmid">23921101</pub-id>
</mixed-citation></ref><ref id="R30"><mixed-citation publication-type="journal"><name><surname>Sayegh</surname><given-names>P</given-names></name>, <name><surname>Arentoft</surname><given-names>A</given-names></name>, <name><surname>Thaler</surname><given-names>NS</given-names></name>, <name><surname>Dean</surname><given-names>AC</given-names></name>, <name><surname>Thames</surname><given-names>AD</given-names></name>, <year>2014</year>. <article-title>Quality of education predicts performance on the wide range achievement test-4th edition word reading subtest. Arch. Clin. Neuropsychol.: the Off</article-title>. <source>J. Natl. Acad. Neuropsychol</source>
<volume>29</volume> (<issue>8</issue>), <fpage>731</fpage>&#x02013;<lpage>736</lpage>.</mixed-citation></ref><ref id="R31"><mixed-citation publication-type="journal"><name><surname>Szucs</surname><given-names>D</given-names></name>, <name><surname>Ioannidis</surname><given-names>JP</given-names></name>, <year>2020</year>. <article-title>&#x0201c;Sample size evolution in neuroimaging research: an evaluation of highly- cited studies (1990&#x02013;2012) and of latest practices (2017&#x02013;2018) in high-impact journals</article-title>,&#x0201d;. <source>Neuroimage</source>
<volume>221</volume>, <fpage>117164</fpage>.<pub-id pub-id-type="pmid">32679253</pub-id>
</mixed-citation></ref><ref id="R32"><mixed-citation publication-type="journal"><name><surname>Tamminga</surname><given-names>CA</given-names></name>, <name><surname>Pearlson</surname><given-names>G</given-names></name>, <name><surname>Keshavan</surname><given-names>M</given-names></name>, <name><surname>Sweeney</surname><given-names>J</given-names></name>, <name><surname>Clementz</surname><given-names>B</given-names></name>, <name><surname>Thaker</surname><given-names>G</given-names></name>, <year>2014</year>. <article-title>Bipolar and schizophrenia network for intermediate phenotypes: outcomes across the psychosis continuum</article-title>. <source>Schizophr. Bull</source>
<volume>40</volume>, <fpage>S131</fpage>&#x02013;<lpage>S137</lpage>.<pub-id pub-id-type="pmid">24562492</pub-id>
</mixed-citation></ref><ref id="R33"><mixed-citation publication-type="journal"><name><surname>Turner</surname><given-names>BO</given-names></name>, <name><surname>Paul</surname><given-names>EJ</given-names></name>, <name><surname>Miller</surname><given-names>MB</given-names></name>, <name><surname>Barbey</surname><given-names>AK</given-names></name>, <year>2018</year>. <article-title>Small sample sizes reduce the replicability of task-based fMRI studies</article-title>. <source>Commun. Biol</source>
<volume>1</volume>, <fpage>62</fpage>.<pub-id pub-id-type="pmid">30271944</pub-id>
</mixed-citation></ref><ref id="R34"><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Xin</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Yao</surname><given-names>Y</given-names></name>, <name><surname>Zhao</surname><given-names>Y</given-names></name>, <name><surname>Qian</surname><given-names>W</given-names></name>, <year>2021</year>. <article-title>Brain functional network modeling and analysis based on fMRI: a systematic review</article-title>. <source>Cogn. Neurodyn</source>
<volume>15</volume>, <fpage>389</fpage>&#x02013;<lpage>403</lpage>.<pub-id pub-id-type="pmid">34040667</pub-id>
</mixed-citation></ref><ref id="R35"><mixed-citation publication-type="journal"><name><surname>Weber</surname><given-names>R</given-names></name>, <name><surname>Hopp</surname><given-names>FR</given-names></name>, <name><surname>Eden</surname><given-names>A</given-names></name>, <name><surname>Lee</surname><given-names>K</given-names></name>, <year>2023</year>. <source>Vicarious Punishment of Moral Violations in Naturalistic Drama Narratives Predicts Cortical Synchronization</source>.</mixed-citation></ref><ref id="R36"><mixed-citation publication-type="journal"><name><surname>Xia</surname><given-names>M</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>He</surname><given-names>Y</given-names></name>, <year>2013</year>. <article-title>BrainNet viewer: a network visualization tool for human brain connec- tomics</article-title>. <source>PLoS One</source>
<volume>8</volume>, <fpage>e68910</fpage>.<pub-id pub-id-type="pmid">23861951</pub-id>
</mixed-citation></ref><ref id="R37"><mixed-citation publication-type="journal"><name><surname>Xifra-Porxas</surname><given-names>A</given-names></name>, <name><surname>Ghosh</surname><given-names>A</given-names></name>, <name><surname>Mitsis</surname><given-names>GD</given-names></name>, <name><surname>Boudrias</surname><given-names>M-H</given-names></name>, <year>2021</year>. <article-title>Estimating brain age from structural MRI and MEG data: insights from dimensionality reduction techniques</article-title>. <source>Neuroimage</source>
<volume>231</volume>, <fpage>117822</fpage>.<pub-id pub-id-type="pmid">33549751</pub-id>
</mixed-citation></ref><ref id="R38"><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Liang</surname><given-names>M</given-names></name>, <name><surname>Qin</surname><given-names>W</given-names></name>, <name><surname>Wan</surname><given-names>B</given-names></name>, <name><surname>Yu</surname><given-names>C</given-names></name>, <name><surname>Ming</surname><given-names>D</given-names></name>, <year>2020</year>. <article-title>Gender differences are encoded differ- ently in the structure and function of the human brain revealed by multimodal mri</article-title>. <source>Front. Hum. Neurosci</source>
<volume>14</volume>.</mixed-citation></ref></ref-list></back><floats-group><fig position="float" id="F1"><label>Fig. 1.</label><caption><p id="P51">Comparison of existing toolkits for analysis of fMRI-based FC data with our ImageNomer software. A more comprehensive list may be found at <ext-link xlink:href="https://en.wikipedia.org/wiki/" ext-link-type="uri">https://en.wikipedia.org/wiki/List_of_functional_connectivity_software</ext-link>.</p></caption><graphic xlink:href="nihms-1951039-f0001" position="float"/></fig><fig position="float" id="F2"><label>Fig. 2.</label><caption><p id="P52">Overview of the ImageNomer architecture.</p></caption><graphic xlink:href="nihms-1951039-f0002" position="float"/></fig><fig position="float" id="F3"><label>Fig. 3.</label><caption><p id="P53">Main view of the ImageNomer program showing resting state FC for all subjects along with demographic data.</p></caption><graphic xlink:href="nihms-1951039-f0003" position="float"/></fig><fig position="float" id="F4"><label>Fig. 4.</label><caption><p id="P54">Mean FC in the PNC dataset along with the Power 264 template (<xref rid="R27" ref-type="bibr">Power et al., 2011</xref>) regions used to sample BOLD signal from brain regions.</p></caption><graphic xlink:href="nihms-1951039-f0004" position="float"/></fig><fig position="float" id="F5"><label>Fig. 5.</label><caption><p id="P55">Demographics of our subset of the PNC dataset. Plots of age vs sex, WRAT vs sex, and WRAT vs age are shown. All plots created using the ImageNomer GUI, without programming input.</p></caption><graphic xlink:href="nihms-1951039-f0005" position="float"/></fig><fig position="float" id="F6"><label>Fig. 6.</label><caption><p id="P56">Distribution of race vs age. We see that there is no race bias on age distribution.</p></caption><graphic xlink:href="nihms-1951039-f0006" position="float"/></fig><fig position="float" id="F7"><label>Fig. 7.</label><caption><p id="P57">Examining race bias on WRAT score in the PNC dataset using ImageNomer. We find whereas age has been regressed from WRAT score, there is still a large racial bias.</p></caption><graphic xlink:href="nihms-1951039-f0007" position="float"/></fig><fig position="float" id="F8"><label>Fig. 8.</label><caption><p id="P58">Correlation between race and FC is much higher than the correlation between race and WRAT score. Additionally, in almost all regions, achievement score-correlated FC is a subset of race-correlated FC.</p></caption><graphic xlink:href="nihms-1951039-f0008" position="float"/></fig><fig position="float" id="F9"><label>Fig. 9.</label><caption><p id="P59">ImageNomer identifies a large overlap between SNPs correlated with race and SNPs correlated with WRAT score. Out of the top 20, 6 of the same SNP appear in both groups (marked with red asterisks). (For interpretation of the references to colour in this figure legend, the reader is referred to the Web version of this article.)</p></caption><graphic xlink:href="nihms-1951039-f0009" position="float"/></fig><fig position="float" id="F10"><label>Fig. 10.</label><caption><p id="P60">Correlation of FC with race in the PNC and BSNIP datasets. Although BSNIP p-values are larger, we see the same pattern of correlation. Additionally, the overall FC in the Default Mode Network (DMN), highlighted in pink, seems to be highly predictive of race. (For interpretation of the references to colour in this figure legend, the reader is referred to the Web version of this article.)</p></caption><graphic xlink:href="nihms-1951039-f0010" position="float"/></fig><fig position="float" id="F11"><label>Fig. 11.</label><caption><p id="P61">Top: distribution of mother and father education level by race category and distribution of WRAT score with mother education level. Bottom: correlation of FC with mother education level and WRAT score. Correlation consists of a correlation image and the negative log base 10 of the Bonferroni-corrected p-value, clipped at &#x02212;5.</p></caption><graphic xlink:href="nihms-1951039-f0011" position="float"/></fig><table-wrap position="float" id="T1"><label>Table 1</label><caption><p id="P62">Summary of prediction results for full models (34,716 features for FC, 10,433 features for SNPs) and top 10 feature models in the PNC dataset. Top 10 features selected on the training set. Statistically significant results are shown in bold.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" valign="top" rowspan="1" colspan="1">Prediction</th><th align="left" valign="top" rowspan="1" colspan="1">Modality</th><th align="left" valign="top" rowspan="1" colspan="1">Metric</th><th align="left" valign="top" rowspan="1" colspan="1">Null Model</th><th align="left" valign="top" rowspan="1" colspan="1">Best Full Model</th><th align="left" valign="top" rowspan="1" colspan="1">Best 10 Features</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Age</td><td align="left" valign="top" rowspan="1" colspan="1">FC</td><td align="left" valign="top" rowspan="1" colspan="1">RMSE, months</td><td align="left" valign="top" rowspan="1" colspan="1">38.4</td><td align="left" valign="top" rowspan="1" colspan="1">
<bold>26</bold>
</td><td align="left" valign="top" rowspan="1" colspan="1">
<bold>32.2</bold>
</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">WRAT Score</td><td align="left" valign="top" rowspan="1" colspan="1">FC</td><td align="left" valign="top" rowspan="1" colspan="1">RMSE</td><td align="left" valign="top" rowspan="1" colspan="1">15.1</td><td align="left" valign="top" rowspan="1" colspan="1">
<bold>13.6</bold>
</td><td align="left" valign="top" rowspan="1" colspan="1">15.1</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">WRAT Score</td><td align="left" valign="top" rowspan="1" colspan="1">SNPs</td><td align="left" valign="top" rowspan="1" colspan="1">RMSE</td><td align="left" valign="top" rowspan="1" colspan="1">15.1</td><td align="left" valign="top" rowspan="1" colspan="1">
<bold>14</bold>
</td><td align="left" valign="top" rowspan="1" colspan="1">&#x02013;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">WRAT Score (AA)</td><td align="left" valign="top" rowspan="1" colspan="1">FC</td><td align="left" valign="top" rowspan="1" colspan="1">RMSE</td><td align="left" valign="top" rowspan="1" colspan="1">13.9</td><td align="left" valign="top" rowspan="1" colspan="1">13.8</td><td align="left" valign="top" rowspan="1" colspan="1">13.9</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">WPAT Score (AA)</td><td align="left" valign="top" rowspan="1" colspan="1">SNPs</td><td align="left" valign="top" rowspan="1" colspan="1">RMSE</td><td align="left" valign="top" rowspan="1" colspan="1">13.9</td><td align="left" valign="top" rowspan="1" colspan="1">13.4</td><td align="left" valign="top" rowspan="1" colspan="1">&#x02013;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">WRAT Score (EA)</td><td align="left" valign="top" rowspan="1" colspan="1">FC</td><td align="left" valign="top" rowspan="1" colspan="1">RMSE</td><td align="left" valign="top" rowspan="1" colspan="1">14</td><td align="left" valign="top" rowspan="1" colspan="1">14.1</td><td align="left" valign="top" rowspan="1" colspan="1">14</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">WPAT Score (EA)</td><td align="left" valign="top" rowspan="1" colspan="1">SNPs</td><td align="left" valign="top" rowspan="1" colspan="1">RMSE</td><td align="left" valign="top" rowspan="1" colspan="1">14</td><td align="left" valign="top" rowspan="1" colspan="1">13.6</td><td align="left" valign="top" rowspan="1" colspan="1">&#x02013;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Race</td><td align="left" valign="top" rowspan="1" colspan="1">FC</td><td align="left" valign="top" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="top" rowspan="1" colspan="1">55%</td><td align="left" valign="top" rowspan="1" colspan="1">
<bold>85%</bold>
</td><td align="left" valign="top" rowspan="1" colspan="1">
<bold>72%</bold>
</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Sex</td><td align="left" valign="top" rowspan="1" colspan="1">FC</td><td align="left" valign="top" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="top" rowspan="1" colspan="1">51%</td><td align="left" valign="top" rowspan="1" colspan="1">
<bold>78%</bold>
</td><td align="left" valign="top" rowspan="1" colspan="1">
<bold>62%</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T2"><label>Table 2</label><caption><p id="P63">Accuracy of transfer learning between the PNC and BSNIP datasets. All predictions are better than the null model, except for identification of the AA group in the PNC dataset by a model trained on BSNIP.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" valign="middle" rowspan="1" colspan="1">Trained on PNC</th><th align="left" valign="middle" rowspan="1" colspan="1"/><th align="left" valign="middle" rowspan="1" colspan="1">Trained on BSNIP</th><th align="left" valign="middle" rowspan="1" colspan="1"/></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Evaluation Group</td><td align="left" valign="middle" rowspan="1" colspan="1">Accuracy</td><td align="left" valign="middle" rowspan="1" colspan="1">Evaluation Group</td><td align="left" valign="middle" rowspan="1" colspan="1">Accuracy</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PNC (all, n = 733)</td><td align="left" valign="middle" rowspan="1" colspan="1">85 &#x000b1; 3%</td><td align="left" valign="middle" rowspan="1" colspan="1">BSNIP (all, n = 1165)</td><td align="left" valign="middle" rowspan="1" colspan="1">79 &#x000b1; 4%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BSNIP AA (n = 387)</td><td align="left" valign="middle" rowspan="1" colspan="1">76 &#x000b1; 5%</td><td align="left" valign="middle" rowspan="1" colspan="1">PNC EA (n = 407)</td><td align="left" valign="middle" rowspan="1" colspan="1">90 &#x000b1; 3%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">BSNIP CA (n = 778)</td><td align="left" valign="middle" rowspan="1" colspan="1">64 &#x000b1; 5%</td><td align="left" valign="middle" rowspan="1" colspan="1">PNC AA (n = 326)</td><td align="left" valign="middle" rowspan="1" colspan="1">38 &#x000b1; 7%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T3"><label>Table 3</label><caption><p id="P64">Prediction of WRAT score in the low SES (mother with no college education) versus high SES (mother with some college education). Predictive ability was barely significant in the low SES group, and not significant in the high SES group.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" valign="middle" rowspan="1" colspan="1">Group</th><th align="left" valign="middle" rowspan="1" colspan="1">WRAT Score RMSE</th><th align="left" valign="middle" rowspan="1" colspan="1">Null Model RMSE</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Low SES (Mother Education &#x02264;12 years)</td><td align="left" valign="middle" rowspan="1" colspan="1">14.1</td><td align="left" valign="middle" rowspan="1" colspan="1">15</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">High SES (Mother Education &#x02265;14 years)</td><td align="left" valign="middle" rowspan="1" colspan="1">15</td><td align="left" valign="middle" rowspan="1" colspan="1">15.3</td></tr></tbody></table></table-wrap></floats-group></article>
