<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">15613242</article-id><article-id pub-id-type="pmc">PMC544851</article-id><article-id pub-id-type="publisher-id">1471-2105-5-206</article-id><article-id pub-id-type="doi">10.1186/1471-2105-5-206</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>An empirical analysis of training protocols for probabilistic gene finders</article-title></title-group><contrib-group><contrib id="A1" corresp="yes" contrib-type="author"><name><surname>Majoros</surname><given-names>William H</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>bmajoros@tigr.org</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Salzberg</surname><given-names>Steven L</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>salzberg@tigr.org</email></contrib></contrib-group><aff id="I1"><label>1</label>The Institute for Genomic Research, 9712 Medical Center Drive, Rockville, MD 20850, USA</aff><pub-date pub-type="collection"><year>2004</year></pub-date><pub-date pub-type="epub"><day>21</day><month>12</month><year>2004</year></pub-date><volume>5</volume><fpage>206</fpage><lpage>206</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/5/206"/><history><date date-type="received"><day>5</day><month>11</month><year>2004</year></date><date date-type="accepted"><day>21</day><month>12</month><year>2004</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2004 Majoros and Salzberg; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2004</copyright-year><copyright-holder>Majoros and Salzberg; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>
               Majoros
               H
               William
               
               bmajoros@tigr.org
            </dc:author><dc:title>
            An empirical analysis of training protocols for probabilistic gene finders
         </dc:title><dc:date>2004</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 5(1): 206-. (2004)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2004)5:1&#x0003c;206&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p>Generalized hidden Markov models (GHMMs) appear to be approaching acceptance as a <italic>de facto </italic>standard for state-of-the-art <italic>ab initio </italic>gene finding, as evidenced by the recent proliferation of GHMM implementations. While prevailing methods for modeling and parsing genes using GHMMs have been described in the literature, little attention has been paid as of yet to their proper training. The few hints available in the literature together with anecdotal observations suggest that most practitioners perform maximum likelihood parameter estimation only at the local submodel level, and then attend to the optimization of global parameter structure using some form of <italic>ad hoc </italic>manual tuning of individual parameters.</p></sec><sec><title>Results</title><p>We decided to investigate the utility of applying a more systematic optimization approach to the tuning of global parameter structure by implementing a global discriminative training procedure for our GHMM-based gene finder. Our results show that significant improvement in prediction accuracy can be achieved by this method.</p></sec><sec><title>Conclusions</title><p>We conclude that training of GHMM-based gene finders is best performed using some form of discriminative training rather than simple maximum likelihood estimation at the submodel level, and that generalized gradient ascent methods are suitable for this task. We also conclude that partitioning of training data for the twin purposes of maximum likelihood initialization and gradient ascent optimization appears to be unnecessary, but that strict segregation of test data must be enforced during final gene finder evaluation to avoid artificially inflated accuracy measurements.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>The number of generalized hidden Markov model (GHMM) gene finders reported in the literature has increased fairly dramatically of late [<xref ref-type="bibr" rid="B1">1</xref>-<xref ref-type="bibr" rid="B8">8</xref>], and the community is now contemplating various ways to extend this attractive framework in order to incorporate homology information, with a handful of such systems having already been built (e.g., [<xref ref-type="bibr" rid="B9">9</xref>-<xref ref-type="bibr" rid="B12">12</xref>]). GHMMs offer a number of clear advantages which would seem to explain this growth in popularity. Chief among these is the fact that the GHMM framework, being (in theory) purely probabilistic, allows for principled approaches to constructing, utilizing, and extending models for accurate prediction of gene structures.</p><p>While the decoding problem for GHMM gene finders is arguably well understood, being a relatively straightforward extension of the same problem for traditional HMMs and amenable to a Viterbi-like solution (albeit a more complex one), methods for optimally training a GHMM gene finder have received scant attention in the gene-finding literature to date. What information is available (e.g., [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B4">4</xref>]) seems to indicate that the common practice is to optimize the submodels of the GHMM independently, without regard for the optimality of the composite model.</p><p>The training of HMMs and GHMMs has traditionally been carried out using some form of <italic>maximum likelihood estimation </italic>(MLE). Baum-Welch training [<xref ref-type="bibr" rid="B13">13</xref>], which is an instance of the well-known <italic>expectation maximization </italic>(EM) procedure, is itself a form of MLE [<xref ref-type="bibr" rid="B14">14</xref>]. In the case of GHMM gene finders, one typically applies some form of MLE to each of the submodels (states) in the GHMM so as to render training features of each type (e.g., exon, intron, donor site) maximally likely under the induced (sub)model; i.e., maximizing:</p><p><inline-graphic xlink:href="1471-2105-5-206-i1.gif"/></p><p>for state <italic>q </italic>and for <italic>S</italic><sub><italic>i </italic></sub>a feature of length <italic>d</italic><sub><italic>i </italic></sub>from the state-<italic>q</italic>-specific training set <italic>T</italic>. The submodels are then merged into a composite model (i.e., the full GHMM) by observing transition probabilities between features in the training data corresponding to each of the GHMM states.</p><p>For example, an exon state in a GHMM can be trained by collecting <italic>n</italic>-gram statistics (i.e., counts of <italic>n</italic>-letter substrings) from known exon sequences and normalizing these into transition probabilities for an (<italic>n</italic>-1)<sup>th</sup>-order Markov chain [<xref ref-type="bibr" rid="B15">15</xref>]. Similarly, intron, intergenic, and untranslated region (UTR) states can be modeled by collecting appropriate statistics from corresponding sample features and using these to train individual content-scoring models, such as Markov chains, neural networks, decision trees, etc. Signal sensors for donor and acceptor splice sites and start and stop codons can be trained by aligning known signals of the appropriate type and counting nucleotide frequencies at each position within a fixed window around the signal; converting these counts to relative frequencies produces probability estimates for use in a weight matrix or similar type of model. Transition and duration probabilities can likewise be estimated by observing appropriate frequencies in training data. All of these estimation activities can be performed independently, resulting in a GHMM consisting of distinct subsets of maximum likelihood parameters.</p><p>Such an approach does not, however, attend to the global optimality of the GHMM as a whole. Ideally, one would like to maximize the expected accuracy of the gene finder on unseen data. A reasonable approximation to this ideal would be to maximize the average probability of the gene parses in the training set:</p><p><inline-graphic xlink:href="1471-2105-5-206-i2.gif"/></p><p>where the collection of model parameters making up the GHMM is denoted &#x003b8; and the elements (<italic>S</italic>, &#x003c6;) of the training set <italic>T </italic>comprise pairs of sequences <italic>S </italic>and their known parses &#x003c6;. This argmax gives us the parameterization <inline-graphic xlink:href="1471-2105-5-206-i3.gif"/> under which the full gene <italic>parses </italic>(rather than the <italic>sequences</italic>) in the training set will be maximally likely (on average). Decomposing each parse &#x003c6; into a series of (<italic>q</italic><sub><italic>i</italic></sub>, <italic>d</italic><sub><italic>i</italic></sub>) pairs, for state <italic>q</italic><sub><italic>i </italic></sub>and state duration (i.e., feature length) <italic>d</italic><sub><italic>i</italic></sub>, we get:</p><p><inline-graphic xlink:href="1471-2105-5-206-i4.gif"/></p><p>where <italic>P</italic><sub><italic>e</italic></sub>, <italic>P</italic><sub><italic>t</italic></sub>, and <italic>P</italic><sub><italic>d </italic></sub>represent the emission, transition, and duration probabilities of the GHMM, respectively. Whereas the common MLE training procedure for GHMMs as described above optimizes the individual terms in the numerator of Equation 3 independently, the argmax above calls instead for these terms to be jointly tuned so as to optimize the entire ratio in parentheses. Intuitively, one can think of this alternate formulation as attempting to account for the process in the Viterbi algorithm (during later decoding) whereby the individual submodels "compete" for nucleotides (in the sense that each nucleotide can be emitted by only one submodel in any given parse, and the Viterbi algorithm chooses the final, predicted parse based on the values of the model parameters). Our hope is that by addressing the issue of submodel competition explicitly during parameter estimation, we will thereby empower the gene finder to better discriminate at a global sequence level between the features modeled by individual submodels in the GHMM, thereby producing more accurate gene predictions.</p><p>A similar optimization problem occurs in the field of speech recognition, in which systems of interacting acoustic models and language models are employed to optimally parse an audio stream into a series of discrete words. Interestingly, the trend in that field, starting with Bahl <italic>et al</italic>. in 1986 [<xref ref-type="bibr" rid="B16">16</xref>], has increasingly been away from the sole use of MLE and toward an alternative approach very similar to that prescribed by Equation 2 known as <italic>global discriminative training </italic>[<xref ref-type="bibr" rid="B17">17</xref>-<xref ref-type="bibr" rid="B19">19</xref>] or <italic>conditional maximum likelihood </italic>[<xref ref-type="bibr" rid="B20">20</xref>]. The problem also appears in a slightly different form in the related field of statistical natural language parsing, in which it has been suggested that global methods for optimizing competing stochastic grammar models may improve the accuracy of systems at the level of whole-sentence parses [<xref ref-type="bibr" rid="B21">21</xref>]. <italic>Maximum discrimination HMMs </italic>have already been applied successfully to problems in the realm of biological sequence analysis [<xref ref-type="bibr" rid="B22">22</xref>], though their use in gene finding has apparently not yet seen widespread adoption. To our knowledge, the only gene finder reported to use discriminative training is HMMgene [<xref ref-type="bibr" rid="B23">23</xref>], a gene finder based on a non-generalized HMM.</p><p>In light of these considerations, it is worth contemplating the possible gains in gene finder accuracy that might be obtained through the use of some form of discriminative training applied to a GHMM &#x02013; that is, training aimed more directly at optimizing the ability of the gene finder to discriminate between exons and non-exons, thereby improving the expected accuracy of the gene finder's predictions. Anecdotal evidence already suggests that investigation of such methods may indeed be fruitful, as the process of manual tuning of GHMM parameters (i.e., "tweaking") after MLE training is commonly acknowledged by those with experience training GHMM-based gene finders (including our own systems). The practice of performing such tuning on the training set, especially when done iteratively, can be viewed as a manual form of gradient ascent optimization using the percentages of correctly predicted nucleotides, exons, and whole genes as surrogates for the &#x003a3;<sub>(S,&#x003c6;)&#x02208;T </sub>P(&#x003c6;|S,&#x003b8;) term in Equation 2.</p><p>We therefore decided to investigate the use of a simple form of global discriminative training for gene-finding. We did this by building a rudimentary gradient ascent optimizer and applying it to a subset of the model parameters for our GHMM-based gene finder, TigrScan, as described in the Methods.</p></sec><sec><title>Results</title><sec><title>Maximum likelihood versus discriminative training</title><p>Results for <italic>Arabidopsis thaliana </italic>are shown in Table <xref ref-type="table" rid="T1">1</xref> and those for <italic>Aspergillus fumigatus </italic>are shown in Table <xref ref-type="table" rid="T2">2</xref>. The two methods being compared are maximum likelihood estimation (MLE) versus maximum likelihood followed by gradient ascent parameter estimation (GRAPE).</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Results on <italic>Arabidopsis thaliana</italic></p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">method</td><td align="left">train</td><td align="left">test</td><td align="left">nucAcc</td><td align="left">exonF</td><td align="left">geneSn</td></tr></thead><tbody><tr><td align="left">GRAPE</td><td align="left">CV</td><td align="left">CV</td><td align="left">95 &#x000b1; 1%</td><td align="left">82 &#x000b1; 2%</td><td align="left">49 &#x000b1; 3%</td></tr><tr><td align="left">GRAPE</td><td align="left">CV</td><td align="left">H</td><td align="left">93 &#x000b1; 1%</td><td align="left">80 &#x000b1; 2%</td><td align="left">44 &#x000b1; 3%</td></tr><tr><td align="left">GRAPE</td><td align="left">T</td><td align="left">T</td><td align="left">95%</td><td align="left">86%</td><td align="left">57%</td></tr><tr><td align="left">GRAPE</td><td align="left">T</td><td align="left">H</td><td align="left">94%</td><td align="left">81%</td><td align="left">48%</td></tr><tr><td align="left">MLE</td><td align="left">CV</td><td align="left">CV</td><td align="left">90 &#x000b1; 1%</td><td align="left">72 &#x000b1; 2%</td><td align="left">33 &#x000b1; 4%</td></tr><tr><td align="left">MLE</td><td align="left">T</td><td align="left">T</td><td align="left">91%</td><td align="left">75%</td><td align="left">36%</td></tr><tr><td align="left">MLE</td><td align="left">T</td><td align="left">H</td><td align="left">90%</td><td align="left">71%</td><td align="left">33%</td></tr></tbody></table><table-wrap-foot><p>GRAPE = GRadient Ascent Parameter Estimation, MLE = Maximum Likelihood Estimation only. CV=cross validation, T = training set, H = 1000-gene hold-out ("test") set. CV in the <italic>train </italic>column means training on 800 genes from T. CV in <italic>test </italic>column means testing on 200 genes from T. In rows with a CV in either column, numbers are averages from 5 runs. nucAcc = nucleotide accuracy, exonF = exon F score, geneSn = gene sensitivity. F = 2SnSp/(Sn+Sp) for Sn = sensitivity and Sp = specificity. CV averages are reported &#x000b1; SD.</p></table-wrap-foot></table-wrap><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Results on <italic>Aspergillus fumigatus</italic></p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">method</td><td align="left">train</td><td align="left">test</td><td align="left">nucAcc</td><td align="left">exonF</td><td align="left">geneSn</td></tr></thead><tbody><tr><td align="left">GRAPE</td><td align="left">CV</td><td align="left">CV</td><td align="left">88 &#x000b1; 1%</td><td align="left">54 &#x000b1; 4%</td><td align="left">35 &#x000b1; 4%</td></tr><tr><td align="left">GRAPE</td><td align="left">CV</td><td align="left">H</td><td align="left">88 &#x000b1; 1%</td><td align="left">51 &#x000b1; 2%</td><td align="left">29 &#x000b1; 1%</td></tr><tr><td align="left">GRAPE</td><td align="left">T</td><td align="left">T</td><td align="left">92%</td><td align="left">65%</td><td align="left">48%</td></tr><tr><td align="left">GRAPE</td><td align="left">T</td><td align="left">H</td><td align="left">87%</td><td align="left">51%</td><td align="left">31%</td></tr><tr><td align="left">MLE</td><td align="left">CV</td><td align="left">CV</td><td align="left">81 &#x000b1; 3%</td><td align="left">27 &#x000b1; 8%</td><td align="left">16 &#x000b1; 5%</td></tr><tr><td align="left">MLE</td><td align="left">T</td><td align="left">T</td><td align="left">88%</td><td align="left">42%</td><td align="left">28%</td></tr><tr><td align="left">MLE</td><td align="left">T</td><td align="left">H</td><td align="left">83%</td><td align="left">30%</td><td align="left">18%</td></tr></tbody></table><table-wrap-foot><p>See Table 1 for legend.</p></table-wrap-foot></table-wrap><p>The <italic>train </italic>column indicates whether training (i.e., parameter estimation) was performed on the entire training set (T) or on separate 800-gene cross-validation partitions (CV). The <italic>test </italic>column indicates whether accuracy was measured on the full training set (T), on one-fifth of the training set (CV), or on the unseen data (H). We will consider the evaluation on H to be the most reliable measure of gene finder accuracy. For any row containing a CV, we report the average of five runs, where each run used a different 800-gene subset of the training data for parameter estimation.</p><p>Both tables give compelling evidence for the value of gradient ascent training, as shown in Figure <xref ref-type="fig" rid="F1">1</xref>. In <italic>Arabidopsis</italic>, gradient ascent applied to the full training set improved over the MLE method from 71% to 81% at the level of exons and 33% to 48% at the level of whole genes. In <italic>Aspergillus </italic>the improvement was even more dramatic: 30% to 51% at the exon level and 18% to 31% for whole genes. A gain of 4% nucleotide accuracy was measured for both organisms.</p><fig position="float" id="F1"><label>Figure 1</label><caption><p><bold>Maximum likelihood versus gradient ascent </bold>Gradient ascent parameter estimation (GRAPE) improves accuracy over MLE at the nucleotide, exon, and whole gene levels. arab = <italic>Arabidopsis thaliana</italic>, asp = <italic>Aspergillus fumigatus</italic>.</p></caption><graphic xlink:href="1471-2105-5-206-1"/></fig></sec><sec><title>Data partitioning and cross validation</title><p>A tangible improvement was still seen when a cross-validation design was used to split the training set so as to separate the data used for maximum likelihood estimation (800 genes) and subsequent gradient ascent (200 genes). However, results from both organisms suggest that this separation did not improve the accuracy of the gene finder, as shown in Figure <xref ref-type="fig" rid="F2">2</xref>. Indeed, on <italic>Arabidopsis</italic>, gradient ascent training produced greater gains in accuracy when performed on the entire training set rather than using the cross-validation structure, while on <italic>Aspergillus </italic>the improvement due to using a cross-validation structure was either small (nucleotide level: 1%), zero (exon level), or negative (gene level: -2%). Thus, the recommended training protocol would be to apply MLE to the entire training set followed by gradient ascent on the full training set as well.</p><fig position="float" id="F2"><label>Figure 2</label><caption><p><bold>Data partitioning for gradient ascent </bold>Separating the training set into an 800-gene MLE set and a 200-gene gradient ascent set provides no improvement over simply performing MLE and GRAPE on the full training set.</p></caption><graphic xlink:href="1471-2105-5-206-2"/></fig><p>Although use of a cross-validation structure to split the training set for the twin purposes of maximum likelihood estimation of ~90,000 parameters and gradient ascent refinement of 29 parameters is therefore not justified (according to the above results), cross-validation does seem to have some value in terms of predicting how well the gene finder will perform on unseen data, as suggested by Figure <xref ref-type="fig" rid="F3">3</xref>.</p><fig position="float" id="F3"><label>Figure 3</label><caption><p><bold>Cross-validation versus testing on unseen data </bold>Cross-validation scores provide a reasonably accurate prediction of performance on unseen data. Results shown for <italic>A. thaliana </italic>only; results for <italic>A. fumigatus </italic>are given in Table 2.</p></caption><graphic xlink:href="1471-2105-5-206-3"/></fig><p>On both genomes and at all levels (nucleotide, exon, gene), accuracy measurements obtained through cross-validation were closer to the accuracy measured on unseen data than were the measurements taken from the full training set, as we expected. This was true both with and without gradient ascent, though when gradient ascent was applied, even the cross-validation results were slightly inflated. The latter observation is presumably attributable to the "peeking" that was permitted (see Methods), whereby the gradient ascent procedure received feedback from the 200 evaluation genes held out from the training set, T. This suggests that estimating even small numbers of parameters (in this case 29) from the test set can artificially inflate accuracy measurements on that set.</p><p>Figure <xref ref-type="fig" rid="F4">4</xref> illustrates the effects of testing the gene finder on the training set. As can be seen from the figure, the accuracy measurements taken from the training set can be substantially inflated relative to the more objective measurements taken from the hold-out set, thereby promoting overly optimistic expectations for how the gene finder will perform on unseen data.</p><fig position="float" id="F4"><label>Figure 4</label><caption><p><bold>Evaluation on the training set </bold>Accuracy measurements taken from the training set were artificially inflated, as expected. Results are shown only for <italic>A. thaliana</italic>; results for <italic>A. fumigatus </italic>were even more extreme.</p></caption><graphic xlink:href="1471-2105-5-206-4"/></fig></sec></sec><sec><title>Discussion</title><p>The results presented above provide a clear demonstration that independent maximum likelihood estimation of submodel parameters is sufficiently neglectful of global GHMM behavior as to compromise gene finder accuracy. Even such a crude method as our 29-parameter gradient ascent procedure proved to be effective at significantly improving accuracy over that achievable by simple MLE training. The potential for more sophisticated global discriminative training methods to produce even greater improvements is surely worthy of investigation.</p><p>It is interesting to observe that the natural language processing and speech recognition communities, from whom HMM-based methods were originally borrowed for use in bioinformatics, have been moving toward global discriminative training methods for some time. The two most popular forms of discriminative training for speech recognition are Maximum Mutual Information (MMI) and Minimum Classification Error (MCE). Both methods can be implemented using an iterative gradient ascent/descent algorithm. Our approach is most similar in spirit to that of MCE.</p><p>In the case of "pure" (i.e., non-generalized) HMMs, expectation-maximization (EM) update formulas have been derived for both MMI and MCE. These formulas allow model parameters to be updated in an <italic>axis-oblique </italic>(rather than <italic>axis-parallel</italic>) manner; i.e., multiple parameters can be adjusted simultaneously, so that the optimizer is less constrained in following the direction of steepest gradient in parameter space. This may reduce the number of steps required for convergence. Indeed, more rapid convergence (in terms of numbers of re-evaluation steps) has been cited as a concrete advantage of these EM-style formulations over more generalized gradient ascent methods [<xref ref-type="bibr" rid="B23">23</xref>]. However, EM-style approaches to the discriminative training problem for HMMs have typically involved a number of simplifying assumptions and/or heuristics, thereby voiding formal assurances of optimality (e.g., [<xref ref-type="bibr" rid="B17">17</xref>,<xref ref-type="bibr" rid="B24">24</xref>,<xref ref-type="bibr" rid="B18">18</xref>,<xref ref-type="bibr" rid="B26">26</xref>]). Furthermore, as with more generalized gradient ascent procedures, EM often tends to find only a local optimum rather than a global one [<xref ref-type="bibr" rid="B13">13</xref>].</p><p>In the case of GHMM-based gene finders, the advantages of EM over a generalized gradient ascent procedure may indeed be rather slim. The very flexibility which we find attractive in GHMMs can be expected to complicate the derivation of such EM-like update formulas for arbitrary GHMM-based gene finders, likely requiring additional assumptions and approximations that would further compromise the optimality of the EM procedure. It was for this reason that we decided to employ a more generalized gradient ascent method for the present study. A rudimentary gradient ascent optimizer is simple to implement, and the use of prediction accuracy as an objective function affords great convenience in approximating &#x003a3;<sub>(S,&#x003c6;)&#x02208;T</sub>P(&#x003c6;|S,&#x003b8;). Although P(&#x003c6;|S,&#x003b8;) can be more directly computed using a modified Forward algorithm [<xref ref-type="bibr" rid="B23">23</xref>], to do so would in theory be no more efficient than running the full gene finder, since the asymptotic run times of the Forward and Viterbi algorithms for GHMMs are equivalent. Nevertheless, inasmuch as the Forward algorithm provides a more direct approximation of P(&#x003c6;|S,&#x003b8;), its use for this purpose is worthy of investigation.</p><p>There are a number of other variations and enhancements which we are at present contemplating for our discriminative trainer. One of these involves the joint training of pairs of submodels in the GHMM using a maximum discrimination criterion rather than the usual one based on maximum likelihood. Although such an approach would not in itself directly attend to the global optimality of the GHMM (indeed, we already apply such an approach to our signal sensors during our so-called "MLE" training regime, as remarked earlier), it would at least seem to offer a promising direction for improving our existing optimizer and may be feasible without increasing the computational cost beyond what is practical.</p><p>For the present, we feel confident in making the recommendation that others tasked with the training of GHMM gene finders consider applying an automated gradient ascent procedure like that described here as a more systematic alternative to manual tuning of parameters following maximum likelihood training of individual submodels. Beyond the obvious advantage of likely improving gene finder accuracy, such an automated method may offer some degree of reproducibility (notwithstanding the typically stochastic nature of such methods) and uniformity for the purposes of comparing gene finders and gene finding algorithms. In addition, we urge those practicing manual tuning on their final "test" set to consider that their reported accuracy results may well be inflated as a result of "peeking" at the test set before the final evaluation &#x02013; a practice that has been criticized in the field of machine learning (eg., [<xref ref-type="bibr" rid="B27">27</xref>]). That significant inflation was seen in our studies as a result of tuning only 29 of the ~90,000 GHMM parameters on the 200-gene "test" set suggests that the phenomenon may conceivably occur to some degree even when an automated procedure is not employed.</p><p>Finally, we would like to make note of an unfortunate consequence of discriminative training of HMMs for biological sequence analysis, namely, that while the resulting models may possess improved ability for discrimination and therefore greater utility for specific tasks such as gene prediction, their suitability as representative models of biological knowledge (especially probabilistic knowledge) may well be reduced relative to models induced with simple MLE techniques. Indeed, some authors in the field of speech recognition (e.g., [<xref ref-type="bibr" rid="B20">20</xref>]) have noted that more accurate discrimination can sometimes be obtained by relaxing sum-to-one constraints for probability distributions, thereby permitting the gradient ascent procedure to automatically discover appropriate weightings between states or inputs. This is reminiscent of the exon "optimism" parameter which we employ and which seems to have no principled justification (and indeed, we might speculate that this extraneous parameter proved useful precisely because it enabled a primitive form of discriminative training by providing an explicit "correction factor" or weighting between submodels). Thus, despite the apparent value of discriminative training in improving gene finder accuracy, our ability to extract biological knowledge by inspecting the parameters of a gene finder trained in this way may be somewhat hindered. For the present, this does not seem to be of great practical significance, but it is a consideration worthy at least of mention.</p></sec><sec><title>Conclusions</title><p>We have shown that discriminative training for GHMM-based gene finders is feasible using a rudimentary gradient ascent approach, and have briefly explored the relation between this method and the EM-like techniques which have been proposed in the field of speech recognition. Our experiments show that the gradient ascent method can result in a gene finder with substantially greater prediction accuracy. It is our hope that even greater gains in accuracy will result from extension and refinement of discriminative training techniques applied to GHMM-based gene finders.</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>Description of the GHMM</title><p>The gene finder TigrScan [<xref ref-type="bibr" rid="B8">8</xref>] is a GHMM-based program similar to Genie [<xref ref-type="bibr" rid="B1">1</xref>] and Genscan [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B28">28</xref>]. The forward-strand model contains six signal states (donor and acceptor sites; start and stop codons; promoter; poly-A signal) and eight content states (intron; intergenic; 5' and 3' UTR; initial, internal, final, and single exons). The reverse-strand model mirrors that of the forward strand. Four relative frequency histograms are used to estimate the duration probabilities of the four exon types; the four noncoding states are assumed to have geometric duration distributions and are therefore each parameterized by a single value representing the mean duration. Each content state is scored using a separate fifth-order Interpolated Markov Model (IMM) [<xref ref-type="bibr" rid="B29">29</xref>]. TigrScan offers a number of signal sensors, including WMMs, WAMs, WWAMs, and MDD trees [<xref ref-type="bibr" rid="B28">28</xref>] having any of the foregoing signal sensors as leaf models; for this study we used only (non-MDD) WAMs, though the order of the Markov chains within the WAMs was allowed to vary. Putative signals scoring below a given signal threshold are ignored by TigrScan. This threshold is chosen separately for each signal sensor so as to achieve a desired sensitivity <italic>Sn </italic>(<italic>Sn </italic>= <italic>TP</italic>/(<italic>TP</italic>+<italic>FN</italic>), <italic>TP </italic>= true positive count, <italic>FN </italic>= false negative count) on a training set of true and "decoy" signals. "Boosting" of signal sensors was performed by iteratively retraining each signal sensor on sets of training features in which the lowest scoring features were duplicated so as to focus the training procedure on the most difficult examples. Boosting has been found to improve signal detection in other application areas [<xref ref-type="bibr" rid="B30">30</xref>]. Most transitions in the GHMM are obligatory (such as "donor site &#x02192; acceptor site"); of the non-obligatory transitions, sum-to-one constraints and the forward/reverse strand equivalence reduce the number which can be independently varied to just four. Transitions into exon states are modified by an exon "optimism" multiplier (similar to that described in [<xref ref-type="bibr" rid="B6">6</xref>]) which has been seen anecdotally to be useful in improving prediction accuracy (unpublished data).</p></sec><sec><title>Parameters to be optimized</title><p>The total number of parameters which need to be estimated when training TigrScan is roughly 90,000; the large bulk of these are the n-gram statistics comprising the IMMs used for the content sensors. As an initial attempt at applying discriminative training to TigrScan, we selected 29 of these ~90,000 parameters to subject to gradient ascent optimization. Although this is a miniscule proportion of the available parameters, our previous experiences with hand-tuning our GHMM on other data sets suggested that these 29 parameters exert a disproportionately large influence on the accuracy of the gene predictions. By limiting the number of parameters to be optimized we hoped to both accelerate the training procedure and also reduce the risk of overtraining. The selected parameters were:</p><p>&#x02022; mean intron, intergenic, and UTR lengths (3)</p><p>&#x02022; transition probabilities (4)</p><p>&#x02022; exon optimism (1)</p><p>&#x02022; WAM size and relative positioning (8)</p><p>&#x02022; WAM order (4)</p><p>&#x02022; signal sensitivity (1)</p><p>&#x02022; number of signal boosting iterations (8)</p><p>&#x02022; skew and kurtosis of exon length distributions</p><p>Modifications to skew and kurtosis of exon length distributions were found during early exploration to produce no improvements; these parameters were therefore left unchanged in all further experiments. All remaining parameters were estimated using standard MLE techniques.</p><p>For those runs in which gradient ascent was disabled (see below), the following methods were used to estimate the above 29 parameters: mean intron and UTR lengths as well as transition probabilities were estimated using MLE from training data; mean intergenic length was set to a fixed value based on the known intergenic lengths in the test set; exon optimism was set to zero; remaining parameters were selected so as to minimize the misclassification rate on a set of true and "decoy" signals selected from the training set.</p></sec><sec><title>Objective function and optimization procedure</title><p>As an <italic>objective function </italic>for use by the gradient ascent procedure, we decided to measure the accuracy of the current parameterization by running the gene finder on a subset of the training genes. Our hope was that this accuracy measure would provide a reasonable approximation of &#x003a3;<sub>(S,&#x003c6;)&#x02208;T </sub>P(&#x003c6;|S,&#x003b8;) by indicating roughly how often the current model &#x003b8; would cause the correct parse &#x003c6; to be predicted for training sequence S. We defined the nucleotide accuracy <italic>A</italic><sub><italic>nuc </italic></sub>as the percentage of nucleotides correctly classified as coding vs. noncoding; <italic>A</italic><sub><italic>exon </italic></sub>was defined as an average of exon sensitivity and specificity (where a predicted exon is considered correct only if both boundary coordinates were predicted correctly); and <italic>A</italic><sub><italic>gene </italic></sub>was defined as the percentage of training genes which were predicted exactly correctly. These were all rounded to integral percentages between 0 and 100%. The objective function was then defined as:</p><p><italic>f</italic>(&#x003b8;) = 100<italic>A</italic><sub><italic>nuc</italic></sub>+<italic>A</italic><sub><italic>exon</italic></sub>+<italic>A</italic><sub><italic>gene</italic></sub>. &#x000a0;&#x000a0;&#x000a0;(4)</p><p>The <italic>A</italic><sub><italic>nuc </italic></sub>and <italic>A</italic><sub><italic>exon </italic></sub>terms were included in an effort to smooth the function, which would otherwise have been insensitive to changes not reflected in the number of genes predicted exactly correctly &#x02013; i.e., a step function. Though the <italic>A</italic><sub><italic>nuc </italic></sub>term was given much greater weight for this study, additional work needs to be undertaken to determine the most suitable set of weights for our objective function.</p><p>Parameters were optimized using an iterative gradient ascent procedure operating in the selected 29-dimensional parameter space, as illustrated schematically in Figure <xref ref-type="fig" rid="F5">5</xref>. Steps were taken in an axis-parallel manner (one step per axis per iteration), with the step size for each axis decreasing by half whenever a local maximum was reached on that axis.</p><fig position="float" id="F5"><label>Figure 5</label><caption><p><bold>Gradient ascent training </bold>Schematic diagram of gradient ascent training procedure. Of 29 parameters modified by gradient ascent, some (e.g., WAM size) were used to control the MLE estimation procedure, while others (e.g., mean intron length) were used directly as parameters to the GHMM. Testing of the gradient direction was performed on the 200-gene cross-validation set, which was part of the 1000-gene training set, T.</p></caption><graphic xlink:href="1471-2105-5-206-5"/></fig></sec><sec><title>Data and experimental design</title><p>The quality of a given parameterization &#x003b8; was measured by evaluating the objective function <italic>f</italic>(&#x003b8;) on a held-out subset of the training set. The training set was limited to 1000 genes, and all experiments were repeated separately on two highly divergent species, the model plant <italic>Arabidopsis thaliana </italic>and the pathogenic fungus <italic>Aspergillus fumigatus</italic>. Five-fold cross-validation was employed, so that the entire optimization procedure was carried out five times on four-fifths of the data (800 genes) and each time evaluated on the remaining one-fifth (200 genes); accuracy results reported here were obtained by averaging the five sets of accuracy numbers obtained from the cross-validation.</p><p>The held-out one-fifth was also used by the gradient ascent procedure to tune the selected 29 parameters. The practice of using a held-out set for smoothing or to estimate a small number of additional parameters is common in the natural language processing field [<xref ref-type="bibr" rid="B31">31</xref>], where it is recognized that such "peeking" at the test set (by which we mean iterative re-estimation of model parameters from the training set after receiving accuracy feedback on the test set) by the training procedure can (unfortunately) artificially inflate reported accuracy numbers. For this reason, an additional 1000 genes were used for testing the gene finder after each cross-validation run. The results of this final testing were <italic>not </italic>made available to the optimizer, but are instead reported here as a more objective assessment of final model accuracy. We will refer to the training set as T and the additional 1000 genes for testing as H. BLAST [<xref ref-type="bibr" rid="B32">32</xref>] was used to ensure that no two genes in T&#x0222a;H were more than 80% similar over 80% of their lengths at the nucleotide level. This training protocol is illustrated in Figure <xref ref-type="fig" rid="F6">6</xref>.</p><fig position="float" id="F6"><label>Figure 6</label><caption><p><bold>Cross-validation experiments </bold>Five-fold cross-validation was used both in the gradient ascent and in the MLE-only experiments. For gradient ascent training, MLE was performed on four-fifths of the training set (T) and then gradient ascent was performed on the other one-fifth. A separate hold-out set (H) of 1000 genes was used to obtain an unbiased evaluation of all final models.</p></caption><graphic xlink:href="1471-2105-5-206-6"/></fig><p>Several variations of this experiment were also performed. To evaluate the utility of splitting the training set and performing MLE and gradient ascent parameter estimation on separate subsets (as described above), we also performed MLE followed by gradient ascent training on the full training set T and again evaluated the induced models on H. To assess whether gradient ascent provided any improvement in accuracy we also trained a model on T using only MLE and evaluated that model on H. Although the virtues of cross-validation have been well explored in the context of many other applications, we decided to use the above experimental design as a convenient opportunity to verify our expectation that it would also prove useful for objective analysis of gene finder accuracy.</p></sec></sec><sec><title>Authors' contributions</title><p>Software implementation and computational experiments were performed by WHM. The manuscript was written by WHM with assistance from SLS.</p></sec></body><back><ack><sec><title>Acknowledgements</title><p>This work was supported in part by NIH grants R01-LM06845 and R01-LM007938. Thanks to I. Korf, D. Kulp, M. Brent, J. Allen, M. Pertea, M. Pop, A. Delcher, and L. Pachter for useful discussions. I. Korf provided valuable comments on a previous version of the manuscript and suggested the use of boosting in the training of signal sensors.</p></sec></ack><ref-list><ref id="B1"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Kulp</surname><given-names>D</given-names></name><name><surname>Haussler</surname><given-names>D</given-names></name><name><surname>Reese</surname><given-names>MG</given-names></name><name><surname>Eeckman</surname><given-names>FH</given-names></name></person-group><person-group person-group-type="editor"><name><surname>States DJ, Agarwal P, Gaasterland T, Hunter L, Smith RF</surname></name></person-group><article-title>A generalized hidden Markov model for the recognition of human genes in DNA</article-title><source>In Proceedings of the Fourth International Conference on Intelligent Systems for Molecular Biology: 12&#x02013;15 June 1996 St Louis</source><year>1996</year><publisher-name>Menlo Park: American Association for Artificial Intelligence</publisher-name><fpage>134</fpage><lpage>142</lpage></citation></ref><ref id="B2"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Burge</surname><given-names>C</given-names></name></person-group><article-title>Identification of genes in human genomic DNA</article-title><source>PhD thesis</source><year>1997</year><publisher-name>Stanford University, Mathematics Department</publisher-name></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Salamov</surname><given-names>A</given-names></name><name><surname>Salovyev</surname><given-names>V</given-names></name></person-group><article-title><italic>Ab initio </italic>gene finding in Drosophila genome DNA</article-title><source>Genome Res</source><year>2000</year><volume>10</volume><fpage>516</fpage><lpage>522</lpage><pub-id pub-id-type="pmid">10779491</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Cawley</surname><given-names>SE</given-names></name><name><surname>Wirth</surname><given-names>AI</given-names></name><name><surname>Speed</surname><given-names>TP</given-names></name></person-group><article-title>Phat &#x02013; a gene finding program for <italic>Plasmodium falciparum</italic></article-title><source>Mol Biochem Parasitol</source><year>2001</year><volume>118</volume><fpage>167</fpage><lpage>174</lpage><pub-id pub-id-type="pmid">11738707</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Majoros</surname><given-names>WM</given-names></name><name><surname>Pertea</surname><given-names>M</given-names></name><name><surname>Antonescu</surname><given-names>C</given-names></name><name><surname>Salzberg</surname><given-names>SL</given-names></name></person-group><article-title>GlimmerM, Exonomy and Unveil: three <italic>ab initio </italic>eukaryotic genefinders</article-title><source>Nucleic Acids Res</source><year>2003</year><volume>31</volume><fpage>3601</fpage><lpage>3604</lpage><pub-id pub-id-type="pmid">12824375</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Stanke</surname><given-names>M</given-names></name><name><surname>Waack</surname><given-names>S</given-names></name></person-group><article-title>Gene prediction with a hidden Markov model and a new intron submodel</article-title><source>Bioinformatics</source><year>2003</year><volume>19</volume><fpage>II215</fpage><lpage>II225</lpage><pub-id pub-id-type="pmid">14534192</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Korf</surname><given-names>I</given-names></name></person-group><article-title>Gene finding in novel genomes</article-title><source>BMC Bioinformeltics</source><year>2004</year><volume>5</volume><fpage>59</fpage></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Majoros</surname><given-names>WM</given-names></name><name><surname>Pertea</surname><given-names>M</given-names></name><name><surname>Salzberg</surname><given-names>SL</given-names></name></person-group><article-title>TigrScan and GlimmerHMM: two open-source <italic>ab initio </italic>eukaryotic gene finders</article-title><source>Bioinformatics</source><year>2004</year><volume>20</volume><fpage>2878</fpage><lpage>2879</lpage><pub-id pub-id-type="pmid">15145805</pub-id></citation></ref><ref id="B9"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Korf</surname><given-names>I</given-names></name><name><surname>Flicek</surname><given-names>P</given-names></name><name><surname>Duan</surname><given-names>D</given-names></name><name><surname>Brent</surname><given-names>MR</given-names></name></person-group><article-title>Integrating genomic homology into gene structure prediction</article-title><source>Bioinformatics</source><year>2001</year><fpage>140</fpage><lpage>148</lpage></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yeh</surname><given-names>R-F</given-names></name><name><surname>Lim</surname><given-names>LP</given-names></name><name><surname>Burge</surname><given-names>CB</given-names></name></person-group><article-title>Computational inference of homologous gene structures in the human genome</article-title><source>Genome Res</source><year>2001</year><volume>11</volume><fpage>803</fpage><lpage>816</lpage><pub-id pub-id-type="pmid">11337476</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Alexandersson</surname><given-names>M</given-names></name><name><surname>Cawley</surname><given-names>S</given-names></name><name><surname>Pachter</surname><given-names>L</given-names></name></person-group><article-title>SLAM: Cross-species gene finding and alignment with a generalized pair hidden Markov model</article-title><source>Genome Res</source><year>2003</year><volume>13</volume><fpage>496</fpage><lpage>502</lpage><pub-id pub-id-type="pmid">12618381</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Pavlovic</surname><given-names>V</given-names></name><name><surname>Cantor</surname><given-names>CR</given-names></name><name><surname>Kasif</surname><given-names>S</given-names></name></person-group><article-title>Human-mouse gene identification by comparative evidence integration and evolutionary analysis</article-title><source>Genome Res</source><year>2003</year><volume>13</volume><fpage>1190</fpage><lpage>1202</lpage><pub-id pub-id-type="pmid">12743024</pub-id></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Baum</surname><given-names>LE</given-names></name></person-group><article-title>An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes</article-title><source>Inequalities</source><year>1972</year><volume>3</volume><fpage>1</fpage><lpage>8</lpage></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rabiner</surname><given-names>LR</given-names></name></person-group><article-title>A tutorial on hidden Markov models and selected applications in speech recognition</article-title><source>Proc of the IEEE</source><year>1989</year><volume>77</volume><fpage>257</fpage><lpage>285</lpage></citation></ref><ref id="B15"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Krogh</surname><given-names>A</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Salzberg SL, Searls DB, Kasif S</surname></name></person-group><article-title>An introduction to hidden Markov models for biological sequences</article-title><source>In Computational Methods in Molecular Biology</source><year>1998</year><publisher-name>Amsterdam: Elsevier Science BV</publisher-name><fpage>45</fpage><lpage>62</lpage></citation></ref><ref id="B16"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Bahl</surname><given-names>LR</given-names></name><name><surname>Brown</surname><given-names>PF</given-names></name><name><surname>de Souza</surname><given-names>PV</given-names></name><name><surname>Mercer</surname><given-names>RL</given-names></name></person-group><article-title>Maximum mutual information estimation of hidden Markov model parameters for speech recognition</article-title><source>In Proceedings of the International Conference on Acoustics, Speech and Signal Processing</source><year>1986</year><publisher-name>IEEE Computer Society Press</publisher-name><fpage>49</fpage><lpage>52</lpage></citation></ref><ref id="B17"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Krogh</surname><given-names>A</given-names></name></person-group><article-title>Hidden Markov models for labeled sequences</article-title><source>Proceedings of the Twelfth IAPR International Conference on Pattern Recognition: 9&#x02013;12 October 1994</source><year>1994</year><publisher-name>Jerusalem. Piscataway: IEEE Computer Society Press</publisher-name><fpage>140</fpage><lpage>144</lpage></citation></ref><ref id="B18"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Jelinek</surname><given-names>F</given-names></name></person-group><source>Statistical Methods for Speech Recognition</source><year>1997</year><publisher-name>Cambridge: Bradford Books</publisher-name></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schl&#x000fc;ter</surname><given-names>R</given-names></name><name><surname>Macherey</surname><given-names>W</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>B</given-names></name><name><surname>Ney</surname><given-names>H</given-names></name></person-group><article-title>Comparison of discriminative training criteria and optimization methods for speech recognition</article-title><source>Speech Communication</source><year>2001</year><volume>34</volume><fpage>287</fpage><lpage>310</lpage></citation></ref><ref id="B20"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Johansen</surname><given-names>FT</given-names></name></person-group><article-title>A comparison of hybrid HMM architectures using global discriminative training</article-title><source>In Proceedings of the Fourth International Conference on Spoken Language Processing: 3&#x02013;4 October 1996 Philadelphia</source><year>1996</year><publisher-name>Piscataway IEEE Computer Society Press</publisher-name><fpage>498</fpage><lpage>501</lpage></citation></ref><ref id="B21"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Toutanova</surname><given-names>K</given-names></name><name><surname>Mitchell</surname><given-names>M</given-names></name><name><surname>Manning</surname><given-names>CD</given-names></name></person-group><article-title>Optimizing local probability models for statistical parsing</article-title><source>In Proceedings of the Fourteenth European Conference on Machine Learning (ECML 2003)</source><year>2003</year><publisher-name>New York: Springer Verlag</publisher-name><fpage>409</fpage><lpage>420</lpage></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Eddy</surname><given-names>S</given-names></name><name><surname>Mitchison</surname><given-names>G</given-names></name><name><surname>Durbin</surname><given-names>R</given-names></name></person-group><article-title>Maximum discrimination hidden Markov models of sequence consensus</article-title><source>J Comput Biol</source><year>1995</year><volume>2</volume><fpage>9</fpage><lpage>23</lpage><pub-id pub-id-type="pmid">7497123</pub-id></citation></ref><ref id="B23"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Krogh</surname><given-names>A</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Gaasterland T, Karp P, Karplus K, Ouzounis C, Sander C, Valencia A</surname></name></person-group><article-title>Two methods for improving performance of an HMM and their application for gene finding</article-title><source>In Proceedings of the Fifth International Conference on Intelligent Systems for Molecular Biology: 21&#x02013;25 June 1997 Halkidiki, Greece</source><year>1997</year><publisher-name> Menlo Park: American Association for Artificial Intelligence</publisher-name><fpage>179</fpage><lpage>186</lpage></citation></ref><ref id="B24"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Reichl</surname><given-names>W</given-names></name><name><surname>Ruske</surname><given-names>G</given-names></name></person-group><article-title>Discriminative training for continuous speech recognition</article-title><source>In Proceedings of the Fourth European Conference on Speech Communication and Technology (EUROSPEECH-95): 18&#x02013;21 September 1995 Madrid</source><year>1995</year><publisher-name>Amsterdam: Institute of Phonetic Sciences</publisher-name><fpage>537</fpage><lpage>540</lpage></citation></ref><ref id="B25"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Normandin</surname><given-names>Y</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Lee C-H, Soong FK, Paliwal KK</surname></name></person-group><article-title>Maximum mutual information estimation of hidden Markov models</article-title><source>In Automatic Speech and Speaker Recognition</source><year>1996</year><publisher-name>Norwell: Klewer Academic Publishers</publisher-name><fpage>58</fpage><lpage>81</lpage></citation></ref><ref id="B26"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Markov</surname><given-names>K</given-names></name><name><surname>Nakagawa</surname><given-names>S</given-names></name><name><surname>Nakamura</surname><given-names>S</given-names></name></person-group><article-title>Discriminative training of HMM using maximum normalized likelihood algorithm</article-title><source>In Proceedings of the International Conference on Acoustics, Speech and Signal Processing: 7&#x02013;11 May 2001 Salt Lake City</source><year>2001</year><publisher-name>IEEE Computer Society Press</publisher-name><fpage>497</fpage><lpage>500</lpage></citation></ref><ref id="B27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Salzberg</surname><given-names>SL</given-names></name></person-group><article-title>On comparing classifiers: a critique of current research and methods</article-title><source>Data Mining and Knowledge Discovery</source><year>1999</year><volume>1</volume><fpage>1</fpage><lpage>12</lpage></citation></ref><ref id="B28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname><given-names>C</given-names></name><name><surname>Karlin</surname><given-names>S</given-names></name></person-group><article-title>Prediction of complete gene structures in human genomic DNA</article-title><source>J Mol Biol</source><year>1997</year><volume>268</volume><fpage>78</fpage><lpage>94</lpage><pub-id pub-id-type="pmid">9149143</pub-id></citation></ref><ref id="B29"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Salzberg</surname><given-names>SL</given-names></name><name><surname>Pertea</surname><given-names>M</given-names></name><name><surname>Delcher</surname><given-names>AL</given-names></name><name><surname>Gardner</surname><given-names>MJ</given-names></name><name><surname>Tettelin</surname><given-names>H</given-names></name></person-group><article-title>Interpolated Markov models for eukaryotic gene finding</article-title><source>Genomics</source><year>1999</year><volume>59</volume><fpage>24</fpage><lpage>31</lpage><pub-id pub-id-type="pmid">10395796</pub-id></citation></ref><ref id="B30"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>J</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>Additive logistic regression: a statistical view of boosting</article-title><source>Annals of Statistics</source><year>2000</year><volume>38</volume><fpage>337</fpage><lpage>374</lpage></citation></ref><ref id="B31"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Manning</surname><given-names>CD</given-names></name><name><surname>Sch&#x000fc;tze</surname><given-names>H</given-names></name></person-group><source>Foundations of statistical natural language processing</source><year>1999</year><publisher-name>Cambridge: MIT Press</publisher-name></citation></ref><ref id="B32"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Altschul</surname><given-names>SF</given-names></name><name><surname>Gish</surname><given-names>W</given-names></name><name><surname>Miller</surname><given-names>W</given-names></name><name><surname>Myers</surname><given-names>EW</given-names></name><name><surname>Lipman</surname><given-names>DJ</given-names></name></person-group><article-title>Basic local alignment search tool</article-title><source>J Mol Biol</source><year>1990</year><volume>215</volume><fpage>403</fpage><lpage>410</lpage><pub-id pub-id-type="pmid">2231712</pub-id></citation></ref></ref-list></back></article>



