<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="review-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">R Soc Open Sci</journal-id><journal-id journal-id-type="iso-abbrev">R Soc Open Sci</journal-id><journal-id journal-id-type="publisher-id">RSOS</journal-id><journal-id journal-id-type="hwp">royopensci</journal-id><journal-title-group><journal-title>Royal Society Open Science</journal-title></journal-title-group><issn pub-type="epub">2054-5703</issn><publisher><publisher-name>The Royal Society</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">38126058</article-id><article-id pub-id-type="pmc">PMC10731323</article-id><article-id pub-id-type="doi">10.1098/rsos.230964</article-id><article-id pub-id-type="publisher-id">rsos230964</article-id><article-categories><subj-group subj-group-type="discipline-codes"><compound-subject><compound-subject-part content-type="code">1003</compound-subject-part></compound-subject></subj-group><subj-group subj-group-type="subject-codes"><compound-subject><compound-subject-part content-type="code">7</compound-subject-part></compound-subject></subj-group><subj-group subj-group-type="heading"><subject>Science, Society and Policy</subject></subj-group><subj-group subj-group-type="type-of-publication"><subject>Review Articles</subject></subj-group></article-categories><title-group><article-title>Multimodal analysis of disinformation and misinformation</article-title><alt-title alt-title-type="short">Multi-modal analysis of disinformation and misinformation</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-3036-2104</contrib-id><name><surname>Wilson</surname><given-names>Anna</given-names></name><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="http://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="http://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-&#x02013;-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-&#x02013;-review-editing/">Writing &#x02013; review &#x00026; editing</role><email>anna.wilson@area.ox.ac.uk</email><xref rid="af1" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Wilkes</surname><given-names>Seb</given-names></name><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-&#x02013;-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-&#x02013;-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af2" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Teramoto</surname><given-names>Yayoi</given-names></name><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-&#x02013;-original-draft/">Writing &#x02013; original draft</role><xref rid="af3" ref-type="aff">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Hale</surname><given-names>Scott</given-names></name><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-&#x02013;-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af4" ref-type="aff">
<sup>4</sup>
</xref></contrib><aff id="af1">
<label>
<sup>1</sup>
</label>
<addr-line>Oxford School of Global and Area Studies, <institution>University of Oxford</institution>, Oxford OX1 2JD, <country>UK</country></addr-line>
</aff><aff id="af2">
<label>
<sup>2</sup>
</label>
<addr-line>Department of Physics, <institution>University of Oxford</institution>, Oxford, <country>UK</country></addr-line>
</aff><aff id="af3">
<label>
<sup>3</sup>
</label>
<addr-line><institution>Landmark Information Group</institution>, Reading, <country>UK</country></addr-line>
</aff><aff id="af4">
<label>
<sup>4</sup>
</label>
<addr-line>Oxford Internet Institute, <institution>University of Oxford</institution>, Oxford, <country>UK</country></addr-line>
</aff></contrib-group><pub-date publication-format="electronic" date-type="pub"><day>20</day><month>12</month><year>2023</year><string-date>December 20, 2023</string-date></pub-date><pub-date publication-format="electronic" date-type="collection"><month>12</month><year>2023</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>12</month><year>2023</year><string-date>December 20, 2023</string-date></pub-date><volume>10</volume><issue>12</issue><elocation-id>230964</elocation-id><history>
<date date-type="received"><day>5</day><month>7</month><year>2023</year><string-date>July 5, 2023</string-date></date>
<date date-type="accepted"><day>22</day><month>11</month><year>2023</year><string-date>November 22, 2023</string-date></date>
</history><permissions><copyright-statement>&#x000a9; 2023 The Authors.</copyright-statement><copyright-year>2023</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Published by the Royal Society under the terms of the Creative Commons Attribution License <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>, which permits unrestricted use, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="rsos.230964.pdf"/><related-object source-id="https://www.webofscience.com/api/gateway/wos/peer-review/10.1098/rsos.230964" source-id-type="url" source-type="peer-review-report" id="d64e236"/><abstract><p>The use of disinformation and misinformation campaigns in the media has attracted much attention from academics and policy-makers. Multimodal analysis or the analysis of two or more semiotic systems&#x02014;language, gestures, images, sounds, among others&#x02014;in their interrelation and interaction is essential to understanding dis-/misinformation efforts because most human communication goes beyond just words. There is a confluence of many disciplines (e.g. computer science, linguistics, political science, communication studies) that are developing methods and analytical models of multimodal communication. This literature review brings research strands from these disciplines together, providing a map of the multi- and interdisciplinary landscape for multimodal analysis of dis-/misinformation. It records the substantial growth starting from the second quarter of 2020&#x02014;the start of the COVID-19 epidemic in Western Europe&#x02014;in the number of studies on multimodal dis-/misinformation coming from the field of computer science. The review examines that category of studies in more detail. Finally, the review identifies gaps in multimodal research on dis-/misinformation and suggests ways to bridge these gaps including future cross-disciplinary research directions. Our review provides scholars from different disciplines working on dis-/misinformation with a much needed bird's-eye view of the rapidly emerging research of multimodal dis-/misinformation.</p></abstract><kwd-group><kwd>literature review</kwd><x xml:space="preserve">, </x><kwd>multimodal dis-/misinformation</kwd><x xml:space="preserve">, </x><kwd>qualitative analysis</kwd><x xml:space="preserve">, </x><kwd>qualitative analysis</kwd><x xml:space="preserve">, </x><kwd>machine learning</kwd></kwd-group><funding-group specific-use="FundRef"><award-group><funding-source>
<institution-wrap><institution>John Fell Fund, University of Oxford</institution><institution-id>http://dx.doi.org/10.13039/501100004789</institution-id></institution-wrap>
</funding-source></award-group></funding-group><funding-group specific-use="FundRef"><award-group><funding-source>
<institution-wrap><institution>Defence Science and Technology Laboratory</institution><institution-id>http://dx.doi.org/10.13039/100010418</institution-id></institution-wrap>
</funding-source></award-group></funding-group></article-meta></front><body><sec id="s1"><label>1<x xml:space="preserve">. </x></label><title>Introduction</title><p>As disinformation and misinformation proliferate in everyday society, so does the collective need to fully understand the nature of this threat so that effective counter-dis-/misinformation strategies can be developed.</p><p>Studies using frameworks from psychology, cognitive science, political science, computer science, and beyond, demonstrate that dis-/misinformation presents a threat to individuals and societies when it negatively impacts their behaviours and decision-making. Misinformation has existed long before the Internet [<xref rid="RSOS230964C1" ref-type="bibr">1</xref>&#x02013;<xref rid="RSOS230964C3" ref-type="bibr">3</xref>], but the Internet has lowered the costs of producing and re-sharing misinformation [<xref rid="RSOS230964C4" ref-type="bibr">4</xref>], contributed to political polarization, and created environments where people often do not stop to reflect about the accuracy of what they see online [<xref rid="RSOS230964C2" ref-type="bibr">2</xref>,<xref rid="RSOS230964C3" ref-type="bibr">3</xref>]. Dis-/misinformation can impact people&#x02019;s beliefs and attitudes [<xref rid="RSOS230964C5" ref-type="bibr">5</xref>] including how they vote in political contests [<xref rid="RSOS230964C6" ref-type="bibr">6</xref>]. Concretely, dis-/misinformation can contribute to vaccine avoidance [<xref rid="RSOS230964C7" ref-type="bibr">7</xref>,<xref rid="RSOS230964C8" ref-type="bibr">8</xref>] and worsen health outcomes [<xref rid="RSOS230964C9" ref-type="bibr">9</xref>]. Dis-/misinformation can also hinder policy action to counter-act climate change [<xref rid="RSOS230964C10" ref-type="bibr">10</xref>], suppress collective cooperation [<xref rid="RSOS230964C11" ref-type="bibr">11</xref>] and impact the defence and security of democratic societies [<xref rid="RSOS230964C12" ref-type="bibr">12</xref>&#x02013;<xref rid="RSOS230964C14" ref-type="bibr">14</xref>].</p><p>Disinformation and misinformation is spread via textual, audio and visual modalities, or multimodally through various combinations of these modalities. While studies of textual dis-/misinformation are more common, there is rapidly growing appreciation of the necessity to do more research on the visual content of dis-/misinformation (see, e.g. the critical review on visual misinformation and the attempt at the systematization of the latter in [<xref rid="RSOS230964C15" ref-type="bibr">15</xref>]). As summarized by Heley <italic toggle="yes">et al.</italic> [<xref rid="RSOS230964C15" ref-type="bibr">15</xref>], research shows visual information is, among other things, more persuasive and manipulative and more effective in prompting emotional responses. Visual information is remembered for longer and often affects people negatively in more covert ways, as visual manipulations tend to be easily overlooked. In an attempt to answer the question of what visual misinformation is, Heley <italic toggle="yes">et al.</italic> [<xref rid="RSOS230964C15" ref-type="bibr">15</xref>] demonstrate that visual content can either be misinformation in itself or constitute part of what we define as multimodal misinformation, e.g. when visual content is contextualized in a certain manner by the accompanying text or speech to become misinformation. Khan <italic toggle="yes">et al.</italic> [<xref rid="RSOS230964C16" ref-type="bibr">16</xref>] similarly use &#x02018;multimodal&#x02019; to label this category of dis-/misinformation drawing upon semiotics. Much of the visual content in dis-/misinformation offered via traditional and new media comes in the form of video which more often than not constitutes multimodal dis-/misinformation. Dis-/misinformation in the form of videos is viewed as more convincing, believable, and is shared more then text and audio versions. For example, Sundar <italic toggle="yes">et al.</italic> [<xref rid="RSOS230964C17" ref-type="bibr">17</xref>] empirically demonstrate that video misinformation is perceived as more credible than text or audio: especially for issues where a person is less informed or interested, &#x02018;seeing is believing&#x02019;. Video, of course, relies on both audio and visual channels, and the role of audio itself should not be underestimated. For example, humans cannot reliably detect speech deepfakes [<xref rid="RSOS230964C18" ref-type="bibr">18</xref>]. The above-mentioned body of research demonstrates the need to examine dis-/misinformation from a multimodal perspective. Our review engages with studies focusing on more than one modality&#x02014;textual, audio, visual&#x02014;and treats all of them as multimodal dis-/misinformation regardless of whether the authors refer to them as fakes, manipulation, propaganda, disinformation, misinformation, malinformation, conspiracy theories or other similar terms.</p><p>Disinformation and misinformation are successful when they are more subtle, more memorable, more entertaining and more believable than factual information. Multimodal strategies are often employed as weapons of communication to conceal persuasive and manipulative intent, and increase interest, thereby making false information more appealing to audiences, more memorable and hence more effective.</p><p>Our three main research questions are informed by the UK&#x02019;s <italic toggle="yes">Online Harms White Paper of April 2019</italic>, the EU&#x02019;s <italic toggle="yes">Action Plan against Disinformation of December 2018</italic>, the joint statement <italic toggle="yes">Managing the COVID-19 Infodemic of September 2020</italic> and our prior policy engagement. They aim to help our understanding of what the field of multimodal research on dis-/misinformation is: What are the dynamics of its development? What is its disciplinary and interdisciplinary nature? What are the theories and methods that drive the development of the field? Are the data used reliable? What modalities are investigated? In what ways does multimodal analysis add value according to the authors whose studies we examine?</p><p>We draw the map of research trends prevailing in the field and identify niches which need to be filled. We offer potential future directions for multimodal research on dis-/misinformation in an attempt to analyse and tackle dis-/misinformation as one of the greatest challenges of the twenty-first century.</p><p>We set out the problem description and the details of data collection and our methodology in &#x000a7;2. We present the analysis of data in &#x000a7;3. After 2020, the number of papers in scope grew substantially&#x02014;mostly as a result of papers from computer science. Firstly, this required narrowing our focus to papers explicitly considering multimodal analysis in &#x000a7;4. Secondly, we also examined computer science papers in more detail in the same section. The rapid growth of computer science studies of multimodal dis-/misinformation after 2020 motivated us to divide our study and its presentation in this paper into two stages: before 2020, and after the first quarter of 2020. Stage 1 of our study is presented in &#x000a7;3, and Stage 2 is presented in &#x000a7;4. Lastly, we discuss and conclude our review in &#x000a7;5.</p><sec id="s1a"><label>1.1<x xml:space="preserve">. </x></label><title>What is disinformation and misinformation?</title><p>We define disinformation in line with the UK&#x02019;s <italic toggle="yes">Online Harms White Paper of April 2019</italic> [<xref rid="RSOS230964C19" ref-type="bibr">19</xref>], as the deliberate dissemination of information that is false, with the express aim to mislead or obfuscate. Misinformation is similar, but lacks intentionality. Note that disinformation can lead others into misinformation.</p><p>It is worth noting that we avoid the use of the term &#x02018;Fake News&#x02019;, given its politicized nature. However, a large number of authors refer to &#x02018;Fake News&#x02019; as the topic of their research, which we see as a subcategory of dis-/misinformation.</p></sec><sec id="s1b"><label>1.2<x xml:space="preserve">. </x></label><title>What is multimodal communication? Why is it important to analyse it?</title><p>Most human communication is more than just words: it is multimodal. Humans use visual, verbal and sound modes in order to communicate. What do intonation, facial expression, gesture and body language add to a communicated message? How do people use emojis, images and videos to communicate on social media? How are television or cinema audiences directed, or manipulated, by the producers&#x02019; choice of timing, settings, camera movement, etc.? How do political entities frame the same event from different angles by foregrounding certain aspects of multimodal communication? Ultimately, how can an understanding of the underlying mechanisms of multimodal communication inform how we live our lives? If we are to understand human communication in its full complexity then we need to answer questions such as these.</p><p>Multimodal communication plays an important role in many areas of research from linguistics to political science, from business to computer science. On the one hand there is a need to develop analytical models and methods for multimodal communication, combined with large multimodal datasets on which these models and methods can be tested. Naturally, this requires an ecosystem suitable for the collection of such datasets, along with pipelines for semi-automatic and automatic annotation. On the other hand, there is a further need to build capacity in the research methods suitable for multimodal communication, and then to deploy this in evidence-based policy settings and other knowledge exchange activities.</p><p>It is worth noting that some authors may have different definitions for modality. Though these definitions generally refer to distinct qualities (e.g. treatments in medicine), this paper requires that a mode be relevant to human communication.</p></sec><sec id="s1c"><label>1.3<x xml:space="preserve">. </x></label><title>What is multimodal analysis of dis-/misinformation? Why is it important? What are the challenges?</title><p>Multimodal analysis is the analysis of two or more modalities&#x02014;language, gestures, images, sounds, among others&#x02014;in their interrelation and interaction. The study of one modality in isolation overlooks the complexity of communication practices in terms of how textual, aural, linguistic, spatial and visual resources are integrated to create a single discourse or communication unit. It is especially evident in the case of media. Multimodal analysis reveals and interprets the use of several modalities in composing media messages. It assesses how messages are transformed into tools of persuasion and manipulation. The latter is of particular relevance to the study of dis-/misinformation communication. The importance of researching dis-/misinformation in a multimodal fashion and at scale has been established thorough research on dis-/misinformation at the International Multimodal Communication Centre at the University of Oxford.<sup><xref rid="FN1" ref-type="fn">1</xref></sup> The ongoing research shows that there is a need to analyse dis-/misinformation not just in the sense of verifiably incorrect information (via fact-checking), but also in the form of certain types of framing of information which aim to mislead or obfuscate less explicitly but more insidiously [<xref rid="RSOS230964C20" ref-type="bibr">20</xref>]. Such framings are more often than not achieved through multimodal communication. Multimodal analysis reveals the detailed composition of multimodal media messages&#x02014;certain combinations of visual, audio and textual information&#x02014;and their relation to socio-political, cultural and historical contexts. It reveals what makes these messages manipulative. The IMCC research has engaged, among other topics, with multimodal dis-/misinformation communicated by the Russian state and targeting international and domestic audiences (see, e.g. Uhrig <italic toggle="yes">et al.</italic> [<xref rid="RSOS230964C21" ref-type="bibr">21</xref>] on the scaling up of multimodal analysis of RT shows in English).</p><p>Consider just one example: the analysis of a Russian TV talk-show &#x02018;Pravo Golosa&#x02019; (2012&#x02013;2019). The show is broadcast in Russian and is representative of Russian disinformation communicated in covert ways. It discusses domestic and international news and invites guests with alternative (anti-Kremlin) viewpoints for the sole purpose of discrediting them in subtle, clever ways. On the surface, the programme seems to be supporting an exchange of views, arguments and constructive critique. In reality, the anchor (and the whole production crew) uses a wide range of multimodal strategies to ensure that the alternative viewpoints are discredited, but the manner of doing so is almost invisible to the untrained eye. At the same time, disinformation is communicated in an engaging and memorable way.</p><p>In one episode from February 2016 discussing Ukrainian politics and the outcomes of Maidan (the Ukrainian &#x02018;revolution&#x02019; of 2014), the anchor relies on frames and conceptual metaphors such as Self&#x02013;Other, Russia is a Great Power, Ukraine is Sick, and Anarchy and Banditry in Ukraine versus Law and Order in Russia. The construction of discrediting (disinformation) viewpoints is rooted in cultural and historical knowledge shared by Russians and Ukrainians. The anchor employs a range of manipulation techniques grounded in the co-presence of speech and co-speech gestures. For example, he uses deictic gestures to construct a strong overarching message: Ukrainian People Are Self versus Ukrainian Politicians Are Other. He first works hard to present the (purposefully selected) pro-Ukrainian panelists as incompetent, untrustworthy and corrupt. He then encourages the audience to associate their impression of the panel with all Ukrainian politicians and the whole of Ukrainian politics. Every time the anchor talks about Ukrainian authorities and politicians in a subtly discrediting way, he gestures towards the Ukrainian panel (the hand gesture pointing to the &#x02018;other&#x02019; ). By contrast, the anchor brings his hand(s) close to his chest (the &#x02018;self&#x02019; gesture normally accompanying words like: I, self, myself, my own, etc.) when talking about the Ukrainian people (on body-directed gestures see [<xref rid="RSOS230964C22" ref-type="bibr">22</xref>,<xref rid="RSOS230964C23" ref-type="bibr">23</xref>]).</p><p>The show also uses co-speech metaphoric gestures as tools of manipulation. These include examples of hand gestures adding crucially important dimensions to the meaning. For example, the purely verbal and relatively neutral &#x02018;You changed the [Ukrainian] regime&#x02019; is transformed via a metaphorical hand gesture made by the anchor into the stronger &#x02018;You overthrew the regime&#x02019; with the implication that &#x02018;overthrowing&#x02019; was illegal/illegitimate. The anchor also speaks with a specific intonation, which adds the epistemic stance of certainty, and makes the statement incorporating the gestural &#x02018;overthrow&#x02019; more categorical. Furthermore, the anchor uses gestures and corporal cues to construct a viewpoint of a strong and powerful Russia versus a weak Ukraine. One example is when the anchor adjusts his posture and moves as though he is preparing for a real physical fist fight. The accompanying hand gestures can be labelled as &#x02018;bring it on&#x02019;.</p><p>These conceptualizations&#x02014;Ukrainian people as SELF versus Ukrainian government/politicians as OTHER, Ukrainian Political Regime is Illegitimate, and Weak Ukraine versus Strong Russia in its &#x02018;bring it on&#x02019; aspect&#x02014;have continued to be communicated by Russian state media as disinformation until the present time. Those conceptualisations were among the key ones on which Putin relied in his speeches of 21 and 24 February 2022 when justifying the start of Russia&#x02019;s &#x02019;special military operation&#x02019; (the war against Ukraine) and more recently in e.g. Putin&#x02019;s speech at the ceremony of annexation of Ukrainian regions on 30 September 2022.</p><p>Experimental psychologists interpret the use of such co-speech gestures as lowering the cognitive load on the audience and distributing semantic information across language and visual inputs. They also emphasize that once the information conveyed by both language and co-speech gesture has been processed by the viewer, the influence of it cannot be undone. For example, Kelly <italic toggle="yes">et al.</italic> [<xref rid="RSOS230964C24" ref-type="bibr">24</xref>] showed that gestures cannot be ignored, even when people are asked to just make judgements on speech. Gesture&#x02013;speech integration is &#x02018;automatic.&#x02019; The viewer does not register what parts of the information are conveyed by which mode, and would not think of the work done by a particular gesture as a manipulative technique. The main implication for disinformation communication behind the examples here is that on the linguistic level the information sounds reasonably neutral, yet when combined with co-speech gesture, it is enriched with semantic nuances that make the overall resulting message into successful disinformation. Such manipulation techniques allow Russia (or other hostile states) to make its disinformation covert&#x02014;more subtle yet powerful&#x02014;and to avoid accountability for the disinformation it communicates, and make it very difficult for viewers to spot that they are being manipulated or understand how they are being manipulated [<xref rid="RSOS230964C23" ref-type="bibr">23</xref>]. The text-only approaches which currently prevail in dis-/misinformation analysis are missing the information communicated multimodally, which makes the results of text-only analyses of dis-/misinformation much less reliable.</p><p>The very nature of multimodal analysis necessitates the development and application of multidisciplinary and interdisciplinary approaches&#x02014;a task which is far from trivial.</p><p>Starting from the 1990s scholars in such disciplines and research areas as communication and advertising [<xref rid="RSOS230964C25" ref-type="bibr">25</xref>&#x02013;<xref rid="RSOS230964C27" ref-type="bibr">27</xref>], semiotics, education studies, linguistics and discourse analysis [<xref rid="RSOS230964C28" ref-type="bibr">28</xref>&#x02013;<xref rid="RSOS230964C30" ref-type="bibr">30</xref>] have been proposing various accounts of multimodal research. These accounts prepare the ground for the development of today&#x02019;s scholarship on multimodal communication analysis. The topics that scholars focus on include: multimodal research accounts of metaphor and other figurative devices [<xref rid="RSOS230964C31" ref-type="bibr">31</xref>]; critical analysis of multimodal discourse [<xref rid="RSOS230964C32" ref-type="bibr">32</xref>]; multimodal analysis of large datasets [<xref rid="RSOS230964C33" ref-type="bibr">33</xref>]; multimodal argumentation and rhetoric in media [<xref rid="RSOS230964C34" ref-type="bibr">34</xref>]; multimodal viewpoint analysis [<xref rid="RSOS230964C35" ref-type="bibr">35</xref>,<xref rid="RSOS230964C36" ref-type="bibr">36</xref>], language and gesture researched from the perspectives of cognitive linguistics [<xref rid="RSOS230964C37" ref-type="bibr">37</xref>&#x02013;<xref rid="RSOS230964C40" ref-type="bibr">40</xref>], psychology of language [<xref rid="RSOS230964C41" ref-type="bibr">41</xref>&#x02013;<xref rid="RSOS230964C43" ref-type="bibr">43</xref>] and semiotics [<xref rid="RSOS230964C44" ref-type="bibr">44</xref>,<xref rid="RSOS230964C45" ref-type="bibr">45</xref>].</p><p>Computer scientists have recently become more interested in multimodal analysis too. Correctly dealing with multimodal inputs is of huge importance to the field, particularly machine learning (ML) research. Applications range from sophisticated robotics to disinformation detection. Multimodal analysis in computer science has been buoyed by recent advancements in both hardware and ML techniques.</p><p>Although there are studies successfully researching multimodal communication, there are also many missed opportunities stemming from the lack of interdisciplinary approaches. There is also a lack of studies focusing on analysing ecologically valid multimodal data in context and at scale. One meaningful initiative which had attempted to bridge the latter gap is the Red Hen Lab [<xref rid="RSOS230964C46" ref-type="bibr">46</xref>].<sup><xref rid="FN2" ref-type="fn">2</xref></sup> Bearing in mind the broader situation with multimodal communication research, we engage with research on multimodal dis-/misinformation more specifically.</p></sec><sec id="s1d"><label>1.4<x xml:space="preserve">. </x></label><title>Research questions</title><p>The importance of considering disinformation through a multimodal lens, and its highly interdisciplinary nature, motivate our research questions:
<list list-type="simple"><list-item><p><bold>RQ1</bold>: To what extent have studies across different disciplines engaged with multimodal analysis of dis-/misinformation? What is the extent of interdisciplinary practices within the field?</p></list-item><list-item><p><bold>RQ2</bold>: What methods and data are used by the studies of multimodal dis-/misinformation? What modalities do studies engage with? What value does multimodal analysis add according to the studies? Is multimodal analysis of dis-/misinformation a well-formed research area? What are the challenges this field is facing?</p></list-item><list-item><p><bold>RQ3</bold>: What types of multimodal studies of dis-/information add value to the field and in what ways?</p></list-item></list>In the process of investigating the above, we will present the map of research trends observed within the field. The ultimate goal is not to dictate specifics, but instead draw the research landscape for multimodal analysis of dis-/misinformation, suggest a future research agenda for the field and inspire best working practices and approaches.</p><p>Our review analysis is divided into two stages: publications before the second quarter of 2020 (Stage 1), and publications from the second quarter of 2020 to August 2022 (Stage 2). Stage 2 reflects the rapid increase in the interest in multimodal analysis by computer scientists.</p></sec></sec><sec sec-type="methods" id="s2"><label>2<x xml:space="preserve">. </x></label><title>Material and methods</title><sec id="s2a"><label>2.1<x xml:space="preserve">. </x></label><title>Keyword selection and database searches</title><p>The reporting strategy follows the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses; <uri xlink:href="http://www.prisma-statement.org">http://www.prisma-statement.org</uri>) reporting the checklist approach to systematic literature reviews (<xref rid="RSOS230964F1" ref-type="fig">figure 1</xref>) [<xref rid="RSOS230964C47" ref-type="bibr">47</xref>].
<fig position="float" id="RSOS230964F1"><label>Figure 1<x xml:space="preserve">. </x></label><caption><p>PRISMA statement. The number of records included at each step are shown.</p></caption><graphic xlink:href="rsos230964f01" position="float"/></fig></p><p>The current review was interested in papers with three characteristics. First, they had to study disinformation or misinformation. Second, they had to focus on more than one modality (e.g. image and text or sound and image or video and text). Finally, the articles had to focus on traditional broadcast or social media.</p><p>Scopus, a database of peer-reviewed articles in a variety of fields including science, technology, medicine, social sciences and arts and humanities, was used to search for records published using the following search criteria:<disp-quote><p>LANGUAGE (English)</p><p>AND</p><p>TITLE-ABS-KEY (&#x02018;Fake news&#x02019; OR disinformation OR misinformation OR &#x02018;stance on stance&#x02019; OR &#x02018;meta-stance&#x02019; OR propaganda OR &#x02018;viewpoint construction&#x02019; OR &#x02018;viewpoint analysis&#x02019; OR &#x02018;multimodal viewpoint&#x02019;)</p><p>AND</p><p>(image* OR gesture* OR facial OR video* OR multimodal OR multi-modal OR multimedia OR (sound AND mode) OR (visual AND mode) OR gaze OR &#x02018;eye tracking&#x02019; OR prosody OR &#x02018;discourse analysis&#x02019; OR &#x02018;media discourse&#x02019; OR &#x02018;political discourse&#x02019; OR non-verbal OR verbal OR vocal OR non-linguistic OR &#x02018;camera movement&#x02019; OR laughter OR applause OR (stress AND (lexical OR word OR sentential OR phonetic)) OR kinetic OR corporal)</p><p>AND</p><p>(social OR media OR twitter OR facebook OR youtube OR wechat OR weibo OR livejournal OR orkut OR &#x02018;VK&#x02019; OR VKontakte OR telegram OR WhatsApp OR Instagram OR reddit OR Wikipedia OR &#x02018;news article&#x02019; OR online OR TV OR broadcast)</p></disp-quote>This yielded 980 results on Scopus in April 2020.</p></sec><sec id="s2b"><label>2.2<x xml:space="preserve">. </x></label><title>Screening</title><p>The results were manually screened based on the title, abstract and keywords. Articles were excluded if they were not written in English, were a collection of conference proceedings rather than individual articles, or were review articles. Furthermore, abstracts were excluded if they focused on a single modality or if they were not on data taken from social media, advertising or broadcast media (e.g. television, radio, newspaper). Likewise, articles were only included if they were about events in the period 1900 to the present. Articles studying strategies to reduce misinformation or disinformation like education programmes and public health campaigns were excluded, as were studies on the misinformation effect and memory malleability in the context of witness reliability.</p><p>Eligible articles were then divided into two categories. The first category contained abstracts that explicitly mentioned the use of multimodal analysis to study the content of dis-/misinformation on social media or broadcast media. The second category included abstracts that looked at mis/disinformation more broadly (e.g. responses to misinformation) or articles that looked at political propaganda with no explicit evaluation of the veracity of the information contained in the materials studied.</p></sec><sec id="s2c"><label>2.3<x xml:space="preserve">. </x></label><title>Identifying the field of research</title><p>In order to identify what disciplines were represented in our dataset, we used the &#x02018;All Science Journal Classification&#x02019; (ASJC) database published by Scopus, which assigns one or more subject and sub-subject code to journals in their database. Eighty-nine of the sources in our dataset&#x02014;primarily conference proceedings and books&#x02014;had to be labelled manually as they were not in the Scopus database. For visualization purposes, we grouped the sub-subjects into intermediate categories.</p></sec><sec id="s2d"><label>2.4<x xml:space="preserve">. </x></label><title>Topic modelling</title><p>Topic modelling using latent Dirichlet allocation (LDA) was performed on the abstracts. This technique uses word frequencies to identify topics and was used to gain a better understanding of the themes that were being studied. We performed this topic modelling on the full list of eligible articles as defined in the &#x02018;Screening&#x02019; section. The number of topics was chosen using coherence scores.</p></sec><sec id="s2e"><label>2.5<x xml:space="preserve">. </x></label><title>Qualitative analysis</title><p>A total of 101 full-text articles was analysed qualitatively. We assessed these based on five criteria, namely: (i) whether they were written in English (irrespective of whether they had an English-language abstract); (ii) whether the full text was available online; (iii) whether grounded in original research; (iv) whether they focused on more than one modality; (v) whether they focused on misinformation, disinformation, &#x02018;fake news&#x02019; or propaganda. Forty-nine of the 101 articles met these criteria and as a result were included in the final in-depth qualitative analysis.</p><p>The full text of all papers was reviewed qualitatively and information about each was added into an extraction table covering the following points:
<list list-type="simple"><list-item><label>(i)<x xml:space="preserve"> </x></label><p>Bibliographical information</p></list-item><list-item><label>(ii)<x xml:space="preserve"> </x></label><p>Data used (source of the data, how they were obtained, how dis-/misinformation was identified)</p></list-item><list-item><label>(iii)<x xml:space="preserve"> </x></label><p>Modalities studied</p></list-item><list-item><label>(iv)<x xml:space="preserve"> </x></label><p>Methodology used</p></list-item><list-item><label>(v)<x xml:space="preserve"> </x></label><p>Working definition of dis-/misinformation</p></list-item><list-item><label>(vi)<x xml:space="preserve"> </x></label><p>Main findings</p></list-item><list-item><label>(vii)<x xml:space="preserve"> </x></label><p>Value of multimodal analysis according to the authors</p></list-item><list-item><label>(viii)<x xml:space="preserve"> </x></label><p>Ethical or social challenges raised by authors</p></list-item></list></p></sec><sec id="s2f"><label>2.6<x xml:space="preserve">. </x></label><title>Co-citation network analysis</title><p>We created a co-citation network of the 49 articles included in the full text qualitative analysis. The reference list for each article was obtained from its SCOPUS entry, and the journal names were extracted using regular expressions. Each journal was a node in the network, and edges were drawn between journals that cited each other. Network analysis was carried out using <monospace>networkx</monospace> and <monospace>community</monospace>. Community detection was carried out using the Louvain algorithm with a minimum community size of 10. In addition to looking at citations between journals, we also obtained what subjects were citing each other, using the SCOPUS database of journals. Of the 2043 journals and publication venues, only 1616 were in the SCOPUS database as books and conference proceedings were not in the database.</p></sec></sec><sec id="s3"><label>3<x xml:space="preserve">. </x></label><title>Stage 1: Results</title><p>The eligible records were identified using the title, abstract and keywords. In total, 303 articles focused on more than one modality, focused on dis-/misinformation and used data collected from social media or broadcast media. A subset of these records (<italic toggle="yes">n</italic> = 101) focused specifically on analysing the content of dis-/misinformation. Most of the analysis presented here focuses on the entire set of eligible records, but some comparisons are drawn between the eligible set and the content-specific subset.</p><p>As expected, the number of studies focusing on dis-/misinformation has increased in the last decade (<xref rid="RSOS230964F2" ref-type="fig">figure 2</xref><italic toggle="yes">a</italic>). This increase appears to happen in two distinct phases. The first increase from 2008 to 2016 is probably due to the increasing popularity of online media including social media platforms like Twitter, Facebook and Instagram as well as online platforms like YouTube. The second phase is much steeper and started from 2016 onward, and is probably due to the increasing sensationalization of &#x02018;fake news&#x02019; and online misinformation. The apparent dip in 2020 is due to the fact that the data collected for Stage 1 only includes the first quarter of that year.
<fig position="float" id="RSOS230964F2"><label>Figure 2<x xml:space="preserve">. </x></label><caption><p>(<italic toggle="yes">a</italic>) Number of records published each year (1900 to 31 March 2020) within scope of Stage 1. (<italic toggle="yes">b</italic>) The number of records for each type of document.</p></caption><graphic xlink:href="rsos230964f02" position="float"/></fig></p><p>Most of the eligible records were published as research articles in journals as well as part of conference proceedings (<xref rid="RSOS230964F2" ref-type="fig">figure 2</xref><italic toggle="yes">b</italic>). There was a total of 703 unique authors, but most of these authors were only in one publication. Only three authors appeared in three records and three authors in four records (<xref rid="RSOS230964F3" ref-type="fig">figure 3</xref><italic toggle="yes">a</italic>). Likewise, there were 241 unique publication venues, but very few of them appeared more than once (<xref rid="RSOS230964F3" ref-type="fig">figure 3</xref><italic toggle="yes">b</italic>). Both of these results suggest that multimodal analysis on dis-/misinformation in media is not concentrated in a select number of established research communities, but rather publications are spread out across many journals and conferences, and few researchers have (yet) done multiple studies on this topic.
<fig position="float" id="RSOS230964F3"><label>Figure 3<x xml:space="preserve">. </x></label><caption><p>The number of records by the same author (<italic toggle="yes">a</italic>) or records published in the same source (<italic toggle="yes">b</italic>).</p></caption><graphic xlink:href="rsos230964f03" position="float"/></fig></p><p>The absence of journals with a large number of records or authors that published a lot in the area suggested that multimodal analysis of dis-/misinformation in media does not have a single research community. This suggested that the eligible sources could belong to a diverse set of disciplines. The 241 sources were cross-referenced with Scopus&#x02019; source database that has the primary subjects and sub-subjects published in journals. For conference proceedings that were not in the database, the subjects and sub-subjects were hand-coded. Most of the records were published in the social sciences as well as the arts and humanities, but there were a lot of other fields represented including computer science and life sciences (<xref rid="RSOS230964F4" ref-type="fig">figure 4</xref><italic toggle="yes">a</italic>). Furthermore, for the records in the social sciences, there was also a diverse set of sub-subjects with the primary areas being communications and political science (<xref rid="RSOS230964F4" ref-type="fig">figure 4</xref><italic toggle="yes">b</italic>). While multimodality is a &#x02018;hot topic&#x02019; in computer science (especially within natural language processing and computer vision), our analysis found that within Stage 1 few computer science publications on multimodality were specifically about dis-/misinformation.
<fig position="float" id="RSOS230964F4"><label>Figure 4<x xml:space="preserve">. </x></label><caption><p>Subject breakdown. The breakdown of subjects for all eligible records (<italic toggle="yes">a</italic>) and the breakdown into sub-subjects for the records in the social sciences (<italic toggle="yes">b</italic>).</p></caption><graphic xlink:href="rsos230964f04" position="float"/></fig></p><p>Finally, in addition to understanding the disciplinary contributions, the methodology used for the multimodal analysis was studied. This analysis focused only on the subset of records with a focus on analysing dis-/misinformation multimodal content. The strategy for multimodal analysis was determined to be qualitative (e.g. discourse analysis) or quantitative (e.g. deep learning) or both. The breakdown of subject areas for this subset was similar to the breakdown of subject areas of the full set of eligible records (<xref rid="RSOS230964F5" ref-type="fig">figure 5</xref><italic toggle="yes">a</italic>). Even though social sciences and humanities were the primary subject areas represented, more than half of the articles used quantitative methods (<xref rid="RSOS230964F5" ref-type="fig">figure 5</xref><italic toggle="yes">b</italic>). This suggests that several of the studies in the social sciences used quantitative methods in addition to the records published in engineering science or computer science journals. Nonetheless, there are still very few papers that attempt to combine both qualitative and quantitative approaches to multimodal analysis.
<fig position="float" id="RSOS230964F5"><label>Figure 5<x xml:space="preserve">. </x></label><caption><p>Methodology. The distribution of subjects for content-specific eligible records (<italic toggle="yes">a</italic>), and the type of methodology used in these records (<italic toggle="yes">b</italic>).</p></caption><graphic xlink:href="rsos230964f05" position="float"/></fig></p><sec id="s3a"><label>3.1<x xml:space="preserve">. </x></label><title>Topic modelling</title><p>Topic modelling was then applied to understand what topics studies on dis-/misinformation in online or broadcast media were focused on. The most common words in the dataset after removal of the most common words in English included many of the search terms used in Scopus like &#x02018;news&#x02019; and &#x02018;propaganda&#x02019; (<xref rid="RSOS230964F6" ref-type="fig">figure 6</xref>). However, there were words like &#x02018;political&#x02019;, &#x02018;war&#x02019; and &#x02018;state&#x02019; that suggest that a lot of the research focuses on dis-/misinformation in the context of political events or conflicts.
<fig position="float" id="RSOS230964F6"><label>Figure 6<x xml:space="preserve">. </x></label><caption><p>The 20 most common words in the abstracts after stop words were removed.</p></caption><graphic xlink:href="rsos230964f06" position="float"/></fig></p><p>Our study trained a LDA topic model on the abstracts of our subjects. LDA is an established method in topic modelling that uses the frequency of words in each abstract to iteratively assign words to topics and topics to abstracts. In order to determine the number of topics to use, the coherence score was used, which is a measure of the quality of the topics that were found. Typically, the coherence score plateaus or falls after reaching a good number of topics. Based on the scores (<xref rid="RSOS230964F7" ref-type="fig">figure 7</xref>), the rest of the analysis was carried out for <italic toggle="yes">N</italic> = 4 topics.
<fig position="float" id="RSOS230964F7"><label>Figure 7<x xml:space="preserve">. </x></label><caption><p>Coherence scores for LDA models for different numbers of topics (minimum 2). The red-highlighted points indicate possible choices of topic numbers.</p></caption><graphic xlink:href="rsos230964f07" position="float"/></fig></p><p><xref rid="RSOS230964TB1" ref-type="table">Table 1</xref> lists the topics detected by the LDA algorithm for <italic toggle="yes">N</italic> = 4 topics, with the top words assigned to each topic and a qualitative interpretation of what each topic may correspond to. Three of the topics correspond to types of misinformation, where Topic 1 corresponded specifically to online misinformation and Topics 0 and 3 corresponded to propaganda for political and religious purposes. Topic 2 appeared to have a mix of different themes including gender, war and religion.
<table-wrap position="float" id="RSOS230964TB1"><label>Table 1<x xml:space="preserve">. </x></label><caption><p>Topic modelling results for <italic toggle="yes">N</italic> = 4 topics.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1">topic no.</th><th align="left" rowspan="1" colspan="1">top words</th><th align="left" rowspan="1" colspan="1">annotation</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">propaganda ISIS media war political social also online radio group</td><td rowspan="1" colspan="1">religious propaganda</td></tr><tr><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">news fake social media content information videos images online misinformation</td><td rowspan="1" colspan="1">online/social media content</td></tr><tr><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">women war public American Muslim popular year one first anti-communist</td><td rowspan="1" colspan="1">gender; religion; war</td></tr><tr><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">media political propaganda social discourse images visual communication new news</td><td rowspan="1" colspan="1">political propaganda</td></tr></tbody></table></table-wrap></p><p>Each abstract will contain words that belong to each topic, so each abstract can be assigned one or more topics that mostly represent it. The predominant topic for each abstract was obtained, and the three topics related to misinformation were similarly represented in the abstracts (<xref rid="RSOS230964F8" ref-type="fig">figure 8</xref><italic toggle="yes">a</italic>). Topic 2 was the main topic for only 18 abstracts, but appeared as a topic in 39 abstracts, which may suggest that this topic contains words that refer to multiple aspects of the dis-/misinformation content being studied. This is confirmed by looking at the frequency with which topics co-occur with each other normalized to their relative frequency. Topic 2 is the only one that seems to co-occur with the other topics (<xref rid="RSOS230964F8" ref-type="fig">figure 8</xref><italic toggle="yes">b</italic>). Among the three dis-/misinformation topics, the two propaganda-related topics (Topics 0 and 3) had a slightly higher co-occurrence compared to the overlap between Topic 1 and either Topic 0 or Topic 3.
<fig position="float" id="RSOS230964F8"><label>Figure 8<x xml:space="preserve">. </x></label><caption><p>The frequency with which each topic was the primary topic in an abstract (<italic toggle="yes">a</italic>) and the normalized frequency with which topics co-occurred with other topics (<italic toggle="yes">b</italic>).</p></caption><graphic xlink:href="rsos230964f08" position="float"/></fig></p><p>We looked at the distribution of different disciplines within each topic (<xref rid="RSOS230964F9" ref-type="fig">figure 9</xref>). There is a clear disciplinary divide in the vocabulary employed to refer to dis-/misinformation, because while articles published in social sciences and humanities journals spoke about it in terms of words related to politics and propaganda, articles published in the physical sciences, like computer science, tended to focus on social media and use terms like &#x02018;fake news&#x02019;.
<fig position="float" id="RSOS230964F9"><label>Figure 9<x xml:space="preserve">. </x></label><caption><p>The frequency with which each topic was the primary topic in an abstract.</p></caption><graphic xlink:href="rsos230964f09" position="float"/></fig></p></sec><sec id="s3b"><label>3.2<x xml:space="preserve">. </x></label><title>Qualitative analysis</title><p>Next, a more in-depth qualitative analysis was carried out on the full text of 49 articles. We engaged with the parameters for the analysis outlined in &#x000a7;2.6 above, which are translated here in the subheadings in this section of our paper. Our engagement produced the following results.</p><sec id="s3b1"><label>3.2.1<x xml:space="preserve">. </x></label><title>Methodology employed by articles</title><p><italic toggle="yes">Methods</italic>. Most studies used either quantitative (<italic toggle="yes">n</italic> = 20) or qualitative (<italic toggle="yes">n</italic> = 22) methods, but around 10% of them used a mixture of both (<italic toggle="yes">n</italic> = 7).</p><p><italic toggle="yes">Data</italic>. Apart from the types of methods used, there was a marked difference in the data sources employed by quantitative papers as opposed to qualitative or mixed methods papers. Over half of the quantitative papers (<italic toggle="yes">n</italic> = 12) used publicly available datasets in their analysis, whereas the qualitative and mixed method papers all collected data specific to their proposed research questions. Two datasets that were used by multiple papers include the Weibo dataset [<xref rid="RSOS230964C48" ref-type="bibr">48</xref>] and the MediaEval Twitter dataset [<xref rid="RSOS230964C49" ref-type="bibr">49</xref>]. Both of these are freely available and individual items have been labelled as rumour/non-rumour or fake/true, respectively.</p><p><italic toggle="yes">Combinations of modalities</italic>. We found most articles focused on two modalities rather than three (noting that articles analysing only one mode are not within the scope of our review). Forty articles focused on the analysis of two modalities. Of these, 38 focused on image and text [<xref rid="RSOS230964C20" ref-type="bibr">20</xref>,<xref rid="RSOS230964C48" ref-type="bibr">48</xref>,<xref rid="RSOS230964C50" ref-type="bibr">50</xref>&#x02013;<xref rid="RSOS230964C85" ref-type="bibr">85</xref>]. Two articles focused on sound and image [<xref rid="RSOS230964C86" ref-type="bibr">86</xref>,<xref rid="RSOS230964C87" ref-type="bibr">87</xref>].</p><p>Nine articles focused on the analysis of three modalities. Seven focused on text, sound/music and video (including dance performance): [<xref rid="RSOS230964C88" ref-type="bibr">88</xref>&#x02013;<xref rid="RSOS230964C94" ref-type="bibr">94</xref>]. Two articles focused on text, image, video (incl. sound): [<xref rid="RSOS230964C95" ref-type="bibr">95</xref>,<xref rid="RSOS230964C96" ref-type="bibr">96</xref>].</p></sec><sec id="s3b2"><label>3.2.2<x xml:space="preserve">. </x></label><title>Qualitative papers: theoretical frameworks and methods used</title><p>Our meta-analysis revealed quantitative methods were used by 20 articles while qualitative methods were used by 22 articles; seven papers additionally used a combination of quantitative and qualitative analyses. We used qualitative analysis to establish which theories and methods were employed by 22 studies that researched dis-/misinformation through qualitative methods. Our meta-analysis revealed:</p><p>
<list list-type="simple"><list-item><label>(I)<x xml:space="preserve"> </x></label><p>Eleven papers used a form of discourse analysis as their core method as rooted in linguistics, rhetoric and/or social semiotics, while combining their discourse analysis with a number of other theories and methods key to their analysis of content. Of those:
<list list-type="simple"><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>two papers used multimodal discourse analysis (MDA) as based on work done by G.R. Kress and T. van Leeuwen ([<xref rid="RSOS230964C50" ref-type="bibr">50</xref>,<xref rid="RSOS230964C65" ref-type="bibr">65</xref>]; with literature cited);</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>three papers used multimodal critical discourse analysis (MCDA) as based on works by D. Machin, A. Mayr, R. Wodak, T. van Leeuwen and G.R. Kress ([<xref rid="RSOS230964C62" ref-type="bibr">62</xref>,<xref rid="RSOS230964C71" ref-type="bibr">71</xref>,<xref rid="RSOS230964C89" ref-type="bibr">89</xref>]; with literature cited);</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper used critical discourse analysis (CDA) drawing upon frameworks offered by T.A. van Dijk and by N. Fairclough in combination with a genre analysis approach by J.M. Swales ([<xref rid="RSOS230964C81" ref-type="bibr">81</xref>]; with literature cited);</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper used systemic functional multimodal discourse analysis (SF-MDA) based on work by O&#x02019;Halloran 2008 to analyse iconisation ([<xref rid="RSOS230964C54" ref-type="bibr">54</xref>]; with literature cited)</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper used &#x02018;diatextual&#x02019; analysis&#x02014;a rhetorical approach to discourse analysis developed by the authors ([<xref rid="RSOS230964C54" ref-type="bibr">54</xref>]; with literature cited);</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper used a rhetorical approach to discourse analysis based on M. Bakhtin&#x02019;s notion of carnival ([<xref rid="RSOS230964C90" ref-type="bibr">90</xref>]; with literature cited);</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper used relied on the work done by T.A. van Dijk and P. Bourdieu for their discourse analysis ([<xref rid="RSOS230964C73" ref-type="bibr">73</xref>]; with literature cited);</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper drew upon S. Hall&#x02019;s work for the analysis of media power and class power and on N. Cook&#x02019;s work for analysis of music ([<xref rid="RSOS230964C94" ref-type="bibr">94</xref>]; with literature cited).</p></list-item></list></p></list-item><list-item><label>(II)<x xml:space="preserve"> </x></label><p>Seven papers engaged in content analysis while using approaches originating from political communication, sociology, history, philosophy and anthropology, and anchored in:
<list list-type="simple"><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>philosophy of language (epistemic activism, resistance, friction) based on work by J. Stanley&#x02019;s work ([<xref rid="RSOS230964C74" ref-type="bibr">74</xref>]; with literature cited);</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>framing analysis (grounded theory) based on works by R. Entman, D. Scheufele, D. Tewksbury &#x00026; S.D. Reese ([<xref rid="RSOS230964C97" ref-type="bibr">97</xref>]; with literature cited);</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>human geography as rooted in anthropology, history and sociology ([<xref rid="RSOS230964C85" ref-type="bibr">85</xref>]; with literature cited);</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>cultural sociology (grounded theory) ([<xref rid="RSOS230964C68" ref-type="bibr">68</xref>]; with literature cited);</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>theories of cognitive dissonance, parasocial interaction, social identification (grounded theory) ([<xref rid="RSOS230964C93" ref-type="bibr">93</xref>]; with literature cited);</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>analysis by considering theories of gendered framing [<xref rid="RSOS230964C67" ref-type="bibr">67</xref>];</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>an ethnographic technique, which is document-driven and across multiple sites, is used by Krafft &#x00026; Donovan [<xref rid="RSOS230964C53" ref-type="bibr">53</xref>], based on the principles of Geiger &#x00026; Ribes 2011.</p></list-item></list></p></list-item><list-item><label>(III)<x xml:space="preserve"> </x></label><p>Six papers did not have a clearly defined theoretical and methodological framework [<xref rid="RSOS230964C66" ref-type="bibr">66</xref>,<xref rid="RSOS230964C77" ref-type="bibr">77</xref>,<xref rid="RSOS230964C83" ref-type="bibr">83</xref>,<xref rid="RSOS230964C86" ref-type="bibr">86</xref>,<xref rid="RSOS230964C87" ref-type="bibr">87</xref>,<xref rid="RSOS230964C92" ref-type="bibr">92</xref>].</p></list-item></list>Out of 22 papers only 12 engaged in multimodal analysis of content which relied on a defined theoretical and methodological framework. Papers from (I) were grounded in frameworks which allow for fine-grained and evidence-based analyses of social interaction and associated power relations, whereas papers from (II) relied rather on intuition of analysts for their interpretation of language and visual inputs in message communication while focusing more on the analysis of context than rather than content. Papers from (III) presented descriptive analyses not rooted in any clearly defined frameworks for qualitative analyses. Out of 22 papers only four from (I) relied on an approach rooted in linguistics and social semiotics for multimodal analysis of discourse and communication of dis/misinformation. We note that despite large bodies of research done on multimodal communication in cognitive and corpus linguistics as well as experimental psychology, both disciplinary areas have been conspicuously absent from multimodal research on dis-/misinformation.</p></sec><sec id="s3b3"><label>3.2.3<x xml:space="preserve">. </x></label><title>Quantitative papers: theoretical frameworks and methods used</title><p>By contrast to the previous section, for quantitative papers (<italic toggle="yes">n</italic> = 20) we observed less variety in the theoretical and methodological approaches that they used. We found that the main approaches of each quantitative paper fell into the following five categories, within which there was much more uniformity compared to the diversity of qualitative papers in each of three categories above:
<list list-type="simple"><list-item><label>(I)<x xml:space="preserve"> </x></label><p>Fifteen papers used some form of black-box statistical analysis; a cursory analysis yields:
<list list-type="simple"><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>twelve papers chose to train multimodal networks with supervised learning;</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>two papers [<xref rid="RSOS230964C60" ref-type="bibr">60</xref>,<xref rid="RSOS230964C63" ref-type="bibr">63</xref>] employed unsupervised learning techniques for their training;</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper [<xref rid="RSOS230964C80" ref-type="bibr">80</xref>] directly leveraged existing AI tools to extract pertinent details from a dataset.</p></list-item></list></p></list-item><list-item><label>(II)<x xml:space="preserve"> </x></label><p>Interpretable statistical methods were applied to six papers in order to analyse the authors&#x02019; data
<list list-type="simple"><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>conducting and analysing surveys were the focus of three papers; of these, two papers [<xref rid="RSOS230964C52" ref-type="bibr">52</xref>,<xref rid="RSOS230964C56" ref-type="bibr">56</xref>] went beyond simple statistics by using a hierarchical/multilevel regression analysis scheme;</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper [<xref rid="RSOS230964C91" ref-type="bibr">91</xref>] developed a vector method of categorizing social media posts based on their reliability and consistency;</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>results from a psychology experiment [<xref rid="RSOS230964C20" ref-type="bibr">20</xref>] were analysed using various statistical methods.</p></list-item></list></p></list-item><list-item><label>(III)<x xml:space="preserve"> </x></label><p>The creation of a multimodal disinformation database was the focus of two papers [<xref rid="RSOS230964C55" ref-type="bibr">55</xref>,<xref rid="RSOS230964C72" ref-type="bibr">72</xref>].</p></list-item><list-item><label>(IV)<x xml:space="preserve"> </x></label><p>One paper [<xref rid="RSOS230964C84" ref-type="bibr">84</xref>] included network and spatial distribution analyses to investigate how disinformation spread.</p></list-item><list-item><label>(V)<x xml:space="preserve"> </x></label><p>One paper [<xref rid="RSOS230964C95" ref-type="bibr">95</xref>] discussed the development of software to perform fact-checking.</p></list-item></list>We note that except for three papers in (II), all the publications above originated from the field of computer science. The majority of quantitative papers tried to automate the detection of false news by the training of ML algorithms. On the other hand, only one paper attempted to study the actual properties of disinformation using ML. This disparity is indicative of a wider issue. It indicates that advanced quantitative methods to detect multimodal disinformation are being prioritized over investigating multimodal features of disinformation; there is no historical corpus to suggest the latter is a solved problem. At the same time, the narrow spread of research highlights the possibility of missed opportunities to answer broader research questions, particularly those outside computer science. An interdisciplinary approach may avoid this.</p><p>Though these quantitative papers evidently engaged with the topic of disinformation, rarely did they investigate the roles that multiple modalities play. This means despite the clear interest from computer science, it is predominantly other disciplines that drive forward the theoretical understanding of multimodal disinformation.</p></sec><sec id="s3b4"><label>3.2.4<x xml:space="preserve">. </x></label><title>Instances where papers employed both qualitative and quantitative methods</title><p>Incorporating both quantitative and qualitative methods probably indicates the development of pre-existing manual approaches. Seven papers were found to do both; these can be considered to fall within three broad categories:
<list list-type="simple"><list-item><label>(I)<x xml:space="preserve"> </x></label><p>Expanding qualitative analyses with quantitative approaches
<list list-type="simple"><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper [<xref rid="RSOS230964C65" ref-type="bibr">65</xref>] used the approach of multimodal discourse analysis by C. Jewitt to annotate a subset of the dataset studied; then a software-based multimodal analyser was used to expand this analysis across their dataset;</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper used systemic functional multimodal discourse analysis (SF-MDA) based on work by O&#x02019;Halloran followed a similar approach as above by using the same software [<xref rid="RSOS230964C78" ref-type="bibr">78</xref>];</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper first has health experts manually annotate videos [<xref rid="RSOS230964C88" ref-type="bibr">88</xref>], and after extracting for a range of video features (such as a transcript or acoustic features) a ML model is constructed to detect health misinformation within videos.</p></list-item></list></p></list-item><list-item><label>(II)<x xml:space="preserve"> </x></label><p>Attempting to clarify quantitative results by subsequently applying qualitative analysis
<list list-type="simple"><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>after automatic coding of URLs was used to determine the types of media shared on WhatsApp, one paper [<xref rid="RSOS230964C96" ref-type="bibr">96</xref>] manually reviewed a randomized subset of the collected data to provide a more fine-grained understanding of the URL content;</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper that created a multiclass classification network examined a collection of the model&#x02019;s outputs; from this they could determine what categories of material their model was able to better recognize [<xref rid="RSOS230964C69" ref-type="bibr">69</xref>].</p></list-item></list></p></list-item><list-item><label>(III)<x xml:space="preserve"> </x></label><p>A collection of disparate methods used as part of a single topic of investigation
<list list-type="simple"><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one paper manually sought to descriptively categorize the types of shared content, before engaging in a network analysis of the spread of such content [<xref rid="RSOS230964C51" ref-type="bibr">51</xref>];</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>one psychology paper employed a range of statistical analyses on their experimental results which was followed by qualitative insights gained from subject interviews [<xref rid="RSOS230964C61" ref-type="bibr">61</xref>].</p></list-item></list></p></list-item></list>While six papers above had computer scientist authors, three of these papers had interdisciplinary authorship. In the context of all the computer science papers we analysed, interdisciplinary authorship accounted for only around 15% of the work. Two of these papers included arts and humanities authors, while the third featured contributions from the medical sciences. This interdisciplinarity unlocked the possibility of new work.</p></sec><sec id="s3b5"><label>3.2.5<x xml:space="preserve">. </x></label><title>Definitions of dis-/misinformation</title><p>All of the articles analysed looked at an aspect of how information can be manipulated or distorted in online or broadcast media in the form of misinformation, disinformation, or more broadly propaganda or bias. More than half of the articles (<italic toggle="yes">n</italic> = 33) assumed the definition of misinformation implicitly and did not provide a definition. Only five articles provided a definition of disinformation and four articles provided a definition of misinformation, while eight articles provided a definition of fake news.</p><p>Of the articles that provided a definition of misinformation and disinformation, only one article [<xref rid="RSOS230964C64" ref-type="bibr">64</xref>] used the same definition as used in this paper. While all of the variations of the definition of disinformation alluded to the intentionality of the agents spreading false information, the definitions of misinformation were more variable. Unlike the definition adopted by this report, none of them noted the unintentional nature of misinformation. This suggests little progress in the field of understanding the mechanisms and subtleties of (multimodal) misinformation.</p><p>For papers that consider detection of dis-/misinformation, it is crucial to define clearly and fully the problem being investigated. The provision of clear and full definitions of dis-/misinformation is an issue the research field needs to address going forward.</p></sec><sec id="s3b6"><label>3.2.6<x xml:space="preserve">. </x></label><title>Value of multimodal analysis according to the authors of studies</title><p>Six out of 49 articles did not discuss the value of multimodal analysis. The articles which addressed this question did so with a varying degree of explicitness. Those which were more explicit noted that multimodal analyses add value in that, for example:
<list list-type="simple"><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>a multimodal approach which considers the functions of language and images and/or videos together has the potential to shed further light on understanding the construction and impact of propaganda [<xref rid="RSOS230964C78" ref-type="bibr">78</xref>], with the visual modality typically being key for communicating across countries and cultures [<xref rid="RSOS230964C82" ref-type="bibr">82</xref>];</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>visual input changes the way a person is perceived [<xref rid="RSOS230964C20" ref-type="bibr">20</xref>];</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>ignoring visual content on social media loses a lot of the content [<xref rid="RSOS230964C80" ref-type="bibr">80</xref>];</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>the text, sound, dance, visuals and context interact as a unit to convey multiple layers of meaning [<xref rid="RSOS230964C94" ref-type="bibr">94</xref>];</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>multimodal considerations of multimodal data boost detection rates when compared to unimodal approaches [<xref rid="RSOS230964C48" ref-type="bibr">48</xref>].</p></list-item></list></p></sec><sec id="s3b7"><label>3.2.7<x xml:space="preserve">. </x></label><title>The overall findings of the published studies</title><p>The articles differed in terms of how clearly they presented their findings and achievements and also in terms of the domains within which those findings and achievements fell, namely: (i) the development of theoretical and methodological approaches for multimodal analysis of dis-/misinformation including but not limited to the detection of fakes; (ii) the creation of new multimodal datasets; (iii) insights from qualitative and/or quantitative analyses of multimodal data (e.g. discourses and media contents) of a certain kind. The broad variety of theories, methodologies, and type of data used by the studies considered prevented us from going deeper into the analysis of their findings and achievements here, but the majority of authors emphasized the importance of analysing visual and sound data in addition to textual data to improve the accuracy and validity of dis-/misinformation and propaganda research including that on the detection of fakes. At the same time the authors commented on the challenges which such multimodal research presents. We engage in a deeper analysis of the findings of the subcategory of the studies reviewed below in Section &#x02018;Stage 2: Zooming into multimodal disinformation and misinformation publications after March 2020&#x02019;&#x02014;those mainly originated from the computer science.</p></sec><sec id="s3b8"><label>3.2.8<x xml:space="preserve">. </x></label><title>Ethical or social challenges raised by the authors of studies</title><p>Out of 49 articles, only one explicitly engaged with of an ethical challenge [<xref rid="RSOS230964C73" ref-type="bibr">73</xref>]. The paper, having observed and followed &#x02018;far-right and neo-facist&#x02019; social media posts, touched upon the ethical issues of researching violent and extremist content. It also discussed how researchers can be protected and whether to reveal the identities of people calling for violence in the context of the wider issue of the invasion of privacy. For the purposes of their study, the author chose to anonymize all the published results.</p><p>Research on multimodal dis-/misinformation ought to engage better, and more explicitly, with ethical and social challenges. Such engagement ultimately translates into informing government policies, among bringing other social benefits and needs to form part of the research field.</p></sec></sec><sec id="s3c"><label>3.3<x xml:space="preserve">. </x></label><title>Co-citation networks</title><p>The network community detection is good at recovering groupings that we had identified based on the methods and research questions of the papers. The co-citation analysis and visualization was performed on journals that were cited a minimum of 5 times.</p><p>Then we looked at what fields were citing each other (results shown in <xref rid="RSOS230964TB2" ref-type="table">table 2</xref>). The subjects are taken from Scopus. With the exception of journals in life sciences (which in our dataset were journals in cognitive neuroscience), journals in one discipline predominantly cited articles published in journals in the same discipline. For example, 80% of the citations within social science and humanities journals are to other journals in the social sciences and humanities. Overall, social science journals made up at least 25% of all citations regardless of the discipline of the citing journal.
<table-wrap position="float" id="RSOS230964TB2"><label>Table 2<x xml:space="preserve">. </x></label><caption><p>The tendency for a subject to cite another subject is shown between four subject categories. Specifically, the proportion, for each subject, of citations for the <italic toggle="yes">citing</italic> journal to the <italic toggle="yes">cited</italic> journal is shown in the right-most column.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1">citing journal subject</th><th align="left" rowspan="1" colspan="1">cited journal subject</th><th align="left" rowspan="1" colspan="1">normalized % citations</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">social sciences and humanities (<italic toggle="yes">n</italic> = 351)</td><td rowspan="1" colspan="1">social sciences and humanities</td><td rowspan="1" colspan="1">80</td></tr><tr><td rowspan="1" colspan="1">social sciences and humanities</td><td rowspan="1" colspan="1">physical sciences</td><td rowspan="1" colspan="1">11</td></tr><tr><td rowspan="1" colspan="1">social sciences and humanities</td><td rowspan="1" colspan="1">life sciences</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">social sciences and humanities</td><td rowspan="1" colspan="1">health sciences</td><td rowspan="1" colspan="1">9</td></tr><tr><td rowspan="1" colspan="1">physical sciences (<italic toggle="yes">n</italic> = 33)</td><td rowspan="1" colspan="1">social sciences and humanities</td><td rowspan="1" colspan="1">33</td></tr><tr><td rowspan="1" colspan="1">physical sciences</td><td rowspan="1" colspan="1">physical sciences</td><td rowspan="1" colspan="1">61</td></tr><tr><td rowspan="1" colspan="1">physical sciences</td><td rowspan="1" colspan="1">life sciences</td><td rowspan="1" colspan="1">6</td></tr><tr><td rowspan="1" colspan="1">physical sciences</td><td rowspan="1" colspan="1">health sciences</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">life sciences (<italic toggle="yes">n</italic> = 39)</td><td rowspan="1" colspan="1">social sciences and humanities</td><td rowspan="1" colspan="1">38</td></tr><tr><td rowspan="1" colspan="1">life sciences</td><td rowspan="1" colspan="1">physical sciences</td><td rowspan="1" colspan="1">31</td></tr><tr><td rowspan="1" colspan="1">life sciences</td><td rowspan="1" colspan="1">life sciences</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">life sciences</td><td rowspan="1" colspan="1">health sciences</td><td rowspan="1" colspan="1">31</td></tr><tr><td rowspan="1" colspan="1">health sciences (<italic toggle="yes">n</italic> = 4)</td><td rowspan="1" colspan="1">social sciences and humanities</td><td rowspan="1" colspan="1">25</td></tr><tr><td rowspan="1" colspan="1">health sciences</td><td rowspan="1" colspan="1">physical sciences</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">health sciences</td><td rowspan="1" colspan="1">life sciences</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">health sciences</td><td rowspan="1" colspan="1">health sciences</td><td rowspan="1" colspan="1">75</td></tr></tbody></table></table-wrap></p></sec></sec><sec id="s4"><label>4<x xml:space="preserve">. </x></label><title>Stage 2: zooming into multimodal disinformation and misinformation publications after March 2020</title><p>The growth of papers falling within our search parameters has an exponential profile, as shown in <xref rid="RSOS230964F2" ref-type="fig">figure 2</xref>. Hence, individually sifting through each paper would become unfeasible. Moreover, as COVID-19 has shaped many research interests, we felt it was appropriate to set March 2020 as a threshold to more narrowly consider only publications that explicitly aimed to research multimodal dis-/misinformation. To do this, the steps of &#x000a7;2 were repeated in August 2022, except that we additionally filtered out any papers that did not contain the (case-insensitive) key words <inline-formula><mml:math id="IM1"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mstyle mathsize="0.85em"><mml:mrow><mml:mi mathvariant="normal">MULTIMODAL</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mstyle mathsize="0.85em"><mml:mrow><mml:mi mathvariant="normal">MULTI</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">MODAL</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math></inline-formula> in either the title or abstract. This reduced the list to <italic toggle="yes">n</italic><sub>0</sub> = 133. Similar to Stage 1, papers that were not directly related to multimodal dis-/misinformation were manually excluded: this gave a final <italic toggle="yes">n</italic><sub>f</sub> = 78. We report on our observations relevant to advancing the field, highlighting best practices.</p><sec id="s4a"><label>4.1<x xml:space="preserve">. </x></label><title>The rise of computer science since second quarter of 2020</title><p>From 2020 onward, computer science (CS) clearly became the significant majority of all these publications considered (<xref rid="RSOS230964F10" ref-type="fig">figure 10</xref>); in fact it largely accounts for the overall growth in publications. Evidently this particular shift to CS merits further analysis. Out of the <italic toggle="yes">n</italic><sub>f</sub> = 78, <italic toggle="yes">n</italic><sub>cs</sub> = 73 had CS authors, from a range of international institutions (<xref rid="RSOS230964TB3" ref-type="table">table 3</xref>). This section aims to motivate the direction of future work.
<fig position="float" id="RSOS230964F10"><label>Figure 10<x xml:space="preserve">. </x></label><caption><p>This chart compares the number of CS versus non CS publications across both Stages 1 and 2 using the stricter selection criteria outlined in Stage 2. All papers were screened to check that they were applicable to dis-/misinformation; the earliest example dated from 2015.</p></caption><graphic xlink:href="rsos230964f10" position="float"/></fig>
<table-wrap position="float" id="RSOS230964TB3"><label>Table 3<x xml:space="preserve">. </x></label><caption><p>The &#x02018;intensity&#x02019; of country&#x02019;s research into multimodal disinformation was estimated by summing the instances of each participating institution across all papers, and normalizing by the number of citeable documents for that host country [<xref rid="RSOS230964C98" ref-type="bibr">98</xref>].</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1">country</th><th align="left" rowspan="1" colspan="1">research intensity (&#x000d7;10<sup>4</sup>)</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">Qatar</td><td rowspan="1" colspan="1">3.961</td></tr><tr><td rowspan="1" colspan="1">Bulgaria</td><td rowspan="1" colspan="1">2.992</td></tr><tr><td rowspan="1" colspan="1">Vietnam</td><td rowspan="1" colspan="1">1.155</td></tr><tr><td rowspan="1" colspan="1">India</td><td rowspan="1" colspan="1">0.927</td></tr><tr><td rowspan="1" colspan="1">Spain</td><td rowspan="1" colspan="1">0.576</td></tr><tr><td rowspan="1" colspan="1">Singapore</td><td rowspan="1" colspan="1">0.433</td></tr><tr><td rowspan="1" colspan="1">China</td><td rowspan="1" colspan="1">0.426</td></tr><tr><td rowspan="1" colspan="1">USA</td><td rowspan="1" colspan="1">0.306</td></tr><tr><td rowspan="1" colspan="1">UK</td><td rowspan="1" colspan="1">0.257</td></tr></tbody></table></table-wrap></p><sec id="s4a1"><label>4.1.1<x xml:space="preserve">. </x></label><title>Automatic detection of multimodal dis-/misinformation with machine learning</title><p>Advances in ML algorithms, coupled with decreased costs to access computing hardware, has led to many more computer scientists applying ML to new tasks. As such, the stated purpose of an overwhelming <inline-formula><mml:math id="IM2"><mml:mn>90.4</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:math></inline-formula> of papers within this Stage 2 review set proposed a new method for the detection of multimodal dis-/misinformation. Applying ML to multimodal dis-/misinformation has garnered such interest that within our review set two new workshops in this area had emerged. A number of papers were from the workshop &#x02018;De-Factify&#x02019;&#x02014;concerning multimodal fact-checking and hate-speech detection&#x02014;and another paper was submitted as part of the workshop &#x02018;MAD2022&#x02019; which focused on multimedia disinformation.</p><p>Usefully combining multiple distinct inputs for use in neural networks is challenging. This is because the various inputs can be weak (uninformative), or may have a strong interdependence. As multimodal disinformation varies widely, so does the salience of each modality, or their interactions. Consequently, Chen <italic toggle="yes">et al.</italic> [<xref rid="RSOS230964C99" ref-type="bibr">99</xref>] and Song <italic toggle="yes">et al.</italic> [<xref rid="RSOS230964C100" ref-type="bibr">100</xref>] show that including multiple modalities, without additional handling, can increase detection noise. This suggests that dynamically altering the importance of each modality is crucial. <xref rid="RSOS230964F11" ref-type="fig">Figure 11</xref> depicts a generalized multimodal disinformation detector; a dynamic implementation is able to vary the weighting between modules 1&#x02013;3 depending on the input data. If the weightings are frozen after training, this na&#x000ef;vely assumes that there is minimal distributional difference between the training data and the real world, which is often unwarranted [<xref rid="RSOS230964C101" ref-type="bibr">101</xref>]. A total of 8 papers did this. Unfortunately, a further 23 papers (<inline-formula><mml:math id="IM3"><mml:mn>31.5</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:math></inline-formula>) in our Stage 2 review set had either a weighting of 0 for module 3&#x02019;s connections, or performed &#x02018;late fusion&#x02019; of single-modality classifier outputs. Neither of these methods is sufficient to infer the general properties of disinformation, which can depend strongly on the multimodal interactions.
<fig position="float" id="RSOS230964F11"><label>Figure 11<x xml:space="preserve">. </x></label><caption><p>This is a general schematic of a bimodal disinformation detector. The blue boxes labelled 1 and 2 are modules that process each modality of the input data. The role of module 3 is to pass information, no matter how complex, between modalities. The outputs of these modules are first aggregated before then being passed into a black box classifier; concatenation and &#x02018;multi-layer perceptrons&#x02019; were the most commonly observed ML methods, respectively.</p></caption><graphic xlink:href="rsos230964f11" position="float"/></fig></p><p>Finding more suitable misclassification penalties (i.e. &#x02018;loss functions&#x02019;) can itself prove effective. Three papers [<xref rid="RSOS230964C102" ref-type="bibr">102</xref>&#x02013;<xref rid="RSOS230964C104" ref-type="bibr">104</xref>] attempted to address the heterogeneity of the disinformation landscape by enforcing orthogonality between distinct news events. This was done by employing an &#x02018;adversarial approach&#x02019; to the loss function during training. The competition between the generator and classifier networks helped to capture differences between domain-specific and domain-independent features. This approach requires defining, or categorizing, these domains <italic toggle="yes">a priori</italic>.</p></sec><sec id="s4a2"><label>4.1.2<x xml:space="preserve">. </x></label><title>Attempts to analyse the models and their decisions</title><p>Most papers engaged with our question of unimodality versus multimodality by disabling a modality input&#x02014;a process referred to as &#x02018;ablation&#x02019;. However, these tended to show only small improvements from the inclusion of images. This may be in part be explained by the strengths of natural language processing algorithms. Some authors additionally performed qualitative analysis of their ablation, but often the examples found, where a multimodal paradigm prevailed, had minimal linguistic rationale. We point to Wu <italic toggle="yes">et al.</italic> [<xref rid="RSOS230964C105" ref-type="bibr">105</xref>] as a clear example where the authors&#x02019; model had captured modality-specific cues.</p><p>Understanding why models respond in the way they do is one of the goals of developing &#x02018;explainable AI&#x02019; or &#x02018;XAI&#x02019;. Given the potential complexity of multimodal disinformation, if such detectors are to be seriously considered in the real world, incorporating XAI methods may become a key requirement. The three main approaches we encountered were:
<list list-type="simple"><list-item><label>(I)<x xml:space="preserve"> </x></label><p>Direct analysis of the detection model; only one paper [<xref rid="RSOS230964C106" ref-type="bibr">106</xref>] explicitly did this, which provided insight into what aspects of the data their model was reacting to</p></list-item><list-item><label>(II)<x xml:space="preserve"> </x></label><p>Examination of the dataset(s) properties
<list list-type="simple"><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>mainly constrained to dataset creation papers (see next section), though estimates for biases are not always performed;</p></list-item><list-item><label>&#x02014;<x xml:space="preserve"> </x></label><p>the models may be configured to present data statistics, for instance presenting the levels of inter-modality interaction [<xref rid="RSOS230964C99" ref-type="bibr">99</xref>], the discordance of each modality [<xref rid="RSOS230964C107" ref-type="bibr">107</xref>], or the model&#x02019;s modality weightings [<xref rid="RSOS230964C108" ref-type="bibr">108</xref>].</p></list-item></list></p></list-item><list-item><label>(III)<x xml:space="preserve"> </x></label><p>Visualizing the decision; visualizing data and the model can offer insight to researchers, but aside from [<xref rid="RSOS230964C109" ref-type="bibr">109</xref>] this was rarely attempted</p></list-item></list>Particularly for (II) and (III), domain experts can be invaluable for finding clues and patterns; this again suggests an interdisciplinary approach may provide new insights. Lastly, we note that for (II), there has been no work systematically exploring biases (and if these biases may be themselves be multimodal) and their effects on disinformation detection.</p></sec><sec id="s4a3"><label>4.1.3<x xml:space="preserve">. </x></label><title>&#x02018;Fake News&#x02019;: definitions, prevalence and classification consequences</title><p>As seen in the papers in Stage 1, key words relating to disinformation are often not defined, only accounting for 37% of this subset. This may be partly explained by widespread adoption of the term &#x02018;Fake News&#x02019;. This is synonymous with binary classification&#x02014;the dominant paradigm. By contrast, only seven papers (9.6%) acted to classify in at least three ways [<xref rid="RSOS230964C102" ref-type="bibr">102</xref>,<xref rid="RSOS230964C110" ref-type="bibr">110</xref>&#x02013;<xref rid="RSOS230964C116" ref-type="bibr">116</xref>]; definitions were introduced to justify their classification objectives. A general disinformation detector cannot fall within the scope of a binary classifier. It is still possible for a binary classifier to able to accurately categorize a subset of disinformation; defining this type of disinformation is then a practical necessity.</p></sec></sec><sec id="s4b"><label>4.2<x xml:space="preserve">. </x></label><title>Multimodal data, its properties and how it was used</title><p>There were five non-CS papers. Of these, full video featured heavily; two papers applied multimodal analysis [<xref rid="RSOS230964C113" ref-type="bibr">113</xref>,<xref rid="RSOS230964C117" ref-type="bibr">117</xref>] to investigate the video content and messaging, whereas one psychology paper investigated the effects of flagging deepfakes on subjects [<xref rid="RSOS230964C118" ref-type="bibr">118</xref>]. On the other hand, only a couple of CS papers went beyond two modalities: one focused on examining sequences of images and their captions from YouTube [<xref rid="RSOS230964C119" ref-type="bibr">119</xref>] and one paper on TikTok went further still by also incorporating audio [<xref rid="RSOS230964C120" ref-type="bibr">120</xref>]. Some papers stated metadata as an additional modality, but as justified in &#x000a7;1.2 this would not be counted in this paper. The sources of data used by papers were not extensive; the levels of data-source mixing are depicted in <xref rid="RSOS230964F12" ref-type="fig">figure 12</xref><italic toggle="yes">a</italic>.
<fig position="float" id="RSOS230964F12"><label>Figure 12<x xml:space="preserve">. </x></label><caption><p>Shown here is where the data originates and how these sources mix. The breakdowns were chosen to highlight the sorts of disinformation studied. A source was counted if the paper&#x02019;s data source was distinct (e.g. datasets &#x02018;X&#x02019; and &#x02018;Y&#x02019; count as two). We counted 91 instances of sources, hence an average mixing of 1.25 sources per paper&#x02014;largely independent. (<italic toggle="yes">a</italic>) Depicting the mixing of data sources and (<italic toggle="yes">b</italic>) breakdown of &#x02018;Miscellaneous&#x02019; in (<italic toggle="yes">a</italic>).</p></caption><graphic xlink:href="rsos230964f12" position="float"/></fig></p><p>We saw a number of detection papers expressing regret at the lack of accessible datasets. The stated aims of seven papers were to add to the range of available training data. The main challenge is labelling large quantities of data. For datasets focusing on collating online news articles, two papers chose to label by quantifying the publishers &#x02018;credibility&#x02019; based on results from fact-checking organizations [<xref rid="RSOS230964C121" ref-type="bibr">121</xref>,<xref rid="RSOS230964C122" ref-type="bibr">122</xref>]. This is problematic though as disinformation is complex and can be intermingled within legitimate news and claims; see, for example [<xref rid="RSOS230964C123" ref-type="bibr">123</xref>]. Using automatic claim matching, two papers sought to create multilingual datasets [<xref rid="RSOS230964C116" ref-type="bibr">116</xref>,<xref rid="RSOS230964C124" ref-type="bibr">124</xref>]. One paper manually verified samples of the data studied, which exclusively focused on Reddit posts [<xref rid="RSOS230964C111" ref-type="bibr">111</xref>]. Another approach is to augment [<xref rid="RSOS230964C125" ref-type="bibr">125</xref>] or synthesize data [<xref rid="RSOS230964C126" ref-type="bibr">126</xref>]. Only one paper considered the threat of the dataset studied, filtering out results that were too straightforward by using an adversarial approach [<xref rid="RSOS230964C126" ref-type="bibr">126</xref>]. Moreover, the authors tested their dataset on real humans as a benchmark&#x02014;an important step, only otherwise fulfilled in [<xref rid="RSOS230964C112" ref-type="bibr">112</xref>]&#x02014;finding both that humans struggled to distinguish between fake and real examples and that their detector performed at a comparable level. It should be noted that while many papers conducted analyses of the datasets&#x02019; textual content, only two papers [<xref rid="RSOS230964C110" ref-type="bibr">110</xref>,<xref rid="RSOS230964C127" ref-type="bibr">127</xref>] additionally collected statistics on their images.</p><p>Only two papers attempted to test their detection models on live unseen data. For instance, Wang <italic toggle="yes">et al.</italic> [<xref rid="RSOS230964C128" ref-type="bibr">128</xref>] scraped and manually labelled COVID-related Instagram posts once a day for a month, and obtained very similar classification results to their initial offline run.</p><p>The early detection of disinformation is vital in limiting its harm, but unseen disinformation can prove challenging for detection (zero-shot classification). If instead a few initial examples within a nascent category are allowed to be manually labelled, this can allow for &#x02018;few-shot classification&#x02019;. To this end, one paper presented a meta-learning approach [<xref rid="RSOS230964C129" ref-type="bibr">129</xref>], which aimed to jointly learn category and global features as they arose. Similarly, one paper held back training examples based on their post time to mimic real life conditions [<xref rid="RSOS230964C130" ref-type="bibr">130</xref>], testing their models performance for different delay times. Overall, we found limited engagement with temporal features of disinformation. In particular, no papers considered the longer term evolution of disinformation, nor presented analysis on any distribution shifts.</p></sec><sec id="s4c"><label>4.3<x xml:space="preserve">. </x></label><title>Interdisciplinary and non-computer science publications</title><p>The stricter corpus requirements left few papers outside CS. Though the sample size is small, the remaining non-CS papers neither notably deviated in approach nor in quality from those papers studied in the Stage 1.</p><p>In one paper [<xref rid="RSOS230964C127" ref-type="bibr">127</xref>] theories of communication both informed the quantitative approach and yielded insight into the statistical features of their dataset. Two CS papers [<xref rid="RSOS230964C110" ref-type="bibr">110</xref>,<xref rid="RSOS230964C112" ref-type="bibr">112</xref>] stood out for their use of interdisciplinary methods. These papers both studied memes&#x02014;inherently multimodal objects&#x02014;and drew from methods outside CS for data annotation and analysis methods, as well as informing their use of quantitative methods.</p></sec></sec><sec id="s5"><label>5<x xml:space="preserve">. </x></label><title>Discussion and conclusion</title><p>Our meta-study has shed some light on how research on multimodal dis-/misinformation in media communication has been evolving in the past 20 years. Our division of meta analysis into two stages&#x02014;before and after the second quarter of 2020&#x02014;has reflected changes in the development of the research area due to the start of the COVID pandemic in the second quarter of 2020, namely the prevalence of multimodal research of dis-/misinformation originating from computer science from 2020 onwards.</p><p>Our meta-analysis in Stage 1 identified 303 articles researching dis-/misinformation in media while also focusing on more than one modality. 101 out of those 303 performed multimodal analysis of the content of dis-/misinformation. Our further in-depth qualitative analysis focused on full texts of 49 articles that met our inclusion criteria.</p><p>Our meta-analysis has revealed that there is no single disciplinary or cross-disciplinary community employing multimodal analysis to study dis-/misinformation. Most authors and publication venues appeared only once in our dataset, which suggested that multimodal analysis of dis-/misinformation was not the main subject of study of any given researcher or research community. The diversity of disciplines, from which articles originated&#x02014;from the social sciences and computer science to management, engineering and health sciences&#x02014;further pointed to the lack of an established research community with the focus on multimodal dis-/misinformation. Our topic modelling analysis of the abstracts revealed a disciplinary split. Abstracts from articles published in the social sciences and humanities scored highly on the propaganda topics and low on the online media topic, whereas those published in the physical sciences (including computer science) scored highly on the online media topic but not on the propaganda topics. This dissociation suggested that a barrier to establishing a cross-disciplinary research community was a lack of common focus, terminology and definition of dis-/misinformation.</p><p>Our in-depth analysis of 49 full-text articles revealed a further binary opposition, this time related to the research focus and method used: articles which employed quantitative methods were primarily interested in creating frameworks to detect whether a particular news item or social media post was &#x02018;fake news&#x02019; or not. By contrast, articles using qualitative methods mainly focused on propaganda analysis, as well as multimodal strategies of persuasion and manipulation in media discourse and communication. Only 7 articles out of 49 articles used a mix of qualitative and quantitative methods. Those originated from disciplines which would traditionally use a qualitative approach to analysis. Quantitative methods were used by them to scale up and further support qualitative analysis.</p><p>The majority of studies engaged with the question of the added value of multimodal analysis as well as of the challenges of developing and applying theories and methods suitable for multimodal analysis of dis-misinformation. For many papers these engagements constituted part of the studies&#x02019; findings.</p><p>Our analysis demonstrated that only one paper out of 49 engaged explicitly with the question of ethical and social challenges of multimodal research on dis-/misinformation. We argue that these challenges need to be addressed by the field better in the future.</p><p>Furthermore, more than half of those 49 articles did not contain definitions of misinformation, disinformation, fake news or propaganda. If present, the definitions varied across studies. This suggested the lack of uniform understanding of the objects of study to which those terms refer.</p><p>Our analysis in Stage 1 revealed that research on multimodal dis-/misinformation would benefit from the development of one established area, with clear definitions of research objects, a goal to address ethical and social challenges, a unified terminology and cross-disciplinary methodological practices. Cross-disciplinary practices would benefit not only disciplines that traditionally use qualitative methods, but also those which would traditionally rely on quantitative methods.</p><p>Although we observed a general increase in studies focusing on multimodal dis-/misinformation in 2008 and 2016, it is only from the second quarter of 2020 that we observed a rapid increase in computer science studies on multimodal dis-/misinformation.</p><p>As our meta-analysis of Stage 2 demonstrated that there was no notable change in the research on multimodal dis-/misinformation which originated from disciplines other than computer science. It is computer science studies which became the driver for the explosive growth of research on multimodal dis-/misinformation. That growth came with scholars across many cultures engaging in multimodal dis-/misinformation research.</p><p>Those changes motivated the emerging need to understand the extent to which the choices in ML techniques, more specifically, were informed by knowledge originating from humanities and social sciences. The in-depth examination of full-text articles at Stage 2 revealed positive dynamics in how the computer science studies under consideration addressed the complexities associated with the analysis of multiple modalities. However, while in aggregate these papers employed a range of multimodal strategies, no single paper brought these together. The missed opportunities were driven to a considerable extent by the lack of interdisciplinary approach to analysis. We also identified clear research gaps, such as the limited work on the temporal nature of multimodal dis-/misinformation.</p><p>Our meta-study at Stages 1 and 2 demonstrated that most studies, regardless of discipline, focused on two modalities rather than three. This may be explained by scholars&#x02019; intention to keep analysis more straightforward, but also by the use of pre-prepared data. Especially within computer science, most studies used existing datasets rather than constructing their own.</p><p>For articles analysed in both Stages 1 and 2, we observed no engagement with questions about how dis-/misinformation evolves over time, including shifts in distribution patterns. We consider that this is at least partly due to a lack of studies which employ true interdisciplinary approaches to investigation of dis-/misinformation.</p><p>Nonetheless, single-discipline papers still brought value to the overall study of multimodal dis-/misinformation. In addition to moving beyond text-only approaches, papers that provided definitions of dis-/misinformation, those constructing novel datasets&#x02014;especially video&#x02014;and those which used a combination of qualitative and quantitative methods were particularly valuable.</p><p>Our meta-analysis has revealed the potential for computer science techniques to aid theories of multimodal dis-/misinformation communication originating from the range of disciplines in humanities and social sciences, and scale up qualitative analysis to provide statistical validity. Explainable AI could be a large help in this regard, especially if developed with social science and humanities expertise. More interaction across the humanities and social sciences with computer science could enable further development of AI methods for multimodal analysis of dis-/misinformation. This would require more interdisciplinary research and collaboration to ensure better understanding of the findings originating from the disciplines of humanities and social sciences.</p><p>Our meta-analysis has also demonstrated the challenges of conducting multimodal analysis of dis-/misinformation and the nature of the associated gap in research. The gap manifests itself through the absence of a coherent body of multimodal research on disinformation and misinformation. The divide between different disciplines and research interests in the field was present throughout our analysis including the topic modelling of abstracts, the co-citation analysis and the manual qualitative analysis on the full text of 127 articles (49 full-text articles were analysed at Stage 1 and 78 articles at Stage 2).</p><p>With the advent of accessible computing technology, large scale quantitative analyses constitute a clear new avenue for research into multimodal disinformation and misinformation. Indeed, we observed a recent uptake of this approach; however, efforts to leverage these methods have largely been confined to computer science. This has resulted in many missed research opportunities and even has manifested in experimental design and analysis that is not motivated by theories of multimodal communication. Moving forward, creating a more unified research landscape is needed, which will require the development of unified terminology and definitions suitable for analysis of multimodal dis-/misinformation, as well as a conscious effort from scholars to cross boundaries of disciplines. Among other things, interdisciplinarity should enable more studies to focus on video data and as a result to examine three modalities&#x02014;verbal (text), sound, visual&#x02014;as opposed to just two modalities. Further development of interdisciplinary approaches to the analysis of multimodal dis-/misinformation should also empower researchers to investigate at scale subtle manipulation which forms a large part of dis-/misinformation communication, but is more difficult to research than &#x02019;fakes&#x02019;.</p></sec></body><back><fn-group><fn id="FN1"><label>1</label><p><uri xlink:href="https://imcc.web.ox.ac.uk/projects">https://imcc.web.ox.ac.uk/projects</uri>.</p></fn><fn id="FN2"><label>2</label><p>Homepage at <uri xlink:href="https://www.redhenlab.org">https://www.redhenlab.org</uri>.</p></fn></fn-group><sec sec-type="data-availability" id="s6"><title>Data accessibility</title><p>This article has no additional data.</p></sec><sec id="s7"><title>Declaration of AI use</title><p>We have not used AI-assisted technologies in creating this article.</p></sec><sec id="s8"><title>Authors' contributions</title><p>A.W.: conceptualization, formal analysis, funding acquisition, investigation, methodology, project administration, resources, supervision, validation, visualization, writing&#x02014;original draft, writing&#x02014;review and editing; S.W.: data curation, formal analysis, visualization, writing&#x02014;original draft, writing&#x02014;review and editing; Y.T.: data curation, formal analysis, visualization, writing&#x02014;original draft; S.H.: conceptualization, formal analysis, funding acquisition, investigation, methodology, supervision, writing&#x02014;review and editing.</p><p>All authors gave final approval for publication and agreed to be held accountable for the work performed therein.</p></sec><sec sec-type="COI-statement" id="s9"><title>Conflict of interest declaration</title><p>We declare we have no competing interests.</p></sec><sec id="s10"><title>Funding</title><p>Our comprehensive review was made possible thanks to funding provided by Defence Science and Technology Laboratory, Ministry of Defence (2019&#x02013;2021) and the John Fell Fund of Oxford University (2021&#x02014;present).</p></sec><ref-list><title>References</title><ref id="RSOS230964C1"><label>1<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Altay</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Berriche</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Acerbi</surname>
<given-names>A</given-names></string-name></person-group>. <year>2023</year>
<article-title>Misinformation on misinformation: conceptual and methodological challenges</article-title>. <source>Soc. Media Soc.</source>
<volume><bold>9</bold></volume>, <fpage>1</fpage>-<lpage>13</lpage>.</mixed-citation></ref><ref id="RSOS230964C2"><label>2<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pennycook</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Rand</surname>
<given-names>DG</given-names></string-name></person-group>. <year>2021</year>
<article-title>The psychology of fake news</article-title>. <source>Trends Cogn. Sci.</source>
<volume><bold>25</bold></volume>, <fpage>388</fpage>-<lpage>402</lpage>. (<pub-id pub-id-type="doi">10.1016/j.tics.2021.02.007</pub-id>)<pub-id pub-id-type="pmid">33736957</pub-id>
</mixed-citation></ref><ref id="RSOS230964C3"><label>3<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pantazi</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Hale</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Klein</surname>
<given-names>O</given-names></string-name></person-group>. <year>2021</year>
<article-title>Social and cognitive aspects of the vulnerability to political misinformation</article-title>. <source>Pol. Psychol.</source>
<volume><bold>42</bold></volume>, <fpage>267</fpage>-<lpage>304</lpage>. (<pub-id pub-id-type="doi">10.1111/pops.12797</pub-id>)</mixed-citation></ref><ref id="RSOS230964C4"><label>4<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Verrall</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Mason</surname>
<given-names>D</given-names></string-name></person-group>. <year>2018</year>
<article-title>The taming of the shrewd</article-title>. <source>RUSI J.</source>
<volume><bold>163</bold></volume>, <fpage>20</fpage>-<lpage>28</lpage>. (<pub-id pub-id-type="doi">10.1080/03071847.2018.1445169</pub-id>)</mixed-citation></ref><ref id="RSOS230964C5"><label>5<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lewandowsky</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Ecker</surname>
<given-names>UK</given-names></string-name>, <string-name><surname>Cook</surname>
<given-names>J</given-names></string-name></person-group>. <year>2017</year>
<article-title>Beyond misinformation: understanding and coping with the &#x02018;Post-Truth&#x02019; Era</article-title>. <source>J. Appl. Res. Mem. Cogn.</source>
<volume><bold>6</bold></volume>, <fpage>353</fpage>-<lpage>369</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jarmac.2017.07.008</pub-id>)</mixed-citation></ref><ref id="RSOS230964C6"><label>6<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaccari</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Morini</surname>
<given-names>M</given-names></string-name></person-group>. <year>2014</year>
<article-title>The power of smears in two american presidential campaigns</article-title>. <source>J. Political Mark.</source>
<volume><bold>13</bold></volume>, <fpage>19</fpage>-<lpage>45</lpage>. (<pub-id pub-id-type="doi">10.1080/15377857.2014.866021</pub-id>)</mixed-citation></ref><ref id="RSOS230964C7"><label>7<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jamieson</surname>
<given-names>KH</given-names></string-name>, <string-name><surname>Romer</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Jamieson</surname>
<given-names>PE</given-names></string-name>, <string-name><surname>Winneg</surname>
<given-names>KM</given-names></string-name>, <string-name><surname>Pasek</surname>
<given-names>J</given-names></string-name></person-group>. <year>2021</year>
<article-title>The role of non-COVID-specific and COVID-specific factors in predicting a shift in willingness to vaccinate: a panel study</article-title>. <source>Proc. Natl Acad. Sci. USA</source>
<volume><bold>118</bold></volume>, <fpage>e2112266118</fpage>. (<pub-id pub-id-type="doi">10.1073/pnas.2112266118</pub-id>)<pub-id pub-id-type="pmid">34930844</pub-id>
</mixed-citation></ref><ref id="RSOS230964C8"><label>8<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pierri</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Perry</surname>
<given-names>BL</given-names></string-name>, <string-name><surname>DeVerna</surname>
<given-names>MR</given-names></string-name>, <string-name><surname>Yang</surname>
<given-names>KC</given-names></string-name>, <string-name><surname>Flammini</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Menczer</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Bryden</surname>
<given-names>J</given-names></string-name></person-group>. <year>2022</year>
<article-title>Online misinformation is linked to early COVID-19 vaccination hesitancy and refusal</article-title>. <source>Sci. Rep.</source>
<volume><bold>12</bold></volume>, 5966. (<pub-id pub-id-type="doi">10.1038/s41598-022-10070-w</pub-id>)</mixed-citation></ref><ref id="RSOS230964C9"><label>9<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ferreira Caceres</surname>
<given-names>MM</given-names></string-name>
<etal>et al.</etal></person-group>
<year>2022</year>
<article-title>The impact of misinformation on the COVID-19 pandemic</article-title>. <source>AIMS Public Health</source>
<volume><bold>9</bold></volume>, 262-277. (<pub-id pub-id-type="doi">10.3934/publichealth.2022018</pub-id>)</mixed-citation></ref><ref id="RSOS230964C10"><label>10<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benegal</surname>
<given-names>SD</given-names></string-name>, <string-name><surname>Scruggs</surname>
<given-names>LA</given-names></string-name></person-group>. <year>2018</year>
<article-title>Correcting misinformation about climate change: the impact of partisanship in an experimental setting</article-title>. <source>Clim. Change</source>
<volume><bold>148</bold></volume>, <fpage>61</fpage>-<lpage>80</lpage>. (<pub-id pub-id-type="doi">10.1007/s10584-018-2192-4</pub-id>)</mixed-citation></ref><ref id="RSOS230964C11"><label>11<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meng</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Broom</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Li</surname>
<given-names>A</given-names></string-name></person-group>. <year>2023</year>
<article-title>Impact of misinformation in the evolution of collective cooperation on networks</article-title>. <source>J. R. Soc. Interface</source>
<volume><bold>20</bold></volume>, <fpage>206</fpage>. (<pub-id pub-id-type="doi">10.1098/rsif.2023.0295</pub-id>)</mixed-citation></ref><ref id="RSOS230964C12"><label>12<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Verrall</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Dunkley</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Gane</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Byrne</surname>
<given-names>R</given-names></string-name></person-group>. <year>2019</year>
<article-title>Dangerous liaisons: a &#x02018;Big Four&#x02019; framework that provides a &#x02018;Hint&#x02019; to understanding an Adversary&#x02019;s strategy for influence</article-title>. <source>RUSI J.</source>
<volume><bold>164</bold></volume>, <fpage>52</fpage>-<lpage>68</lpage>. (<pub-id pub-id-type="doi">10.1080/03071847.2019.1643255</pub-id>)</mixed-citation></ref><ref id="RSOS230964C13"><label>13<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gill</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Goolsby</surname>
<given-names>R</given-names></string-name></person-group> (eds). <year>2022</year>
<source>COVID-19 disinformation: a multi-national, whole of society perspective</source>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer Nature</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C14"><label>14<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Verrall</surname>
<given-names>N</given-names></string-name></person-group>. <year>2022</year>
<comment>COVID-19 disinformation, misinformation and malinformation during the pandemic infodemic: a view from the United Kingdom. In <italic toggle="yes">COVID-19 disinformation: a multi-national, whole of society perspective</italic> (eds R Gill, R Goolsby), pp. 81&#x02013;112. Cham, Switzerland: Springer Nature</comment>.</mixed-citation></ref><ref id="RSOS230964C15"><label>15<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heley</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Gaysynsky</surname>
<given-names>A</given-names></string-name>, <string-name><surname>King</surname>
<given-names>AJ</given-names></string-name></person-group>. <year>2022</year>
<article-title>Missing the bigger picture: the need for more research on visual health misinformation</article-title>. <source>Sci. Commun.</source>
<volume><bold>44</bold></volume>, <fpage>514</fpage>-<lpage>527</lpage>. (<pub-id pub-id-type="doi">10.1177/10755470221113833</pub-id>)<pub-id pub-id-type="pmid">36082150</pub-id>
</mixed-citation></ref><ref id="RSOS230964C16"><label>16<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khan</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Brohman</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Addas</surname>
<given-names>S</given-names></string-name></person-group>. <year>2021</year>
<article-title>The anatomy of &#x02018;fake news&#x02019;: studying false messages as digital objects</article-title>. <source>J. Inform. Technol.</source>
<volume><bold>37</bold></volume>, <fpage>122</fpage>-<lpage>143</lpage>. (<pub-id pub-id-type="doi">10.1177/02683962211037693</pub-id>)</mixed-citation></ref><ref id="RSOS230964C17"><label>17<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sundar</surname>
<given-names>SS</given-names></string-name>, <string-name><surname>Molina</surname>
<given-names>MD</given-names></string-name>, <string-name><surname>Cho</surname>
<given-names>E</given-names></string-name></person-group>. <year>2021</year>
<article-title>Seeing is believing: is video modality more powerful in spreading fake news via online messaging apps?</article-title>
<source>J. Comput.-Mediat. Commun.</source>
<volume><bold>26</bold></volume>, <fpage>301</fpage>-<lpage>319</lpage>. (<pub-id pub-id-type="doi">10.1093/jcmc/zmab010</pub-id>)</mixed-citation></ref><ref id="RSOS230964C18"><label>18<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mai</surname>
<given-names>KT</given-names></string-name>, <string-name><surname>Bray</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Davies</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Griffin</surname>
<given-names>LD</given-names></string-name></person-group>. <year>2023</year>
<article-title>Warning: humans cannot reliably detect speech deepfakes</article-title>. <source>PLoS ONE</source>
<volume><bold>18</bold></volume>, <fpage>1</fpage>-<lpage>20</lpage>.</mixed-citation></ref><ref id="RSOS230964C19"><label>19<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><comment>UK Government. 2019 Online Harms White Paper. See <uri xlink:href="https://www.gov.uk/government/consultations/online-harms-white-paper">https://www.gov.uk/government/consultations/online-harms-white-paper</uri> (Last accessed 21 Nov. 2023)</comment>.</mixed-citation></ref><ref id="RSOS230964C20"><label>20<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ecker</surname>
<given-names>UKH</given-names></string-name>, <string-name><surname>Lewandowsky</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Chang</surname>
<given-names>EP</given-names></string-name>, <string-name><surname>Pillai</surname>
<given-names>R</given-names></string-name></person-group>. <year>2014</year>
<article-title>The effects of subtle misinformation in news headlines</article-title>. <source>J. Exp. Psychol.: Appl.</source>
<volume><bold>20</bold></volume>, <fpage>323</fpage>-<lpage>335</lpage>. (<pub-id pub-id-type="doi">10.1037/xap0000028</pub-id>)<pub-id pub-id-type="pmid">25347407</pub-id>
</mixed-citation></ref><ref id="RSOS230964C21"><label>21<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Uhrig</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Payne</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Pavlova</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Burenko</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Dykes</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Baltazan</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Burrows</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Hale</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Torr</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Wilson</surname>
<given-names>A</given-names></string-name></person-group>. <year>2023</year>
<comment>Studying time conceptualisation via speech, prosody, and hand gesture: interweaving manual and computational methods of analysis. In <italic toggle="yes">Gesture and speech in interaction</italic> (<italic toggle="yes">GeSpIn</italic>) <italic toggle="yes">conference</italic> (eds W Pouw <italic toggle="yes">et al.</italic>). Nijmegen, Netherlands: Max Planck Institute for Psycholinguistics</comment>.</mixed-citation></ref><ref id="RSOS230964C22"><label>22<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cooperrider</surname>
<given-names>K</given-names></string-name></person-group>. <year>2014</year>
<article-title>Body-directed gestures: pointing to the self and beyond</article-title>. <source>J. Pragmat.</source>
<volume><bold>71</bold></volume>, <fpage>1</fpage>-<lpage>16</lpage>. (<pub-id pub-id-type="doi">10.1016/j.pragma.2014.07.003</pub-id>)</mixed-citation></ref><ref id="RSOS230964C23"><label>23<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilson</surname>
<given-names>A</given-names></string-name></person-group>. <year>2020</year>
<article-title>It&#x02019;s time to do news again</article-title>. <source>Zeitschrift f&#x000fc;r Anglistik und Amerikanistik</source>
<volume><bold>68</bold></volume>, <fpage>379</fpage>-<lpage>409</lpage>. (<pub-id pub-id-type="doi">10.1515/zaa-2020-2016</pub-id>)</mixed-citation></ref><ref id="RSOS230964C24"><label>24<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kelly</surname>
<given-names>SD</given-names></string-name>, <string-name><surname>&#x000d6;zy&#x000fc;rek</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Maris</surname>
<given-names>E</given-names></string-name></person-group>. <year>2010</year>
<article-title>Two sides of the same coin: speech and gesture mutually interact to enhance comprehension</article-title>. <source>Psychol. Sci.</source>
<volume><bold>21</bold></volume>, <fpage>260</fpage>-<lpage>267</lpage>. (<pub-id pub-id-type="doi">10.1177/0956797609357327</pub-id>)<pub-id pub-id-type="pmid">20424055</pub-id>
</mixed-citation></ref><ref id="RSOS230964C25"><label>25<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kostelnick</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Roberts</surname>
<given-names>DD</given-names></string-name></person-group>. <year>1998</year>
<source>Designing visual language: strategies for professional communicators</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Allyn and Bacon</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C26"><label>26<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Messaris</surname>
<given-names>P</given-names></string-name></person-group>. <year>1997</year>
<source>Visual persuasion: the role of images in advertising</source>. <publisher-name>Thousand Oaks, CA: SAGE Publications</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C27"><label>27<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scott</surname>
<given-names>LM</given-names></string-name></person-group>. <year>1994</year>
<article-title>Images in advertising: the need for a theory of visual rhetoric</article-title>. <source>J. Consum. Res.</source>
<volume><bold>21</bold></volume>, <fpage>252</fpage>-<lpage>273</lpage>. (<pub-id pub-id-type="doi">10.1086/209396</pub-id>)</mixed-citation></ref><ref id="RSOS230964C28"><label>28<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>N&#x000f6;th</surname>
<given-names>W</given-names></string-name></person-group>. <year>1995</year>
<source>Handbook of semiotics</source>. <publisher-loc>Bloomington, Indiana</publisher-loc>: <publisher-name>Indiana University Press</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C29"><label>29<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kress</surname>
<given-names>GR</given-names></string-name>, <string-name><surname>van Leeuwen</surname>
<given-names>T</given-names></string-name></person-group>. <year>1996</year>
<source>Reading images: the grammar of visual design</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Routledge</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C30"><label>30<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Jewitt</surname>
<given-names>C</given-names></string-name></person-group>. <year>2009</year>
<source>The Routledge handbook of multimodal analysis</source>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>Routledge</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C31"><label>31<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Forceville</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Urios-Aparisi</surname>
<given-names>E</given-names></string-name></person-group> (eds). <year>2009</year>
<source>Multimodal metaphor</source>. <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Mouton de Gruyter</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C32"><label>32<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Machin</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Mayr</surname>
<given-names>A</given-names></string-name></person-group>. <year>2012</year>
<source>How to do critical discourse analysis: a multimodal introduction</source>. <publisher-name>London, UK: SAGE Publications</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C33"><label>33<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steen</surname>
<given-names>F</given-names></string-name>
<etal>et al.</etal></person-group>
<year>2018</year>
<article-title>Toward an infrastructure for data-driven multimodal communication research</article-title>. <source>Linguist. Vanguard</source>
<volume><bold>4</bold></volume>, <fpage>20170041</fpage>. (<pub-id pub-id-type="doi">10.1515/lingvan-2017-0041</pub-id>)</mixed-citation></ref><ref id="RSOS230964C34"><label>34<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Tseronis</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Forceville</surname>
<given-names>C</given-names></string-name></person-group> (eds). <year>2017</year>
<source>Multimodal argumentation and rhetoric in media genres</source>. <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>John Benjamins Publishing Company</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C35"><label>35<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Dancygier</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Sweetser</surname>
<given-names>E</given-names></string-name></person-group> (eds). <year>2012</year>
<source>Viewpoint in language: a multimodal perspective</source>. <publisher-name>Cambridge University Press, Cambridge, UK</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C36"><label>36<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vandelanotte</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Dancygier</surname>
<given-names>B</given-names></string-name></person-group>. <year>2017</year>
<article-title>Multimodal artefacts and the texture of viewpoint</article-title>. <source>J. Pragmat.</source>
<volume><bold>122</bold></volume>, <fpage>1</fpage>-<lpage>9</lpage>. (<pub-id pub-id-type="doi">10.1016/j.pragma.2017.10.011</pub-id>)</mixed-citation></ref><ref id="RSOS230964C37"><label>37<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Cienki</surname>
<given-names>A</given-names></string-name>, <string-name><surname>M&#x000fc;ller</surname>
<given-names>C</given-names></string-name></person-group> (eds). <year>2008</year>
<source>Metaphor and gesture</source>. <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>John Benjamins Publishing Company</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C38"><label>38<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Cienki</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Iriskhanova</surname>
<given-names>O</given-names></string-name></person-group> (eds). <year>2018</year>
<source>Aspectuality across languages: event construal in speech and gesture</source>. <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>John Benjamins Publishing Company</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C39"><label>39<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Grishina</surname>
<given-names>E</given-names></string-name></person-group>. <year>2017</year>
<comment><italic toggle="yes">Russian gesticulation from a linguistic point of view. Corpus research</italic> [&#x00420;&#x00443;&#x00441;&#x00441;&#x0043a;&#x00430;&#x0044f; &#x00436;&#x00435;&#x00441;&#x00442;&#x00438;&#x0043a;&#x00443;&#x0043b;&#x0044f;&#x00446;&#x00438;&#x0044f; &#x00441; &#x0043b;&#x00438;&#x0043d;&#x00433;&#x00432;&#x00438;&#x00441;&#x00442;&#x00438;&#x00447;&#x00435;&#x00441;&#x0043a;&#x0043e;&#x00439; &#x00442;&#x0043e;&#x00447;&#x0043a;&#x00438; &#x00437;&#x00440;&#x00435;&#x0043d;&#x00438;&#x0044f;: &#x0043a;&#x0043e;&#x00440;&#x0043f;&#x00443;&#x00441;&#x0043d;&#x0044b;&#x00435; &#x00438;&#x00441;&#x00441;&#x0043b;&#x00435;&#x00434;&#x0043e;&#x00432;&#x00430;&#x0043d;&#x00438;&#x0044f;]. Izdatel&#x02019;skij dom Jazyk: Jazyki slavjanskoj kul&#x02019;tury, Moscow</comment>.</mixed-citation></ref><ref id="RSOS230964C40"><label>40<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turner</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Avelar</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Mendes de Oliveira</surname>
<given-names>M</given-names></string-name></person-group>. <year>2019</year>
<article-title>Blended classic joint attention and multimodal deixis</article-title>. <source>Signo</source>
<volume><bold>44</bold></volume>, <fpage>3</fpage>-<lpage>9</lpage>. (<pub-id pub-id-type="doi">10.17058/signo.v44i79.12710</pub-id>)</mixed-citation></ref><ref id="RSOS230964C41"><label>41<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hostetter</surname>
<given-names>AB</given-names></string-name></person-group>. <year>2011</year>
<article-title>When do gestures communicate? A meta-analysis</article-title>. <source>Psychol. Bull.</source>
<volume><bold>137</bold></volume>, <fpage>297</fpage>-<lpage>315</lpage>. (<pub-id pub-id-type="doi">10.1037/a0022128</pub-id>)<pub-id pub-id-type="pmid">21355631</pub-id>
</mixed-citation></ref><ref id="RSOS230964C42"><label>42<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Beattie</surname>
<given-names>G</given-names></string-name></person-group>. <year>2016</year>
<source>Rethinking body language: how hand movements reveal hidden thoughts</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Routledge</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C43"><label>43<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kita</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Alibali</surname>
<given-names>MW</given-names></string-name>, <string-name><surname>Chu</surname>
<given-names>M</given-names></string-name></person-group>. <year>2017</year>
<article-title>How do gestures influence thinking and speaking? The gesture-for-conceptualization hypothesis</article-title>. <source>Psychol. Rev.</source>
<volume><bold>124</bold></volume>, <fpage>245</fpage>. (<pub-id pub-id-type="doi">10.1037/rev0000059</pub-id>)<pub-id pub-id-type="pmid">28240923</pub-id>
</mixed-citation></ref><ref id="RSOS230964C44"><label>44<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Waisman</surname>
<given-names>OS</given-names></string-name></person-group>. <year>2010</year>
<source>Body, language and meaning in conflict situations: a semiotic analysis of gesture-word mismatches in Israeli-Jewish and Arab discourse</source>, <volume>vol. 62</volume>. <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>John Benjamins Publishing</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C45"><label>45<x xml:space="preserve">. </x></label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Adami</surname>
<given-names>E</given-names></string-name></person-group>. <year>2010</year>
<comment>ELF and sign-making practices on YouTube: between globalization and specificities. <italic toggle="yes">From international to local English &#x02013; and back again</italic> (eds R Facchinetti, D Crystal, B Seidlhofer), pp. 235&#x02013;264.</comment>
<publisher-loc>Bern, Switzerland</publisher-loc>: <publisher-name>Peter Lang AG</publisher-name>.</mixed-citation></ref><ref id="RSOS230964C46"><label>46<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steen</surname>
<given-names>FF</given-names></string-name>
<etal>et al.</etal></person-group>
<year>2018</year>
<article-title>Toward an infrastructure for data-driven multimodal communication research</article-title>. <source>Linguistics Vanguard</source>
<volume><bold>4</bold></volume>. (<pub-id pub-id-type="doi">10.1515/lingvan-2017-0041</pub-id>)</mixed-citation></ref><ref id="RSOS230964C47"><label>47<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moher</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Liberati</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Tetzlaff</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Altman</surname>
<given-names>DG</given-names></string-name></person-group>. <year>2009</year>
<article-title>Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement</article-title>. <source>Ann. Intern. Med.</source>
<volume><bold>151</bold></volume>, <fpage>264</fpage>-<lpage>269</lpage>. (<pub-id pub-id-type="doi">10.7326/0003-4819-151-4-200908180-00135</pub-id>)<pub-id pub-id-type="pmid">19622511</pub-id>
</mixed-citation></ref><ref id="RSOS230964C48"><label>48<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Jin</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Cao</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Guo</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Luo</surname>
<given-names>J</given-names></string-name></person-group>. <year>2017</year>
<comment>Multimodal fusion with recurrent neural networks for rumor detection on microblogs. In <italic toggle="yes">MM 2017 - Proc. of the 2017 ACM Multimedia Conf.</italic>, Mountain View, CA, 19 October 2017, pp. 795&#x02013;816</comment>. New York, NY: ACM.</mixed-citation></ref><ref id="RSOS230964C49"><label>49<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Boididou</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Andreadou</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Papadopoulos</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Dang-Nguyen</surname>
<given-names>DT</given-names></string-name>, <string-name><surname>Boato</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Riegler</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Kompatsiaris</surname>
<given-names>Y</given-names></string-name></person-group>. <year>2015</year>
<comment>Verifying multimedia use at MediaEval 2015. In <italic toggle="yes">CEUR Workshop Proc.: MediaEval Benchmarking Initiative for Multimedia Evaluation</italic>, Wurzen, Germany, 14-15 September 2015, vol. 1436. Aachen, Germany: CEUR-WS.</comment></mixed-citation></ref><ref id="RSOS230964C50"><label>50<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kalkina</surname>
<given-names>V</given-names></string-name></person-group>. <year>2020</year>
<article-title>Between humour and public commentary: digital re-appropriation of the soviet propaganda posters as internet memes</article-title>. <source>J. Creat. Commun.</source>
<volume><bold>15</bold></volume>, <fpage>131</fpage>-<lpage>146</lpage>. (<pub-id pub-id-type="doi">10.1177/0973258619893780</pub-id>)</mixed-citation></ref><ref id="RSOS230964C51"><label>51<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Resende</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Melo</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Sousa</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Messias</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Vasconcelos</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Almeida</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Benevenuto</surname>
<given-names>F</given-names></string-name></person-group>. <year>2019</year>
<comment>(Mis)Information dissemination in WhatsApp: gathering, analyzing and countermeasures. In <italic toggle="yes">The World Wide Web Conference</italic>, San Francisco, CA, 13-19 May 2019, pp. 818&#x02013;828. New York, NY: ACM.</comment></mixed-citation></ref><ref id="RSOS230964C52"><label>52<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hameleers</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Powell</surname>
<given-names>TE</given-names></string-name>, <string-name><surname>Van Der Meer</surname>
<given-names>TG</given-names></string-name>, <string-name><surname>Bos</surname>
<given-names>L</given-names></string-name></person-group>. <year>2020</year>
<article-title>A picture paints a thousand lies? The effects and mechanisms of multimodal disinformation and rebuttals disseminated via social media</article-title>. <source>Pol. Commun.</source>
<volume><bold>37</bold></volume>, <fpage>281</fpage>-<lpage>301</lpage>. (<pub-id pub-id-type="doi">10.1080/10584609.2019.1674979</pub-id>)</mixed-citation></ref><ref id="RSOS230964C53"><label>53<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krafft</surname>
<given-names>PM</given-names></string-name>, <string-name><surname>Donovan</surname>
<given-names>J</given-names></string-name></person-group>. <year>2020</year>
<article-title>Disinformation by design: the use of evidence collages and platform filtering in a media manipulation campaign</article-title>. <source>Political Commun.</source>
<volume><bold>37</bold></volume>, <fpage>194</fpage>-<lpage>214</lpage>. (<pub-id pub-id-type="doi">10.1080/10584609.2019.1686094</pub-id>)</mixed-citation></ref><ref id="RSOS230964C54"><label>54<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scardigno</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Mininni</surname>
<given-names>G</given-names></string-name></person-group>. <year>2020</year>
<article-title>The rhetoric side of fake news: a new weapon for anti-politics?</article-title>
<source>World Futures</source>
<volume><bold>76</bold></volume>, <fpage>81</fpage>-<lpage>101</lpage>. (<pub-id pub-id-type="doi">10.1080/02604027.2019.1703158</pub-id>)</mixed-citation></ref><ref id="RSOS230964C55"><label>55<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jindal</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Sood</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Singh</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Vatsa</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Chakraborty</surname>
<given-names>T</given-names></string-name></person-group>. <year>2020</year>
<article-title>NewsBag: a multimodal benchmark dataset for fake news detection</article-title>. <source>CEUR Workshop Proc.</source>
<volume><bold>2560</bold></volume>, <fpage>138</fpage>-<lpage>145</lpage>.</mixed-citation></ref><ref id="RSOS230964C56"><label>56<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schaewitz</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Kluck</surname>
<given-names>JP</given-names></string-name>, <string-name><surname>Kl&#x000f6;sters</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Kr&#x000e4;mer</surname>
<given-names>NC</given-names></string-name></person-group>. <year>2020</year>
<article-title>When is disinformation (in)credible? Experimental findings on message characteristics and individual differences</article-title>. <source>Mass Commun. Soc.</source>
<volume><bold>5436</bold></volume>, <fpage>484</fpage>-<lpage>509</lpage>. (<pub-id pub-id-type="doi">10.1080/15205436.2020.1716983</pub-id>)</mixed-citation></ref><ref id="RSOS230964C57"><label>57<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vishwakarma</surname>
<given-names>DK</given-names></string-name>, <string-name><surname>Varshney</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Yadav</surname>
<given-names>A</given-names></string-name></person-group>. <year>2019</year>
<article-title>Detection and veracity analysis of fake news via scrapping and authenticating the web search</article-title>. <source>Cogn. Syst. Res.</source>
<volume><bold>58</bold></volume>, <fpage>217</fpage>-<lpage>229</lpage>. (<pub-id pub-id-type="doi">10.1016/j.cogsys.2019.07.004</pub-id>)</mixed-citation></ref><ref id="RSOS230964C58"><label>58<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Qi</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Cao</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Yang</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Guo</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Li</surname>
<given-names>J</given-names></string-name></person-group>. <year>2019</year>
<comment>Exploiting multi-domain visual information for fake news detection. In <italic toggle="yes">Proc. of IEEE Int. Conf. on Data Mining, ICDM</italic>, Beijing, 8-11 November 2019</comment>, pp. 518&#x02013;527. New York, NY: IEEE. (<pub-id pub-id-type="doi">10.1109/ICDM.2019.00062</pub-id>)</mixed-citation></ref><ref id="RSOS230964C59"><label>59<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Singhal</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Shah</surname>
<given-names>RR</given-names></string-name>, <string-name><surname>Chakraborty</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Kumaraguru</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Satoh</surname>
<given-names>S</given-names></string-name></person-group>. <year>2019</year>
<comment>SpotFake: a multi-modal framework for fake news detection. In <italic toggle="yes">2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM)</italic>, Singapore, 11-13 September 2019, pp. 39&#x02013;47. New York, NY: IEEE. (</comment><pub-id pub-id-type="doi">10.1109/BigMM.2019.00-44</pub-id><comment>)</comment></mixed-citation></ref><ref id="RSOS230964C60"><label>60<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Cui</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Lee</surname>
<given-names>D</given-names></string-name></person-group>. <year>2019</year>
<comment>SAME: sentiment-aware multi-modal embedding for detecting fake news. In <italic toggle="yes">Proc. of the 2019 IEEE/ACM Int. Conf. on Advances in Social Networks Analysis and Mining</italic>, Vancouver, BC, 27-30 August 2019, pp. 41&#x02013;46</comment>. New York, NY: ACM. (<pub-id pub-id-type="doi">10.1145/3341161.3342894</pub-id>)</mixed-citation></ref><ref id="RSOS230964C61"><label>61<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Huang</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Zhu</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Mustafaraj</surname>
<given-names>E</given-names></string-name></person-group>. <year>2019</year>
<comment>How dependable are &#x02018;first impressions&#x02019; to distinguish between real and fake news websites? In <italic toggle="yes">HT 2019 - Proc. of the 30th ACM Conference on Hypertext and Social Media</italic>, Berlin, Germany, 12-16 September 2019, pp. 201&#x02013;210</comment>. New York, NY: ACM. (<pub-id pub-id-type="doi">10.1145/3342220.3343670</pub-id>)</mixed-citation></ref><ref id="RSOS230964C62"><label>62<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>
<given-names>CA</given-names></string-name></person-group>. <year>2019</year>
<article-title>Weaponized iconoclasm in internet memes featuring the expression &#x02018;Fake News&#x02019;</article-title>. <source>Discourse Commun.</source>
<volume><bold>13</bold></volume>, <fpage>303</fpage>-<lpage>319</lpage>. (<pub-id pub-id-type="doi">10.1177/1750481319835639</pub-id>)</mixed-citation></ref><ref id="RSOS230964C63"><label>63<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Jaiswal</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Wu</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Abdalmageed</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Masi</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Natarajan</surname>
<given-names>P</given-names></string-name></person-group>. <year>2019</year>
<comment>AIRD: adversarial learning framework for image repurposing detection. In <italic toggle="yes">Proc. of the IEEE Computer Society Conf. on Computer Vision and Pattern Recognition</italic>, Long Beach, CA, 15-20 June 2019, pp. 11322&#x02013;11331</comment>. New York, NY: IEEE.</mixed-citation></ref><ref id="RSOS230964C64"><label>64<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Khattar</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Gupta</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Goud</surname>
<given-names>JS</given-names></string-name>, <string-name><surname>Varma</surname>
<given-names>V</given-names></string-name></person-group>. <year>2019</year>
<comment>MVAE: multimodal variational autoencoder for fake news detection. In <italic toggle="yes">The Web Conf. 2019 - Proc. of the World Wide Web Conf.e, WWW May 2019</italic>, San Francisco, CA, 13-16 May 2019, pp. 2915&#x02013;2921</comment>. New York, NY: ACM. (<pub-id pub-id-type="doi">10.1145/3308558.3313552</pub-id>)</mixed-citation></ref><ref id="RSOS230964C65"><label>65<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>O&#x02019;Halloran</surname>
<given-names>KL</given-names></string-name>, <string-name><surname>Tan</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Wignell</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Bateman</surname>
<given-names>JA</given-names></string-name>, <string-name><surname>Pham</surname>
<given-names>DS</given-names></string-name>, <string-name><surname>Grossman</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Moere</surname>
<given-names>AV</given-names></string-name></person-group>. <year>2019</year>
<article-title>Interpreting text and image relations in violent extremist discourse: a mixed methods approach for big data analytics</article-title>. <source>Terror. Political. Violence</source>
<volume><bold>31</bold></volume>, <fpage>454</fpage>-<lpage>474</lpage>. (<pub-id pub-id-type="doi">10.1080/09546553.2016.1233871</pub-id>)</mixed-citation></ref><ref id="RSOS230964C66"><label>66<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Graham</surname>
<given-names>AP</given-names></string-name></person-group>. <year>2019</year>
<article-title>Hostile visual encounters: fighting to control photographic meaning in the DRC&#x02019;s digital age</article-title>. <source>Africa</source>
<volume><bold>89</bold></volume>, <fpage>266</fpage>-<lpage>285</lpage>. (<pub-id pub-id-type="doi">10.1017/S0001972019000056</pub-id>)</mixed-citation></ref><ref id="RSOS230964C67"><label>67<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nee</surname>
<given-names>RC</given-names></string-name>, <string-name><surname>De Maio</surname>
<given-names>M</given-names></string-name></person-group>. <year>2019</year>
<article-title>A &#x02018;Presidential Look&#x02019;? An analysis of gender framing in 2016 persuasive memes of hillary clinton</article-title>. <source>J. Broadcast. Electron. Media</source>
<volume><bold>63</bold></volume>, <fpage>304</fpage>-<lpage>321</lpage>. (<pub-id pub-id-type="doi">10.1080/08838151.2019.1620561</pub-id>)</mixed-citation></ref><ref id="RSOS230964C68"><label>68<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhukova</surname>
<given-names>E</given-names></string-name></person-group>. <year>2019</year>
<article-title>Image substitutes and visual fake history: historical images of atrocity of the Ukranian Famine 1932&#x02013;1933 on social media</article-title>. <source>Visual Commun.</source>
<volume><bold>21</bold></volume>, <fpage>3</fpage>-<lpage>27</lpage>. (<pub-id pub-id-type="doi">10.1177/1470357219888673</pub-id>)</mixed-citation></ref><ref id="RSOS230964C69"><label>69<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Volkova</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Ayton</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Arendt</surname>
<given-names>DL</given-names></string-name>, <string-name><surname>Huang</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Hutchinson</surname>
<given-names>B</given-names></string-name></person-group>. <year>2019</year>
<comment>Explaining multimodal deceptive news prediction models. In <italic toggle="yes">Proc. of the 13th Int. AAAI Conf. on Web and Social Media,</italic> M&#x000fc;nich, Germany, 11-14 June 2019, pp. 659&#x02013;662</comment>. Washington, DC: AAAI. (<pub-id pub-id-type="doi">10.1609/icwsm.v13i01.3266</pub-id>)</mixed-citation></ref><ref id="RSOS230964C70"><label>70<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Angiani</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Lombardo</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Balba</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Mordonini</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Fornacciari</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Tomaiuolo</surname>
<given-names>M</given-names></string-name></person-group>. <year>2018</year>
<comment>Image-based hoax detection. In</comment>
<italic toggle="yes">Goodtechs '18: Proceedings of the 4th EAI International Conference on Smart Objects and Technologies for Social Good</italic>, Bologna, Italy, 28-30 November 2018, pp 159&#x02013;164. New York, NY: ACM. (<pub-id pub-id-type="doi">10.1145/3284869.3284903</pub-id>)</mixed-citation></ref><ref id="RSOS230964C71"><label>71<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brookes</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Harvey</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Chadborn</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Dening</surname>
<given-names>T</given-names></string-name></person-group>. <year>2018</year>
<article-title>&#x02018;Our Biggest Killer&#x02019;: multimodal discourse representations of dementia in the british press</article-title>. <source>Soc. Semiot.</source>
<volume><bold>28</bold></volume>, <fpage>371</fpage>-<lpage>395</lpage>. (<pub-id pub-id-type="doi">10.1080/10350330.2017.1345111</pub-id>)</mixed-citation></ref><ref id="RSOS230964C72"><label>72<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Sabir</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Wu</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Almageed</surname>
<given-names>WA</given-names></string-name>, <string-name><surname>Natarajan</surname>
<given-names>P</given-names></string-name></person-group>. <year>2018</year> Deep Multimodal Image-Repurposing Detection. In <italic toggle="yes">Proceedings of the 26th ACM international conference on Multimedia (MM '18)</italic>. Seoul, 22-26 October 2018, pp. 1337&#x02013;1345. New York, NY: ACM. (<pub-id pub-id-type="doi">10.1145/3240508.3240707</pub-id>)</mixed-citation></ref><ref id="RSOS230964C73"><label>73<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DeCook</surname>
<given-names>JR</given-names></string-name></person-group>. <year>2018</year>
<article-title>Memes and symbolic violence: #proudboys and the use of memes for propaganda and the construction of collective identity</article-title>. <source>Learn. Media Technol.</source>
<volume><bold>43</bold></volume>, <fpage>485</fpage>-<lpage>504</lpage>. (<pub-id pub-id-type="doi">10.1080/17439884.2018.1544149</pub-id>)</mixed-citation></ref><ref id="RSOS230964C74"><label>74<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Medina</surname>
<given-names>J</given-names></string-name></person-group>. <year>2018</year>
<article-title>Resisting racist propaganda: distorted visual communication and epistemic activism</article-title>. <source>South. J. Philos.</source>
<volume><bold>56</bold></volume>, <fpage>50</fpage>-<lpage>75</lpage>. (<pub-id pub-id-type="doi">10.1111/sjp.12301</pub-id>)</mixed-citation></ref><ref id="RSOS230964C75"><label>75<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Knshnan</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Chen</surname>
<given-names>M</given-names></string-name></person-group>. <year>2018</year>
<comment>Identifying tweets with fake news. In <italic toggle="yes">Proc. 2018 IEEE 19th Int. Conf. on Information Reuse and Integration for Data Science, IRI,</italic> vol. 67, Salt Lake City, UT, 6 July 2018, pp. 460&#x02013;464. New York, NY: IEEE</comment>. (<pub-id pub-id-type="doi">10.1109/IRI.2018.00073</pub-id>)</mixed-citation></ref><ref id="RSOS230964C76"><label>76<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Wang</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Ma</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Jin</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Yuan</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Xun</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Jha</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Su</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Gao</surname>
<given-names>J</given-names></string-name></person-group>. <year>2018</year>
<comment>EANN: event adversarial neural networks for multi-modal fake news detection. In <italic toggle="yes">Proc. of the 24th ACM SIGKDD Int. Conf. on Knowledge Discovery &#x00026; Data Mining</italic>, London, UK, 19-23 August 2018, pp. 849&#x02013;857. New York, NY: ACM</comment>. (<pub-id pub-id-type="doi">10.1145/3219819.3219903</pub-id>)</mixed-citation></ref><ref id="RSOS230964C77"><label>77<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Kasra</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Shen</surname>
<given-names>C</given-names></string-name>, <string-name><surname>O&#x02019;Brien</surname>
<given-names>JF</given-names></string-name></person-group>. <year>2018</year>
<comment>Seeing is believing: how people fail to identify fake images on the web.</comment> In <italic toggle="yes">Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems</italic>, Paper No.: LBW516. Montreal, Canada, 21-27 April 2018, pp. 1&#x02013;6. New York, NY: ACM. (<pub-id pub-id-type="doi">10.1145/3170427.3188604</pub-id>)</mixed-citation></ref><ref id="RSOS230964C78"><label>78<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tan</surname>
<given-names>S</given-names></string-name>, <string-name><surname>O&#x02019;Halloran</surname>
<given-names>KL</given-names></string-name>, <string-name><surname>Wignell</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Chai</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Lange</surname>
<given-names>R</given-names></string-name></person-group>. <year>2018</year>
<article-title>A multimodal mixed methods approach for examining recontextualisation patterns of violent extremist images in online media</article-title>. <source>Discourse Context Media</source>
<volume><bold>21</bold></volume>, <fpage>18</fpage>-<lpage>35</lpage>. (<pub-id pub-id-type="doi">10.1016/j.dcm.2017.11.004</pub-id>)</mixed-citation></ref><ref id="RSOS230964C79"><label>79<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Agarwal</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Sehwag</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Singh</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Vatsa</surname>
<given-names>M</given-names></string-name></person-group>. <year>2019</year>
<comment>Deceiving face presentation attack detection via image transforms. In <italic toggle="yes">Proc. - 2019 IEEE 5th Int. Conf. on Multimedia Big Data, 5 December 2019</italic>, Singapore, 11-13 September 2019, pp. 373&#x02013;382</comment>. New York, NY: IEEE. (<pub-id pub-id-type="doi">10.1109/BigMM.2019.00018</pub-id>)</mixed-citation></ref><ref id="RSOS230964C80"><label>80<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Dewan</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Suri</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Bharadhwaj</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Mithal</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Kumaraguru</surname>
<given-names>P</given-names></string-name></person-group>. <year>2017</year>
<comment>Towards understanding crisis events on online social networks through pictures. In <italic toggle="yes">Proc. of the 2017 IEEE/ACM Int. Conf. on Advances in Social Networks Analysis and Mining,</italic> Sydney, Australia 31 July - 3 August 2017, pp. 439&#x02013;446</comment>. New York, NY: IEEE. (<pub-id pub-id-type="doi">10.1145/3110025.3110062</pub-id>)</mixed-citation></ref><ref id="RSOS230964C81"><label>81<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amiri</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Hashemi</surname>
<given-names>MR</given-names></string-name>, <string-name><surname>Rezaei</surname>
<given-names>J</given-names></string-name></person-group>. <year>2015</year>
<article-title>The representation of islamophobia: a critical discourse analysis of yahoo news</article-title>. <source>Int. J. Control Theory Appl.</source>
<volume><bold>8</bold></volume>, <fpage>599</fpage>-<lpage>618</lpage>. (<pub-id pub-id-type="doi">10.17485/ijst/2015/v8i28/87385</pub-id>)</mixed-citation></ref><ref id="RSOS230964C82"><label>82<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seo</surname>
<given-names>H</given-names></string-name></person-group>. <year>2014</year>
<article-title>Visual propaganda in the age of social media: an empirical analysis of twitter images during the 2012 Israeli&#x02013;Hamas conflict</article-title>. <source>Vis. Commun. Q.</source>
<volume><bold>21</bold></volume>, <fpage>150</fpage>-<lpage>161</lpage>. (<pub-id pub-id-type="doi">10.1080/15551393.2014.955501</pub-id>)</mixed-citation></ref><ref id="RSOS230964C83"><label>83<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mina</surname>
<given-names>AX</given-names></string-name></person-group>. <year>2014</year>
<article-title>Batman, pandaman and the blind man: a case study in social change memes and internet censorship in China</article-title>. <source>J. Vis. Cult.</source>
<volume><bold>13</bold></volume>, <fpage>359</fpage>-<lpage>375</lpage>. (<pub-id pub-id-type="doi">10.1177/1470412914546576</pub-id>)</mixed-citation></ref><ref id="RSOS230964C84"><label>84<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Gupta</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Lamba</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Kumaraguru</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Joshi</surname>
<given-names>A</given-names></string-name></person-group>. <year>2013</year>
<comment>Faking sandy: characterizing and identifying fake images on Twitter during Hurricane Sandy. In <italic toggle="yes">WWW 2013 Companion: Proc. of the 22nd Int. Conf. on World Wide Web</italic>, Rio de Janeiro, 13-17 May 2013, pp. 729&#x02013;736.</comment> New York, NY: ACM. (<pub-id pub-id-type="doi">10.1145/2487788.2488033</pub-id>)</mixed-citation></ref><ref id="RSOS230964C85"><label>85<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brayshay</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Selwood</surname>
<given-names>J</given-names></string-name></person-group>. <year>2002</year>
<article-title>Dreams, propaganda and harsh realities: landscapes of group settlement in the forest districts of Western Australia in the 1920s</article-title>. <source>Landsc. Res.</source>
<volume><bold>27</bold></volume>, <fpage>81</fpage>-<lpage>101</lpage>. (<pub-id pub-id-type="doi">10.1080/01426390220110784</pub-id>)</mixed-citation></ref><ref id="RSOS230964C86"><label>86<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deaville</surname>
<given-names>J</given-names></string-name></person-group>. <year>2019</year>
<article-title>Pitched battles: music and sound in anglo-American and german newsreels of world war II</article-title>. <source>J. Musicol. Res.</source>
<volume><bold>38</bold></volume>, <fpage>32</fpage>-<lpage>43</lpage>. (<pub-id pub-id-type="doi">10.1080/01411896.2019.1568153</pub-id>)</mixed-citation></ref><ref id="RSOS230964C87"><label>87<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khaldarova</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Pantti</surname>
<given-names>M</given-names></string-name></person-group>. <year>2016</year>
<article-title>Fake news: the narrative battle over the Ukranian conflict</article-title>. <source>Journal. Pract.</source>
<volume><bold>10</bold></volume>, <fpage>891</fpage>-<lpage>901</lpage>. (<pub-id pub-id-type="doi">10.1080/17512786.2016.1163237</pub-id>)</mixed-citation></ref><ref id="RSOS230964C88"><label>88<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Hou</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Loeb</surname>
<given-names>S</given-names></string-name>, <string-name><surname>P&#x000e9;rez-Rosas</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Mihalcea</surname>
<given-names>R</given-names></string-name></person-group>. <year>2020</year>
<comment>Towards automatic detection of misinformation in online medical videos. In <italic toggle="yes">ICMI 2019 - Proc. of the 2019 Int. Conf. on Multimodal Interaction</italic>, Utrecht, The Netherlands, 25-29 October 2020, pp. 235&#x02013;243. New York, NY: ACM</comment>. (<pub-id pub-id-type="doi">10.1145/3340555.3353763</pub-id>)</mixed-citation></ref><ref id="RSOS230964C89"><label>89<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mackay</surname>
<given-names>RR</given-names></string-name></person-group>. <year>2015</year>
<article-title>Multimodal legitimation: selling scottish independence</article-title>. <source>Discourse Soc.</source>
<volume><bold>26</bold></volume>, <fpage>323</fpage>-<lpage>348</lpage>. (<pub-id pub-id-type="doi">10.1177/0957926514564737</pub-id>)</mixed-citation></ref><ref id="RSOS230964C90"><label>90<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meddaugh</surname>
<given-names>PM</given-names></string-name></person-group>. <year>2010</year>
<article-title>Bakhtin, colbert, and the center of discourse: is there no &#x02018;Truthiness&#x02019; in Humor?</article-title>
<source>Crit. Stud. Media Commun.</source>
<volume><bold>27</bold></volume>, <fpage>376</fpage>-<lpage>390</lpage>. (<pub-id pub-id-type="doi">10.1080/15295030903583606</pub-id>)</mixed-citation></ref><ref id="RSOS230964C91"><label>91<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Kang</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Hwang</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Yu</surname>
<given-names>H</given-names></string-name></person-group>. <year>2020</year>
<comment>Multi-modal component embedding for fake news detection. In <italic toggle="yes">Proc. of the 2020 14th Int. Conf. on Ubiquitous Information Management and Communication, IMCOM 2020</italic>, Taichung, Taiwan, 3-5 January 2020, pp. 1&#x02013;6. New York, NY: IEEE. (</comment><pub-id pub-id-type="doi">10.1109/IMCOM48794.2020.9001800</pub-id><comment>)</comment></mixed-citation></ref><ref id="RSOS230964C92"><label>92<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meikle</surname>
<given-names>G</given-names></string-name></person-group>. <year>2012</year>
<article-title>&#x02018;Find Out Exactly What to Think&#x02013;Next!&#x02019;: Chris Morris, Brass Eye, and journalistic authority</article-title>. <source>Popular Commun.</source>
<volume><bold>10</bold></volume>, <fpage>14</fpage>-<lpage>26</lpage>. (<pub-id pub-id-type="doi">10.1080/15405702.2012.638569</pub-id>)</mixed-citation></ref><ref id="RSOS230964C93"><label>93<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bradshaw</surname>
<given-names>AS</given-names></string-name>, <string-name><surname>Treise</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Shelton</surname>
<given-names>SS</given-names></string-name>, <string-name><surname>Cretul</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Raisa</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Bajalia</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Peek</surname>
<given-names>D</given-names></string-name></person-group>. <year>2020</year>
<article-title>Propagandizing anti-vaccination: analysis of vaccines revealed documentary series</article-title>. <source>Vaccine</source>
<volume><bold>38</bold></volume>, <fpage>2058</fpage>-<lpage>2069</lpage>. (<pub-id pub-id-type="doi">10.1016/j.vaccine.2019.12.027</pub-id>)<pub-id pub-id-type="pmid">31980194</pub-id>
</mixed-citation></ref><ref id="RSOS230964C94"><label>94<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Monte</surname>
<given-names>EP</given-names></string-name></person-group>. <year>2017</year>
<article-title>Romancing the nation, effacing history: reading Kenya through patriotic choral music</article-title>. <source>Soc. Dyn.</source>
<volume><bold>43</bold></volume>, <fpage>451</fpage>-<lpage>469</lpage>. (<pub-id pub-id-type="doi">10.1080/02533952.2017.1394648</pub-id>)</mixed-citation></ref><ref id="RSOS230964C95"><label>95<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Bagade</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Pale</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Sheth</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Agarwal</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Chakrabarti</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Chebrolu</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Sudarshan</surname>
<given-names>S</given-names></string-name></person-group>. <year>2020</year>
<comment>The Kauwa-Kaate fake news detection system: demo. In <italic toggle="yes">Proceedings of the 7th ACM IKDD CoDS and 25th COMAD (CoDS COMAD 2020),</italic> Hyderabad, India, 5-7 January 2020<italic toggle="yes">,</italic> pp. 302&#x02013;306</comment>. New York, NY: ACM. (<pub-id pub-id-type="doi">10.1145/3371158.3371402</pub-id>)</mixed-citation></ref><ref id="RSOS230964C96"><label>96<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Machado</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Kira</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Narayanan</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Kollanyi</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Howard</surname>
<given-names>PN</given-names></string-name></person-group>. <year>2019</year>
<comment>A study of misinformation in WhatsApp groups with a focus on the Brazilian Presidential Elections.</comment> In <italic toggle="yes">Companion Proceedings of The 2019 World Wide Web Conference (WWW '19),</italic> San Francisco, CA, 13-17 May 2019, pp. 1013&#x02013;1019. New York, NY: ACM. (<pub-id pub-id-type="doi">10.1145/3308560.3316738</pub-id>)</mixed-citation></ref><ref id="RSOS230964C97"><label>97<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Seo</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Ebrahim</surname>
<given-names>H</given-names></string-name></person-group>. <year>2016</year>
<article-title>Visual propaganda on facebook: a comparative analysis of syrian conflicts</article-title>. <source>Media War Confl.</source>
<volume><bold>9</bold></volume>, <fpage>227</fpage>-<lpage>251</lpage>. (<pub-id pub-id-type="doi">10.1177/1750635216661648</pub-id>)</mixed-citation></ref><ref id="RSOS230964C98"><label>98<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><comment>Scimago Journal &#x00026; Country Rank. See <uri xlink:href="https://www.scimagojr.com/countryrank.php?order=itpord=descyear=2020">https://www.scimagojr.com/countryrank.php?order=itpord=descyear=2020</uri> (Last accessed 18 January 2023)</comment>.</mixed-citation></ref><ref id="RSOS230964C99"><label>99<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Chen</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Li</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Sui</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Lv</surname>
<given-names>Q</given-names></string-name>, <string-name><surname>Tun</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Shang</surname>
<given-names>L</given-names></string-name></person-group>. <year>2022</year>
<comment>Cross-modal ambiguity learning for multimodal fake news detection. In <italic toggle="yes">Proc. of the ACM Web Conf. 2022</italic>, Lyon, France 25-28 April 2022, pp. 2897&#x02013;2905. New York, NY: ACM.</comment> (<pub-id pub-id-type="doi">10.1145/3485447.3511968</pub-id>)</mixed-citation></ref><ref id="RSOS230964C100"><label>100<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Song</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Ning</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Wu</surname>
<given-names>B</given-names></string-name></person-group>. <year>2021</year>
<article-title>A multimodal fake news detection model based on crossmodal attention residual and multichannel convolutional neural networks</article-title>. <source>Inf. Process. Manag.</source>
<volume><bold>58</bold></volume>, <fpage>102437</fpage>. (<pub-id pub-id-type="doi">10.1016/j.ipm.2020.102437</pub-id>)</mixed-citation></ref><ref id="RSOS230964C101"><label>101<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Torralba</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Efros</surname>
<given-names>AA</given-names></string-name></person-group>. <year>2011</year>
<comment>Unbiased look at dataset bias. In <italic toggle="yes">CVPR 2011</italic>, Colorado Springs, CO, 22 August 2011, pp. 1521&#x02013;1528. New York, NY: IEEE</comment>. (<pub-id pub-id-type="doi">10.1109/CVPR.2011.5995347</pub-id>)</mixed-citation></ref><ref id="RSOS230964C102"><label>102<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Song</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Ning</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Wu</surname>
<given-names>B</given-names></string-name></person-group>. <year>2021</year>
<article-title>Knowledge augmented transformer for adversarial multidomain multiclassification multimodal fake news detection</article-title>. <source>Neurocomputing</source>
<volume><bold>462</bold></volume>, <fpage>88</fpage>-<lpage>100</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neucom.2021.07.077</pub-id>)</mixed-citation></ref><ref id="RSOS230964C103"><label>103<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Wu</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Sun</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Zhou</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Jing</surname>
<given-names>XY</given-names></string-name></person-group>. <year>2022</year>
<article-title>Modality and event adversarial networks for multi-modal fake news detection</article-title>. <source>IEEE Signal Process Lett.</source>
<volume><bold>29</bold></volume>, <fpage>1382</fpage>-<lpage>1386</lpage>. (<pub-id pub-id-type="doi">10.1109/LSP.2022.3181893</pub-id>)</mixed-citation></ref><ref id="RSOS230964C104"><label>104<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Zhang</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Chen</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Zeng</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Guo</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Miao</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Cui</surname>
<given-names>L</given-names></string-name></person-group>. <year>2020</year>
<comment>BDANN: BERT-based domain adaptation neural network for multi-modal fake news detection. In <italic toggle="yes">2020 Int. Joint Conf. on Neural Networks</italic> (<italic toggle="yes">IJCNN</italic>), Glasgow, UK, 25-27 September 2020, pp. 1&#x02013;8. New York, NY: IEEE</comment>. (<pub-id pub-id-type="doi">10.1109/IJCNN48605.2020.9206973</pub-id>)</mixed-citation></ref><ref id="RSOS230964C105"><label>105<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Wu</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Zhan</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Xu</surname>
<given-names>Z</given-names></string-name></person-group>. <year>2021</year>
<comment>Multimodal fusion with co-attention networks for fake news detection. In <italic toggle="yes">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</italic>, Online, 1-6 August 2021, pp. 2560&#x02013;2569. Stroudsburg, PA: Association for Computational Linguistics</comment>.</mixed-citation></ref><ref id="RSOS230964C106"><label>106<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Ferreira</surname>
<given-names>VC</given-names></string-name>, <string-name><surname>Kundu</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Franca</surname>
<given-names>FMG</given-names></string-name></person-group>. <year>2022</year>
<comment>Analysis of fake news classification for insight into the roles of different data types. In <italic toggle="yes">2022 IEEE 16th Int. Conf. on Semantic Computing</italic> (<italic toggle="yes">ICSC</italic>), Laguna Hills, CA, 26 January 2022, pp. 75&#x02013;82. New York, NY: IEEE</comment>. (<pub-id pub-id-type="doi">10.1109/ICSC52841.2022.00018</pub-id>)</mixed-citation></ref><ref id="RSOS230964C107"><label>107<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Singhal</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Dhawan</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Shah</surname>
<given-names>RR</given-names></string-name>, <string-name><surname>Kumaraguru</surname>
<given-names>P</given-names></string-name></person-group>. <year>2021</year>
<comment>Inter-modality discordance for multimodal fake news detection. In <italic toggle="yes">ACM Multimedia Asia</italic> (MMAsia '21), Gold Coast, Australia, 1-3 December 2021, pp. 1&#x02013;7. New York, NY: Association for Computing Machinery. (</comment><pub-id pub-id-type="doi">10.1145/3469877.3490614</pub-id><comment>)</comment></mixed-citation></ref><ref id="RSOS230964C108"><label>108<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Shang</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Kou</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>D</given-names></string-name></person-group>. <year>2022</year>
<comment>A duo-generative approach to explainable multimodal COVID-19 misinformation detection. In <italic toggle="yes">WWW '22: Proc. of the ACM Web Conf. 2022,</italic> Lyon, France, 25-29 April 2022, pp. 3623&#x02013;3631. New York, NY: Association for Computing Machinery</comment>. (<pub-id pub-id-type="doi">10.1145/3485447.3512257</pub-id>)</mixed-citation></ref><ref id="RSOS230964C109"><label>109<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wagle</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Kaur</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Kamat</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Patil</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Kotecha</surname>
<given-names>K</given-names></string-name></person-group>. <year>2021</year>
<article-title>Explainable AI for multimodal credibility analysis: case study of online beauty health (Mis)-information</article-title>. <source>IEEE Access</source>
<volume><bold>9</bold></volume>, <fpage>127 985</fpage>-<lpage>128 022</lpage>. (<pub-id pub-id-type="doi">10.1109/ACCESS.2021.3111527</pub-id>)</mixed-citation></ref><ref id="RSOS230964C110"><label>110<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Dimitrov</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Ali</surname>
<given-names>BB</given-names></string-name>, <string-name><surname>Shaar</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Alam</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Silvestri</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Firooz</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Nakov</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Martino</surname>
<given-names>GDS</given-names></string-name></person-group>. <year>2021</year>
<comment>Detecting propaganda techniques in memes. In <italic toggle="yes">Proc. of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</italic> (<italic toggle="yes">Volume 1: Long Papers</italic>), Bangkok, Thailand, 1-6 August 2021, pp. 6603&#x02013;6617. Stroudsburg, PA: Association for Computational Linguistics</comment>. (<uri xlink:href="https://aclanthology.org/2021.acl-long.516">https://aclanthology.org/2021.acl-long.516</uri>)</mixed-citation></ref><ref id="RSOS230964C111"><label>111<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Nakamura</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Levy</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>WY</given-names></string-name></person-group>. <year>2020</year>
<comment>Fakeddit: a new multimodal benchmark dataset for fine-grained fake news detection. In <italic toggle="yes">Proc. of The 12th Language Resources and Evaluation Conference</italic>, Marseilles, France, 20-25 March 2020, pp. 6149&#x02013;6157. Paris, France: European Language Resources Association</comment>. (<pub-id pub-id-type="doi">10.48550/arXiv.1911.03854</pub-id>)</mixed-citation></ref><ref id="RSOS230964C112"><label>112<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Pramanick</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Sharma</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Dimitrov</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Akhtar</surname>
<given-names>MS</given-names></string-name>, <string-name><surname>Nakov</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Chakraborty</surname>
<given-names>T</given-names></string-name></person-group>. <year>2021</year>
<comment>MOMENTA: a multimodal framework for detecting harmful memes and their targets. In <italic toggle="yes">Findings of the Association for Computational Linguistics: EMNLP 2021</italic>, Punta Cana, Dominican Republic, 7-11 November 2021, pp. 4439&#x02013;4455. Stroudsburg, PA: Association for Computational Linguistics</comment>. (<pub-id pub-id-type="doi">10.18653/v1/2021.findings-emnlp.379</pub-id>)</mixed-citation></ref><ref id="RSOS230964C113"><label>113<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gamir-&#x00154;&#x00131;os</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Tarullo</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Ib&#x000e1;&#x000f1;ez-Cuquerella</surname>
<given-names>M</given-names></string-name></person-group>. <year>2021</year>
<article-title>La desinformaci&#x000f3; multimodal sobre l&#x02019;alteritat a Internet. Difusi&#x000f3; de boles racistes, xen&#x000f2;fobes i islam&#x000f2;fobes el 2020 [Multimodal disinformation about otherness on the internet. The spread of racist, xenophobic and Islamophobic fake news in 2020]</article-title>. <source>An&#x000e0;lisi: Quaderns de Comunicaci&#x000f3; i Cultura</source>
<volume><bold>64</bold></volume>, <fpage>49</fpage>-<lpage>64</lpage>. (<pub-id pub-id-type="doi">10.5565/rev/analisi.3398</pub-id>)</mixed-citation></ref><ref id="RSOS230964C114"><label>114<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Gao</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Hoffmann</surname>
<given-names>HF</given-names></string-name>, <string-name><surname>Oikonomou</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Kiskovski</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Bandhakavi</surname>
<given-names>A</given-names></string-name></person-group>. <year>2022</year>
<comment>Logically at factify 2022: multimodal fact verification</comment>. In <italic toggle="yes">Proceedings of CEUR Workshop on Multimodal Fact-Checking and Hate Speech Detection</italic>, Vancouver, BC, Februrary 22 - March 1 2022, pp. 1-22. Aachen, Germany, Germany: CEUR-WS. (<pub-id pub-id-type="doi">10.48550/arXiv.2112.09253</pub-id>)</mixed-citation></ref><ref id="RSOS230964C115"><label>115<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Bhattacharjee</surname>
<given-names>S.D.</given-names></string-name>, <string-name><surname>Yuan</surname>
<given-names>J</given-names></string-name></person-group>. <year>2022</year>. Multimodal Co-training for Fake News Identification Using Attention-aware Fusion. In Pattern Recognition. ACPR 2021. Lecture Notes in Computer Science, vol 13189. (eds C Wallraven, Q Liu, H Nagahara). Cham, Switzerland: Springer Nature. <pub-id pub-id-type="doi">10.1007/978-3-031-02444-3_21</pub-id>.</mixed-citation></ref><ref id="RSOS230964C116"><label>116<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Nielsen</surname>
<given-names>DS</given-names></string-name>, <string-name><surname>McConville</surname>
<given-names>R</given-names></string-name></person-group>. <year>2022</year>
<comment>MuMiNA: large-scale multilingual multimodal fact-checked misinformation social network dataset. In <italic toggle="yes">Proc. of the 45th Int. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR '22)</italic>, Madrid, Spain, 11-15 July 2022, pp. 3141&#x02013;3153. New York, NY: Association for Computing Machinery. </comment><pub-id pub-id-type="doi">10.1145/3477495.3531744</pub-id>.</mixed-citation></ref><ref id="RSOS230964C117"><label>117<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Mehran</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Bayati</surname>
<given-names>UA</given-names></string-name>, <string-name><surname>Mottet</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Lemieux</surname>
<given-names>AF</given-names></string-name></person-group>. <year>2021</year>
<comment>Deep analysis of Taliban videos: differential use of multimodal, visual and sonic forms across strategic themes. <italic toggle="yes">Stud. Confl. Terror.</italic>
<bold>0</bold>, 1&#x02013;21</comment>. (<pub-id pub-id-type="doi">10.1080/1057610X.2020.1866739</pub-id>)</mixed-citation></ref><ref id="RSOS230964C118"><label>118<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Shin</surname>
<given-names>SY</given-names></string-name></person-group>. <year>2021</year>
<article-title>Something that they never said: multimodal disinformation and source vividness in understanding the power of AI-enabled deepfake news</article-title>. <source>Media Psychol.</source>
<volume><bold>25</bold></volume>, <fpage>531</fpage>-<lpage>546</lpage>. (<pub-id pub-id-type="doi">10.1080/15213269.2021.2007489</pub-id>)</mixed-citation></ref><ref id="RSOS230964C119"><label>119<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Choi</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Ko</surname>
<given-names>Y</given-names></string-name></person-group>. <year>2022</year>
<article-title>Effective fake news video detection using domain knowledge and multimodal data fusion on youtube</article-title>. <source>Pattern Recognit. Lett.</source>
<volume><bold>154</bold></volume>, <fpage>44</fpage>-<lpage>52</lpage>. (<pub-id pub-id-type="doi">10.1016/j.patrec.2022.01.007</pub-id>)</mixed-citation></ref><ref id="RSOS230964C120"><label>120<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Shang</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Kou</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>D</given-names></string-name></person-group>. <year>2021</year>
<comment>A multimodal misinformation detector for COVID-19 short videos on TikTok. In <italic toggle="yes">2021 IEEE Int. Conf. on Big Data</italic> (<italic toggle="yes">Big Data</italic>), Orlando, FL, 15-18 December 2021, pp. 899&#x02013;908. New York, NY: IEEE. (</comment><pub-id pub-id-type="doi">10.1109/BigData52589.2021.9671928</pub-id><comment>)</comment></mixed-citation></ref><ref id="RSOS230964C121"><label>121<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Zhou</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Mulay</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Ferrara</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Zafarani</surname>
<given-names>R</given-names></string-name></person-group>. <year>2020</year>
<comment>ReCOVery: a multimodal repository for COVID-19 news credibility research. In <italic toggle="yes">Proc. of the 29th ACM Int. Conf. on Information &#x00026; Knowledge Management</italic> (CIKM '20)<italic toggle="yes">,</italic> Online, 19-23 October 2020, pp. 3205&#x02013;3212. New York, NY: Association for Computing Machinery. (</comment><pub-id pub-id-type="doi">10.1145/3340531.3412880</pub-id><comment>)</comment></mixed-citation></ref><ref id="RSOS230964C122"><label>122<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Chen</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Chu</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Subbalakshmi</surname>
<given-names>KP</given-names></string-name></person-group>. <year>2021</year>
<comment>MMCoVaR: multimodal COVID-19 vaccine focused data repository for fake news detection and a baseline architecture for classification. In <italic toggle="yes">Proc. of the 2021 IEEE/ACM Int. Conf. on Advances in Social Networks Analysis and Mining (ASONAM '21),</italic> The Hague, Netherlands, 8-11 November 2022, pp. 31&#x02013;38. New York, NY: Association for Computing Machinery. (</comment><pub-id pub-id-type="doi">10.1145/3487351.3488346</pub-id><comment>)</comment></mixed-citation></ref><ref id="RSOS230964C123"><label>123<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elswah</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Howard</surname>
<given-names>PN</given-names></string-name></person-group>. <year>2020</year>
<article-title>&#x02018;Anything that Causes Chaos&#x02019;: the organizational behavior of Russia today (RT)</article-title>. <source>J. Commun.</source>
<volume><bold>70</bold></volume>, <fpage>623</fpage>-<lpage>645</lpage>. (<pub-id pub-id-type="doi">10.1093/joc/jqaa027</pub-id>)</mixed-citation></ref><ref id="RSOS230964C124"><label>124<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Li</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Jiang</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Shu</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Liu</surname>
<given-names>H</given-names></string-name></person-group>. <year>2020</year>
<comment>Toward a multilingual and multimodal data repository for COVID-19 disinformation. In <italic toggle="yes">2020 IEEE Int. Conf. on Big Data</italic> (<italic toggle="yes">Big Data</italic>), Atlanta, GA, 10 December 2020, pp. 4325&#x02013;4330. New York, NY: IEEE</comment>. (<pub-id pub-id-type="doi">10.1109/BigData50022.2020.9378472</pub-id>)</mixed-citation></ref><ref id="RSOS230964C125"><label>125<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sharma</surname>
<given-names>DK</given-names></string-name>, <string-name><surname>Garg</surname>
<given-names>S</given-names></string-name></person-group>. <year>2021</year>
<article-title>IFND: a benchmark dataset for fake news detection</article-title>. <source><italic toggle="yes">Complex Intell. Syst.</italic></source>
<volume><bold>9</bold></volume>, <fpage>2843</fpage>-<lpage>2863</lpage>.</mixed-citation></ref><ref id="RSOS230964C126"><label>126<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Luo</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Darrell</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Rohrbach</surname>
<given-names>A</given-names></string-name></person-group>. <year>2021</year>
<comment>NewsCLIPpings: automatic generation of out-of-context multimodal media. In <italic toggle="yes">Proc. of the 2021 Conf. on Empirical Methods in Natural Language Processing</italic>, Punta Cana, Dominican Republic, 8-11 November 2021, pp. 6801&#x02013;6817. Stroudsburg, PA: Association for Computational Linguistics.</comment> (<pub-id pub-id-type="doi">10.18653/v1/2021.emnlp-main.545</pub-id>)</mixed-citation></ref><ref id="RSOS230964C127"><label>127<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Singh</surname>
<given-names>VK</given-names></string-name>, <string-name><surname>Ghosh</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Sonagara</surname>
<given-names>D</given-names></string-name></person-group>. <year>2020</year>
<article-title>Detecting fake news stories via multimodal analysis</article-title>. <source>J. Assoc. Inform. Sci. Technol.</source>
<volume><bold>72</bold></volume>, <fpage>3</fpage>-<lpage>17</lpage>. (<pub-id pub-id-type="doi">10.1002/asi.24359</pub-id>)</mixed-citation></ref><ref id="RSOS230964C128"><label>128<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Yin</surname>
<given-names>Z</given-names></string-name>, <string-name><surname>Argyris</surname>
<given-names>YA</given-names></string-name></person-group>. <year>2021</year>
<article-title>Detecting medical misinformation on social media using multimodal deep learning</article-title>. <source>IEEE J. Biomed. Health Inform.</source>
<volume><bold>25</bold></volume>, <fpage>2193-</fpage>-<lpage>2203</lpage>. (<pub-id pub-id-type="doi">10.1109/JBHI.2020.3037027</pub-id>)<pub-id pub-id-type="pmid">33170786</pub-id>
</mixed-citation></ref><ref id="RSOS230964C129"><label>129<x xml:space="preserve">. </x></label><mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Wang</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Ma</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Jha</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Gao</surname>
<given-names>J</given-names></string-name></person-group>. <year>2021</year>
<comment>Multimodal emergent fake news detection via meta neural process networks. In <italic toggle="yes">Proc. of the 27th ACM SIGKDD Conf. on Knowledge Discovery &#x00026; Data Mining (KDD '21)</italic>, Online, 14-18 August 2021, pp. 3708&#x02013;3716. New York, NY: Association for Computing Machinery. (</comment><pub-id pub-id-type="doi">10.1145/3447548.3467153</pub-id><comment>)</comment></mixed-citation></ref><ref id="RSOS230964C130"><label>130<x xml:space="preserve">. </x></label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lv</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Wang</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Shao</surname>
<given-names>C</given-names></string-name></person-group>. <year>2022</year>
<article-title>TMIF: transformer-based multi-modal interactive fusion for automatic rumour detection</article-title>. <source><italic toggle="yes">Multimedia Syst.</italic></source>
<volume><bold>29</bold></volume>, <fpage>2979</fpage>-<lpage>2989</lpage>. (<pub-id pub-id-type="doi">10.1007/s00530-022-00916-8</pub-id>)</mixed-citation></ref></ref-list></back></article>
