<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC10732421</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0289162</article-id><article-id pub-id-type="publisher-id">PONE-D-23-09299</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Materials Science</subject><subj-group><subject>Materials</subject><subj-group><subject>Insulators</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Mathematical and Statistical Techniques</subject><subj-group><subject>Mathematical Functions</subject><subj-group><subject>Convolution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject><subj-group><subject>Deep Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Computer Vision</subject><subj-group><subject>Target Detection</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Energy and Power</subject><subj-group><subject>Power Distribution</subject><subj-group><subject>Power Grids</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Geometry</subject><subj-group><subject>Aspect Ratio</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>A lightweight YOLOv7 insulator defect detection algorithm based on DSC-SE</article-title><alt-title alt-title-type="running-head">A lightweight YOLOv7 insulator defect detection algorithm based on DSC-SE</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4519-2160</contrib-id><name><surname>Zhang</surname><given-names>Yulu</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Jiazhao</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Fu</surname><given-names>Wei</given-names></name><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Ma</surname><given-names>Juan</given-names></name><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref><xref rid="aff003" ref-type="aff">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0006-6739-0711</contrib-id><name><surname>Wang</surname><given-names>Gang</given-names></name><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>School of Electrical and Information Engineering, Beihua University, Jilin, China</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Chui Yang Liu Hospital, Tsinghua University, Beijing, China</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Affiliated Hospital of Beihua University, Jilin, China</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Yun</surname><given-names>Ji-Hoon</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Seoul National University of Science &#x00026; Technology, REPUBLIC OF KOREA</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors declare no conflict of interest.</p></fn><corresp id="cor001">* E-mail: <email>bhwanggang@163.com</email></corresp></author-notes><pub-date pub-type="epub"><day>20</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>18</volume><issue>12</issue><elocation-id>e0289162</elocation-id><history><date date-type="received"><day>9</day><month>4</month><year>2023</year></date><date date-type="accepted"><day>12</day><month>7</month><year>2023</year></date></history><permissions><copyright-statement>&#x000a9; 2023 Zhang et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Zhang et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0289162.pdf"/><abstract><p>As the UAV(Unmanned Aerial Vehicle) carrying target detection algorithm in transmission line insulator inspection, we propose a lightweight YOLOv7 insulator defect detection algorithm for the problems of inferior insulator defect detection speed and high model complexity. Firstly, a lightweight DSC-SE module is designed using a DSC(Depthwise Separable Convolution) fused SE channel attention mechanism to substitute the SC(Standard Convolution) of the YOLOv7 backbone extraction network to decrease the number of parameters in the network as well as to strengthen the shallow network&#x02019;s ability to obtain information about target features. Then, in the feature fusion part, GSConv(Grid Sensitive Convolution) is used instead of standard convolution to further lessen the number of parameters and the computational effort of the network. EIoU-loss(Efficient-IoU) is performed in the prediction head part to make the model converge faster. According to the experimental results, the recognition accuracy rate of the improved model is 95.2%, with a model size of 7.9M. Compared with YOLOv7, the GFLOPs are reduced by 54.5%, the model size is compressed by 37.8%, and the accuracy is improved by 4.9%. The single image detection time on the Jetson Nano is 105ms and the capture rate is 13FPS. With guaranteed accuracy and detection speed, it meets the demands of real-time detection.</p></abstract><funding-group><award-group id="award001"><funding-source>
<institution>scientific research project of Jilin Provincial Science and Technology Program</institution>
</funding-source><award-id>20190303038SF and 20200404154YY</award-id><principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0006-6739-0711</contrib-id>
<name><surname>Wang</surname><given-names>Gang</given-names></name>
</principal-award-recipient></award-group><funding-statement>This research was funded by the scientific research project of Jilin Provincial Science and Technology Program(20190303038SF and 20200404154YY). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="10"/><table-count count="5"/><page-count count="19"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>Data relevant to this study are available from <ext-link xlink:href="http://github.com//InsulatorData/insulatorDataSet.git" ext-link-type="uri">github.com//InsulatorData/insulatorDataSet.git</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>Data relevant to this study are available from <ext-link xlink:href="http://github.com//InsulatorData/insulatorDataSet.git" ext-link-type="uri">github.com//InsulatorData/insulatorDataSet.git</ext-link>.</p></notes></front><body><sec id="sec001"><title>1 Introduce</title><p>As power grid systems continue to improve, the length of overhead transmission lines is increasing, and insulator strings are crucial components in these lines. They provide mechanical stability and electrical insulation to support the conductor and prevent it from touching the tower. However, long-term exposure to environmental factors can cause insulators to break, self-destruct, or develop defects. This can have a significant impact on the reliability and safety of power grids, which in turn can pose a risk to society&#x02019;s economy. Therefore, detecting flaws in insulators is essential for the operation and maintenance of electricity grids. Insulator detection is usually carried out by professional inspectors who have extensive experience in recording the operating condition of insulators. The limitations of the distribution range of overhead transmission lines and the overhead environment make it difficult to inspect insulators for minor surface defects through manual observation with the naked eye or binoculars. Climbing up the tower for a closer look is an inefficient method, making it challenging to meet the requirements of real-time insulator inspection with manual inspection alone. As a result, using UAVs to take photos and carry out damage identification has become a mainstream inspection method, which greatly reduces the maintenance workload of field staff [<xref rid="pone.0289162.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0289162.ref002" ref-type="bibr">2</xref>].</p><p>Traditional methods for insulator image detection often rely on manual intervention for feature extraction, for example, using Hough transform detection, watershed algorithms, edge detection, and utilization of information on space and color [<xref rid="pone.0289162.ref003" ref-type="bibr">3</xref>&#x02013;<xref rid="pone.0289162.ref007" ref-type="bibr">7</xref>]. These techniques have limited improvement potential, are weakly generalizable, and make it difficult to satisfy the demands of small target identification in complex backdrops. They are commonly deployed for large-scale target images featuring simple backgrounds.</p><p>As computer technology develops rapidly, it also drives the rapid development of deep learning. Deep learning is a neural network-based artificial intelligence strategy that enables the automatic classification and recognition of data through the training and optimization of large amounts of data. Deep learning has become a popular tool for power system target detection in recent years because it can swiftly and accurately locate targets in images [<xref rid="pone.0289162.ref008" ref-type="bibr">8</xref>&#x02013;<xref rid="pone.0289162.ref010" ref-type="bibr">10</xref>]. Convolutional neural networks are commonly used in deep learning and can be used to detect defects in transmission lines, substations, and other equipment to reduce the workload of manual inspections. The difficulty and workload of feature extraction are significantly reduced by deep learning algorithms&#x02019; ability to automatically learn features from the original data without human extraction of features. Furthermore, the performance of deep learning-based object detection is excellent, and the methods can be split into two main categories. One category includes two-stage algorithms like Mask R-CNN [<xref rid="pone.0289162.ref011" ref-type="bibr">11</xref>], Fast R-CNN [<xref rid="pone.0289162.ref012" ref-type="bibr">12</xref>], Faster R-CNN [<xref rid="pone.0289162.ref013" ref-type="bibr">13</xref>], and Feature Pyramid Network (FPN) [<xref rid="pone.0289162.ref014" ref-type="bibr">14</xref>], which generate candidate frames for possible targets before identifying the target class and correcting the candidate boxes. Another class is the one-stage target detecting algorithm, which is represented as the YOLO family [<xref rid="pone.0289162.ref015" ref-type="bibr">15</xref>&#x02013;<xref rid="pone.0289162.ref018" ref-type="bibr">18</xref>] and SSD [<xref rid="pone.0289162.ref019" ref-type="bibr">19</xref>]. This algorithm can pinpoint the target&#x02019;s location on the picture and forecast the category&#x02019;s confidence level. Because of their quick detection times, compact models, and adaptable deployment, single-segment YOLO series algorithms are frequently used in the industry. They can perform both target recognition and boundary regression.</p><p>For target detection, Zhao et al. [<xref rid="pone.0289162.ref020" ref-type="bibr">20</xref>] used a fine-tuning method to improve the anchor frame generation method of Faster R-CNN and non-extreme suppressing in the area of suggestion networking to solve the detection problem of mutual occlusion. Tan et al. [<xref rid="pone.0289162.ref021" ref-type="bibr">21</xref>] suggested a method to improve the R-CNN network by fusing gradient, texture, and grayscale features to improve recall and accuracy. Li et al. [<xref rid="pone.0289162.ref022" ref-type="bibr">22</xref>] proposed an improved YOLOv5 insulator detection model with light correction enhancement, which converted the image from RGB color space to HSV space and corrected it by a two-dimensional adaptive gamma transform to improve the detection accuracy of insulators under light interference. According to Feng et al. [<xref rid="pone.0289162.ref023" ref-type="bibr">23</xref>], a YOLOv5-based automatic insulator detection method with K-value clustering successfully locates and identifies insulator flaws, but the detection accuracy is only 86.8%. The aforementioned techniques have improved the accuracy of detection to a satisfactory level, but they are still time- and resource-intensive to compute. To achieve lightweight processing, Wu et al. [<xref rid="pone.0289162.ref024" ref-type="bibr">24</xref>] presented a Centerent-based insulator inspection approach that reduced redundant information and enhanced network detection accuracy while also simplifying the backbone network to achieve lightweight processing. Sadykova et al. [<xref rid="pone.0289162.ref025" ref-type="bibr">25</xref>] provided a cost-effective method for aerial insulator detection by UAV based on YOLO&#x02019;s deep learning neural network model, which improved the insulator detection efficiency. Zhang et al. [<xref rid="pone.0289162.ref026" ref-type="bibr">26</xref>] proposed a SOD-YOLO model based on UAV image analysis, using the K-means algorithm to re-cluster, using channel pruning to lighten the processing, and adding a CBAM attention mechanism to improve the detection accuracy of small targets. Qiu et al. [<xref rid="pone.0289162.ref027" ref-type="bibr">27</xref>] compared YOLO-based UAV real-time detection algorithms, and the experimental results showed that YOLOv2 and YOLOv4-Tiny based on resnet50 have better detection accuracy and speed. Tulbure et al. [<xref rid="pone.0289162.ref028" ref-type="bibr">28</xref>] talked about modern defect detection models based on deep convolutional neural networks to provide a more efficient method for modern industrial inspection tasks. Cao et al. [<xref rid="pone.0289162.ref029" ref-type="bibr">29</xref>] used GhostNet to lighten the network, which reduced the memory consumption but the model&#x02019;s robustness was subpar. Yang et al. [<xref rid="pone.0289162.ref030" ref-type="bibr">30</xref>] proposed an improved YOLOv3 algorithm for UAV detection of insulators, using EIoU as a regression loss function, which significantly improved the overlap between the prediction frame and the real frame and accelerated the convergence rate.</p><p>Unmanned aircraft systems (UAS) are equipped with limited computing memory and resources, which poses a challenge to the high computational complexity of embedded models. This complexity is divided into spatial and temporal components, with spatial complexity determining the number of parameters in the model and temporal complexity dictating detection time. However, many deep learning-based insulator detection techniques suffer from high computational complexity, slow detection speed, and difficulty in embedding them in UAV devices. In 2022, Wang et al. [<xref rid="pone.0289162.ref031" ref-type="bibr">31</xref>] proposed YOLOv7 as the latest target detection model, which surpasses all current models in terms of both detection speed and accuracy. YOLOv7 performs exceptionally well in detecting objects in images, providing precise location and classification information. YOLOv7-tiny is a lighter version of YOLOv7, with fewer parameters and lower computational complexity, making it suitable for real-time object detection in resource-constrained environments. However, YOLOv7-tiny may not be suitable for detecting tiny insulator defects, as it lacks sufficient salient features and does not meet the model requirements for size and contrast, resulting in reduced detection accuracy compared to larger and more complex models. There is not much research on applying YOLOv7-Tiny in UAS to inspect insulators, and there is room to improve the accuracy and detection speed of the model for the real-time nature of UAVs in insulator defect detection tasks. We provide a simple, quick, and effective insulator defect identification approach based on the YOLOv7-tiny model to address this issue. The primary contributions are as follows: The DSC-SE module utilizes the Depthwise Separable Convolution and SE attention mechanism as a small parametric and enhanced feature extraction module to replace the normal convolution of the YOLOv7-tiny backbone feature extraction network. This reduces the size of the model and enhances the detection of small targets. The feature fusion network uses GSConv, which has comparable extraction capability to standard convolution, to reduce the time complexity of the model. Additionally, the VOVGSCSP module is designed with residual edges based on the GSConv to accelerate inference while maintaining accuracy. Finally, the EIoU loss is chosen as the localization loss function to accelerate the convergence of the model and to better measure the similarity between the detection results and the real insulator frame.</p></sec><sec id="sec002"><title>2 Related algorithms</title><sec id="sec003"><title>2.1 YOLOv7</title><p>YOLOv7 is an upgraded version of YOLOv5, which introduces ELAN structure and MP (MaxPool) structure based on YOLOv5&#x02019;s backbone extraction network with FPS in the range of 5&#x02013;160. As shown in <xref rid="pone.0289162.g001" ref-type="fig">Fig 1A</xref>, the ELAN module is an efficient network structure with two branches. One goes through a 1&#x000d7;1 convolution to do the channel number change, the other first goes through a 1&#x000d7;1 convolution module to do the channel number change and then goes through four 3&#x000d7;3 convolution modules to do the feature extraction, and finally, the two branches are summed to get the final feature extraction result. The ELAN module continuously enhances the learning capability of the network by controlling the shortest and longest gradient path. As shown in <xref rid="pone.0289162.g001" ref-type="fig">Fig 1B</xref>, the MP module also has two branches that serve to perform downsampling. The first branch involves a maximum pooling layer for downsampling followed by a 1&#x000d7;1 convolution to change the channel count. The second branch, on the other hand, first undergoes a 1&#x000d7;1 convolution to change the channel count and then a 3&#x000d7;3 convolution kernel with a 2-step convolution block, which is also downsampled. The results of the two branches are added together to get the result of super downsampling.</p><fig position="float" id="pone.0289162.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.g001</object-id><label>Fig 1</label><caption><title>Improved part of YOLOv7.</title><p>(a) shows the ELAN network structure; (b) shows the MP network structure.</p></caption><graphic xlink:href="pone.0289162.g001" position="float"/></fig><p>In addition, YOLOv7 has a low computational cost and efficient training speed, which makes it promising for a wide range of practical applications. A more compact variant of YOLOv7, YOLOv7-tiny, has a quicker detection rate and less computational complexity. The model has a good performance-to-speed ratio and is appropriate for real-time object detection in resource-constrained environments. The backbone network, head network, and output layer are the three divisions of the YOLOv7-tiny network. ELAN and MP structures are used in the network&#x02019;s backbone for feature extraction. where the ELAN structure is stacked with 2 fewer convolutional layers than the standard version of YOLOv7, the activation function becomes LeakyReLU, which does not change the width and height of the input feature layers, and enhances the interaction between each feature layer through expansion, random combination and splicing to enhance the learning capability of the network; The head network contains several convolutional layers and layers of pooling for additional feature extraction. The output layer, which is the core component of YOLOv7-tiny, has detection and classification heads for identifying and categorizing objects. Mosaic data augmentation is a new training method used by YOLOv7. Multiple images can be stitched together to form a larger image using mosaic data augmentation, increasing the diversity and amount of training data. This method can improve the model&#x02019;s accuracy and generalization.</p></sec><sec id="sec004"><title>2.2 SE attention</title><p>The SE (Squeeze-and-Excitation) [<xref rid="pone.0289162.ref032" ref-type="bibr">32</xref>] attention mechanism is a convolutional neural network attention mechanism, as shown in <xref rid="pone.0289162.g002" ref-type="fig">Fig 2</xref>. Its main idea is to improve model performance by assigning channel weights adaptively. Following the convolutional layer, the SE attention mechanism obtains the average response of each channel via a global pooling layer before generating the weight coefficients for each channel via two layers that are fully interconnected (a compression layer and an activation layer).</p><fig position="float" id="pone.0289162.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.g002</object-id><label>Fig 2</label><caption><title>SE network architecture.</title></caption><graphic xlink:href="pone.0289162.g002" position="float"/></fig><p>To improve the model&#x02019;s attention to important features, these weight coefficients are used to weigh each channel in the feature map. The following benefits pertain to the SE attention mechanism:</p><list list-type="order"><list-item><p>It improves the model&#x02019;s performance by adaptively learning the significance of each channel.</p></list-item><list-item><p>Global pooling and fully connected layers are used to implement it; no additional parameters are needed.</p></list-item><list-item><p>It may be included in already-existing convolutional neural networks, enhancing model performance without requiring complete model retraining.</p></list-item></list><p>The SE module enhances feature representation by adaptively learning the relationships between feature channels. In insulator detection, the SE module helps the network to better understand the insulator features and improve the focus on critical information. This helps to improve the accuracy and robustness of insulator detectors.</p></sec></sec><sec id="sec005"><title>3 Improved YOLOv7-tiny network structure</title><sec id="sec006"><title>3.1 DSC-SE</title><p>The YOLOv7-tiny backbone network utilizes a multitude of convolutional blocks that comprise 3x3 standard convolutional layers and pooling layers in a series to extract features. The input image undergoes processing by the backbone network to generate a sequence of feature maps, as illustrated in <xref rid="pone.0289162.g003" ref-type="fig">Fig 3</xref>. However, the shallow layer network has a small perceptual field, and each neuron can only capture local information from the input image. The shallow network has fewer layers and a limited number of filters that extract the basic features of insulator texture and color. This makes the network insufficient in extracting insulator pixels with small defects and a large number of parameters, making it challenging to extract maximum effective feature information with smaller arithmetic power.</p><fig position="float" id="pone.0289162.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.g003</object-id><label>Fig 3</label><caption><title>Schematic diagram of the convolutional extraction information of the backbone network.</title></caption><graphic xlink:href="pone.0289162.g003" position="float"/></fig><p>To solve the above problems, a lightweight module DSC-SE based on deep convolution and attention mechanism is designed, and the structure is shown in <xref rid="pone.0289162.g004" ref-type="fig">Fig 4</xref>.</p><fig position="float" id="pone.0289162.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.g004</object-id><label>Fig 4</label><caption><title>Structure diagram of the DSC-SE module.</title></caption><graphic xlink:href="pone.0289162.g004" position="float"/></fig><p>DSC-SE module through a 1&#x000d7;1 convolution BN processing and Hard-swish activation function after and multiple 3&#x000d7;3 DSC in series and through the SE module of channel attention mechanism, can solve the problem of extracting too much interference information, and then spliced with 1&#x000d7;1 convolution to achieve feature extraction, it can get more important feature information to some extent and enhance the model&#x02019;s ability to obtain features of insulators and defective parts.</p><p>Defects in insulator inspection tasks often have small dimensions and low contrast, which makes accurate detection a challenge. In this case, the DSC depth-separable convolution module plays an important role. By decomposing the standard convolution into depth and point-by-point convolutions, the DSC module reduces computational complexity while maintaining expressiveness. This is very beneficial for insulator detection because the task requires real-time or quasi-real-time analysis over a large number of image samples, so efficiency is critical. Assuming that there are M input channels for standard convolution, N output channels, <italic toggle="yes">D</italic><sub><italic toggle="yes">k</italic></sub>&#x000d7;<italic toggle="yes">D</italic><sub><italic toggle="yes">k</italic></sub> size for the convolution kernel, and H&#x000d7;W size for the output characteristic graph, the ratio of the standard convolution to the Depthwise Separable Convolution is <italic toggle="yes">R</italic><sub><italic toggle="yes">Q</italic></sub>:
<disp-formula id="pone.0289162.e001">
<alternatives><graphic xlink:href="pone.0289162.e001.jpg" id="pone.0289162.e001g" position="anchor"/><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>Q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>M</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mi>M</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>(1)</label>
</disp-formula></p><p>From Eq (<xref rid="pone.0289162.e001" ref-type="disp-formula">1</xref>), it can be seen that the number of parameters is about 1/9 of the standard convolution after processing with 3 &#x000d7; 3 DSC, which reduces the number of parameters while ensuring the extraction of more feature information. The depth-separable convolution is used in layers 3 and 5 of the backbone network, and the DSC-SE module is used in layers 2, 7, 8, 9, 10, 11, 12, 13, and 14. The calculation process of the DCS-SE module is:
<disp-formula id="pone.0289162.e002">
<alternatives><graphic xlink:href="pone.0289162.e002.jpg" id="pone.0289162.e002g" position="anchor"/><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>(2)</label>
</disp-formula>
<disp-formula id="pone.0289162.e003">
<alternatives><graphic xlink:href="pone.0289162.e003.jpg" id="pone.0289162.e003g" position="anchor"/><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>&#x02026;</mml:mn><mml:mi>D</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>(3)</label>
</disp-formula>
<disp-formula id="pone.0289162.e004">
<alternatives><graphic xlink:href="pone.0289162.e004.jpg" id="pone.0289162.e004g" position="anchor"/><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>(4)</label>
</disp-formula>
<disp-formula id="pone.0289162.e005">
<alternatives><graphic xlink:href="pone.0289162.e005.jpg" id="pone.0289162.e005g" position="anchor"/><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Y</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>(5)</label>
</disp-formula>
<disp-formula id="pone.0289162.e006">
<alternatives><graphic xlink:href="pone.0289162.e006.jpg" id="pone.0289162.e006g" position="anchor"/><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>(6)</label>
</disp-formula>
where, denotes the feature map extracted by standard convolution for the input (taking 1, 2&#x02026;n), denotes the result of deep separable convolution, denotes the feature map obtained by SE attention mechanism, denotes the result of splicing with, and denotes the final extracted feature map of DSC-SE obtained by standard convolution.</p><p>The DSC-SE module, while globally extracting the channel information, adaptively learns the dependencies between different channels with the help of SE, adaptively enhances the feature pixels of the target, weakening the interference of complex background information on insulator fault detection and ensuring the accurate extraction of defects by the network while expanding the shallow network perception field.</p></sec><sec id="sec007"><title>3.2 GSConv convolution and VOVGSCSP module</title><p>To address the problem that insulators are complex and small targets in the overhead scene, and the depth-separable convolution extraction will lose a large amount of channel information, we introduce the lightweight convolution GSConv&#x02014;which is a hybrid convolution of SC, DSC, and shuffle [<xref rid="pone.0289162.ref033" ref-type="bibr">33</xref>]. The shuffle operation, as shown in <xref rid="pone.0289162.g005" ref-type="fig">Fig 5</xref> below, is used to permeate the information generated by SC into each part of the information generated by DSC. By exchanging local feature information uniformly on different channels, this method allows the information from the normal convolution to be completely mixed into the output of the deeply separable convolution, which can reduce the computational cost while maintaining maximum accuracy.</p><fig position="float" id="pone.0289162.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.g005</object-id><label>Fig 5</label><caption><title>The network architecture of GSConv.</title></caption><graphic xlink:href="pone.0289162.g005" position="float"/></fig><p>The FLOPs are typically used to describe the convolutional computation&#x02019;s time complexity. As a result, the SC, DSC, and GSConv time complexity is:
<disp-formula id="pone.0289162.e007">
<alternatives><graphic xlink:href="pone.0289162.e007.jpg" id="pone.0289162.e007g" position="anchor"/><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>(7)</label>
</disp-formula>
<disp-formula id="pone.0289162.e008">
<alternatives><graphic xlink:href="pone.0289162.e008.jpg" id="pone.0289162.e008g" position="anchor"/><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>(8)</label>
</disp-formula>
<disp-formula id="pone.0289162.e009">
<alternatives><graphic xlink:href="pone.0289162.e009.jpg" id="pone.0289162.e009g" position="anchor"/><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives>
<label>(9)</label>
</disp-formula>
where W is the original feature map&#x02019;s width and H is the output feature map&#x02019;s height. The number of input channels per convolution kernel is <italic toggle="yes">C</italic><sub>1</sub>, the number of channels in the output feature map is <italic toggle="yes">C</italic><sub>2</sub>, and <italic toggle="yes">K</italic><sub>1</sub>&#x000d7;<italic toggle="yes">K</italic><sub>2</sub> is the magnitude of the convolution kernel. It can be seen that the time complexity of GSConv is less than that of SC and DSC. In insulator detection, the GSConv module enables the network to focus more on the grid region where the insulators are located and extract more accurate features by adaptive convolution. This helps to improve the localization accuracy and detection performance of the insulator detector.</p><p>This paper proposes the use of lightweight convolutional GSConv instead of SC to reduce the computational cost by up to 50%. However, a new model is still needed to further reduce inference time while maintaining accuracy. To address this, the paper introduces the VoV-GSCSP network module, shown in <xref rid="pone.0289162.g006" ref-type="fig">Fig 6</xref>, which is based on the GSConv primary aggregation method and draws inspiration from VoVNet [<xref rid="pone.0289162.ref034" ref-type="bibr">34</xref>] and CSPNet [<xref rid="pone.0289162.ref035" ref-type="bibr">35</xref>]. The module replaces ordinary convolution with GSConv in the feature fusion network and replaces five ordinary convolutions after the Concat layer with the VoV-GSCSP module. This approach speeds up computation and inference while ensuring accuracy.</p><fig position="float" id="pone.0289162.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.g006</object-id><label>Fig 6</label><caption><title>The network architecture of VOVGSCSP.</title></caption><graphic xlink:href="pone.0289162.g006" position="float"/></fig><p>The framework of the light-weight insulator defect inspection model based on the refined YOLOv7-tiny is shown in <xref rid="pone.0289162.g007" ref-type="fig">Fig 7</xref> below by replacing the standard convolution of the backbone extracting features network with the depth-separable convolution and DSC-SE modules, and by introducing the GSConv convolution and VOVGSCSP modules in the feature fusion network.</p><fig position="float" id="pone.0289162.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.g007</object-id><label>Fig 7</label><caption><title>The improved network structure of YOLOv7-tiny.</title></caption><graphic xlink:href="pone.0289162.g007" position="float"/></fig></sec><sec id="sec008"><title>3.3 Improvement of the loss function</title><p>In the YOLOv7-tiny model, classification loss, localization loss, and confidence loss are weighted sums that add up to an overall net loss. The confidence loss and categorization loss functions among them use the binary cross-entropy loss, and the localization loss function uses the CIoU-loss.</p><disp-formula id="pone.0289162.e010">
<alternatives><graphic xlink:href="pone.0289162.e010.jpg" id="pone.0289162.e010g" position="anchor"/><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">&#x003c1;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi mathvariant="normal">&#x003b1;</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:math></alternatives>
<label>(10)</label>
</disp-formula><disp-formula id="pone.0289162.e011">
<alternatives><graphic xlink:href="pone.0289162.e011.jpg" id="pone.0289162.e011g" position="anchor"/><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>(11)</label>
</disp-formula><disp-formula id="pone.0289162.e012">
<alternatives><graphic xlink:href="pone.0289162.e012.jpg" id="pone.0289162.e012g" position="anchor"/><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>4</mml:mn><mml:mrow><mml:msup><mml:mi>&#x003c0;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">arctan</mml:mi><mml:mfrac><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">arctan</mml:mi><mml:mfrac><mml:mi>w</mml:mi><mml:mi>h</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives>
<label>(12)</label>
</disp-formula><p>The formula for calculating the Euclidean distance (&#x003c1;) between two locations is given by the centroids (b and c) of the predicted and actual boxes. The diagonal length of the minimal bounding rectangle of the predicted and actual boxes is represented by s, while the aspect ratio similarity of the two frames is represented by v. The dimensions of the actual box are denoted by a and b, while the width and height of the anticipated box are represented by w and h, respectively. The ratio&#x02019;s parameter is used to balance the equation.</p><p>Although the CIoU loss considers the boundaries of the regression&#x02019;s aspect ratio, the centroid of distance, and overlapping region. However, returning to the term v in Eq(12), it can be seen that there are two problems: (1) uses the aspect ratio of the prediction box and the target box, then when the prediction box&#x02019;s size satisfies <inline-formula id="pone.0289162.e013"><alternatives><graphic xlink:href="pone.0289162.e013.jpg" id="pone.0289162.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:math></alternatives></inline-formula>, the penalty term of this item in CIoU loses its effect. (2) According to the following gradient formula of w, h, it can be concluded that <inline-formula id="pone.0289162.e014"><alternatives><graphic xlink:href="pone.0289162.e014.jpg" id="pone.0289162.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula> are inversely related to each other. That is, when one of w, h increases during training, the other must decreases, which occasionally inhibits the model from efficiently optimizing the similarity.</p><disp-formula id="pone.0289162.e015">
<alternatives><graphic xlink:href="pone.0289162.e015.jpg" id="pone.0289162.e015g" position="anchor"/><mml:math id="M15" display="block" overflow="scroll"><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>8</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c0;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">arctan</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">arctan</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mfrac><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></alternatives>
<label>(13)</label>
</disp-formula><disp-formula id="pone.0289162.e016">
<alternatives><graphic xlink:href="pone.0289162.e016.jpg" id="pone.0289162.e016g" position="anchor"/><mml:math id="M16" display="block" overflow="scroll"><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>8</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c0;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">arctan</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">arctan</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mfrac><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></alternatives>
<label>(14)</label>
</disp-formula><p>EIoU Loss [<xref rid="pone.0289162.ref036" ref-type="bibr">36</xref>] is selected as the network&#x02019;s regression loss function in this paper to address this issue. The overlap loss <italic toggle="yes">L</italic><sub><italic toggle="yes">Iou</italic></sub> between the forecast box and the real box, the central distance loss <italic toggle="yes">L</italic><sub><italic toggle="yes">dis</italic></sub> between the prediction frame and the real frame, and the width and height loss <italic toggle="yes">L</italic><sub><italic toggle="yes">asp</italic></sub> between the prediction frame and the real frame are the three components that EIoU divides the loss function into.
<disp-formula id="pone.0289162.e017">
<alternatives><graphic xlink:href="pone.0289162.e017.jpg" id="pone.0289162.e017g" position="anchor"/><mml:math id="M17" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">&#x003c1;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">&#x003c1;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">&#x003c1;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>(15)</label>
</disp-formula>
where <inline-formula id="pone.0289162.e018"><alternatives><graphic xlink:href="pone.0289162.e018.jpg" id="pone.0289162.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0289162.e019"><alternatives><graphic xlink:href="pone.0289162.e019.jpg" id="pone.0289162.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> are the width and height of the minimal bounding rectangles of the predicted and real frames, respectively.</p><p>The first two parts of the EIoU loss follow the CIoU method, and the width-height loss is such that the difference between the width and height of the predicted and real boxes is minimized, resulting in faster convergence. In insulator detection, the EIoU loss function can better measure the similarity between detection results and real insulator frames, thus improving the learning of the model and enhancing the detection accuracy.</p></sec></sec><sec id="sec009"><title>4 Experiments</title><sec id="sec010"><title>4.1 Data set introduction</title><p>The experimental dataset used in this study comprises two parts: the China Academy of Electric Power (CPLID) dataset from GitHub and the Baidu Internet images dataset. The former includes 1417 images of insulators, out of which 600 are normal and 647 are broken. The images were captured via aerial photography of UAV inspection sites. The dataset was annotated using the open-source LabelImg annotation tool and categorized as insulator and defect. The completed dataset was randomly divided into training, validation, and test sets in the ratio of 7:2:1 to ensure an unbiased evaluation of the model. Given the need to reduce overfitting during training, a large number of data samples were used for model training. The training set was augmented using various techniques such as mirroring, flipping, changing contrast, and saturation resulting in a total of 1732 samples.</p></sec><sec id="sec011"><title>4.2 Evaluation index</title><p>Precision P, recall R, and mAP(mean average precision) are common assessment metrics that were utilized in this experiment to evaluate the model&#x02019;s performance. The formulas are as follows:
<disp-formula id="pone.0289162.e020">
<alternatives><graphic xlink:href="pone.0289162.e020.jpg" id="pone.0289162.e020g" position="anchor"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>(16)</label>
</disp-formula>
<disp-formula id="pone.0289162.e021">
<alternatives><graphic xlink:href="pone.0289162.e021.jpg" id="pone.0289162.e021g" position="anchor"/><mml:math id="M21" display="block" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>(17)</label>
</disp-formula>
<disp-formula id="pone.0289162.e022">
<alternatives><graphic xlink:href="pone.0289162.e022.jpg" id="pone.0289162.e022g" position="anchor"/><mml:math id="M22" display="block" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:munderover><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives>
<label>(18)</label>
</disp-formula>
<disp-formula id="pone.0289162.e023">
<alternatives><graphic xlink:href="pone.0289162.e023.jpg" id="pone.0289162.e023g" position="anchor"/><mml:math id="M23" display="block" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives>
<label>(19)</label>
</disp-formula></p><p>The positive samples correctly identified by the model are referred to as TP, while negative samples incorrectly identified are referred to as FP. Misclassification of positive samples as negative classes is denoted by FN, where n represents the number of sample classes in the data set. Furthermore, the number of model parameters, floating point operations, and detection speed metrics were increased to provide a more accurate estimation of the performance of the lightweight model.</p></sec><sec id="sec012"><title>4.3 Experimental environment and configuration</title><p><xref rid="pone.0289162.t001" ref-type="table">Table 1</xref> shows a portion of the experimental setting used in this paper, which employed the Pytorch framework in a Python 3.9 environment to implement the full algorithm of the model.</p><table-wrap position="float" id="pone.0289162.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.t001</object-id><label>Table 1</label><caption><title>Configuration of the experimental environment.</title></caption><alternatives><graphic xlink:href="pone.0289162.t001" id="pone.0289162.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Configuration</th><th align="center" rowspan="1" colspan="1">Version</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">
<bold>CPU</bold>
</td><td align="center" rowspan="1" colspan="1">AMD 5600H</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>GPU</bold>
</td><td align="center" rowspan="1" colspan="1">Nvidia GeForce GTX 3050</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>CUDA</bold>
</td><td align="center" rowspan="1" colspan="1">11.6</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>Operation system</bold>
</td><td align="center" rowspan="1" colspan="1">Windows11</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>Frameworks</bold>
</td><td align="center" rowspan="1" colspan="1">Pytorch</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>Compilation environment</bold>
</td><td align="center" rowspan="1" colspan="1">Pycharm</td></tr></tbody></table></alternatives></table-wrap><p>The paper specifies that the experimental training process was conducted using the gradient descent method with the Adam optimizer. The initial learning rate was set to 0.01 and the learning rate was adjusted using Cosine Annealing LR. The image input size was 640 &#x000d7; 640 and the batch size was 16. The total training comprised 150 rounds.</p></sec></sec><sec id="sec013"><title>5 Experimental results and analysis</title><p>YOLOv7 originally employed CIoU as the localization loss function. To choose a more suitable loss function, we chose the performance of EIoU and SIoU on the YOLOv7 model to compare with it. The comparative effects are illustrated in <xref rid="pone.0289162.t002" ref-type="table">Table 2</xref>.</p><table-wrap position="float" id="pone.0289162.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.t002</object-id><label>Table 2</label><caption><title>Performance comparison of the different loss functions.</title></caption><alternatives><graphic xlink:href="pone.0289162.t002" id="pone.0289162.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="justify" rowspan="1" colspan="1">Loss Function</th><th align="justify" rowspan="1" colspan="1">Precision(%)</th><th align="justify" rowspan="1" colspan="1">Recall(%)</th><th align="justify" rowspan="1" colspan="1">mAP0.5(%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">CIoU</td><td align="left" rowspan="1" colspan="1">92.4</td><td align="left" rowspan="1" colspan="1">86.3</td><td align="left" rowspan="1" colspan="1">93.4</td></tr><tr><td align="left" rowspan="1" colspan="1">SIoU</td><td align="left" rowspan="1" colspan="1">93.8</td><td align="left" rowspan="1" colspan="1">87.3</td><td align="left" rowspan="1" colspan="1">93.3</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>EIoU</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>95.2</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>87.6</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>93.8</bold>
</td></tr></tbody></table></alternatives></table-wrap><p>As indicated in <xref rid="pone.0289162.t002" ref-type="table">Table 2</xref> above, the usage of EIoU can increase precision and recall by 2.8% and 1.3% in comparison to the other two approaches. The outcomes of the experiment demonstrate that the EIoU loss function performs well in insulator fault identification.</p><p>In <xref rid="pone.0289162.g008" ref-type="fig">Fig 8</xref>, we present the PR curves of our improved model, where the vertical axis represents the precision rate P and the horizontal axis represents the recall rate R. The PR curves indicate that the accuracy rate only slightly decreases as the recall rate increases. When the confidence level is set to 0.5, the mean average precision (mAP) for detecting normal insulators is 92.5%, for defective insulators is 95%, and for the total category is 93.8%.</p><fig position="float" id="pone.0289162.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.g008</object-id><label>Fig 8</label><caption><title>The precision-recall curve of our model.</title></caption><graphic xlink:href="pone.0289162.g008" position="float"/></fig><p>To evaluate the detection performance of the algorithm, we compared the improved algorithm with the commonly used algorithm for detecting insulators in terms of accuracy rate and loss value. The accuracy curves are presented in <xref rid="pone.0289162.g009" ref-type="fig">Fig 9A</xref>, where the average accuracy of our improved model remains stable at 95% after 100 epochs, while YOLOv7-tiny remains stable at 90%. The loss curves of different models are shown in <xref rid="pone.0289162.g009" ref-type="fig">Fig 9B</xref>, where it can be observed that the loss curves of different models decrease rapidly within about 30 epochs and eventually stabilize after 100 rounds. Our improved model demonstrates a faster decline and better convergence than YOLOv7-tiny within 20 rounds, indicating that the adjustment of the loss function improves the network&#x02019;s convergence. YOLOv4-tiny exhibits the worst loss.</p><fig position="float" id="pone.0289162.g009"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.g009</object-id><label>Fig 9</label><caption><title>Comparison of different methods.</title><p>(a) Precision Curve; (b) Loss Curve.</p></caption><graphic xlink:href="pone.0289162.g009" position="float"/></fig><sec id="sec014"><title>5.1 Ablation test</title><p>Several ablation experiments were created for comparative verification to test the performance of the improved module. The results are listed in <xref rid="pone.0289162.t003" ref-type="table">Table 3</xref>, &#x0221a; which shows the addition of this module.</p><table-wrap position="float" id="pone.0289162.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.t003</object-id><label>Table 3</label><caption><title>Results of ablation experiments.</title></caption><alternatives><graphic xlink:href="pone.0289162.t003" id="pone.0289162.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Method</th><th align="center" colspan="4" rowspan="1">Improvement</th><th align="justify" rowspan="1" colspan="1">mAP(%)</th><th align="center" rowspan="1" colspan="1">GFLOPs (%)</th><th align="center" rowspan="2" colspan="1">Params(M)</th></tr><tr><th align="center" rowspan="1" colspan="1"/><th align="center" rowspan="1" colspan="1">DSC-SE</th><th align="center" rowspan="1" colspan="1">GSConv</th><th align="center" rowspan="1" colspan="1">VOVGSCSP</th><th align="center" rowspan="1" colspan="1">EIoU</th><th align="left" rowspan="1" colspan="1"/><th align="center" rowspan="1" colspan="1"/></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">
<bold>1</bold>
</td><td align="center" rowspan="1" colspan="1"/><td align="center" rowspan="1" colspan="1"/><td align="center" rowspan="1" colspan="1"/><td align="center" rowspan="1" colspan="1"/><td align="center" rowspan="1" colspan="1">93.1</td><td align="center" rowspan="1" colspan="1">13.2</td><td align="center" rowspan="1" colspan="1">6.01</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>2</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1"/><td align="center" rowspan="1" colspan="1"/><td align="center" rowspan="1" colspan="1"/><td align="center" rowspan="1" colspan="1">90.1</td><td align="center" rowspan="1" colspan="1">6.9</td><td align="center" rowspan="1" colspan="1">4.18</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>3</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1"/><td align="center" rowspan="1" colspan="1"/><td align="center" rowspan="1" colspan="1">92.8</td><td align="center" rowspan="1" colspan="1">5.7</td><td align="center" rowspan="1" colspan="1">3.35</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>4</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1"/><td align="center" rowspan="1" colspan="1">93.4</td><td align="center" rowspan="1" colspan="1">6.0</td><td align="center" rowspan="1" colspan="1">3.74</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>5</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1"/><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1">93.1</td><td align="center" rowspan="1" colspan="1">5.7</td><td align="center" rowspan="1" colspan="1">3.35</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>6</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>&#x0221a;</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>93.8</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>6.0</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>3.74</bold>
</td></tr></tbody></table></alternatives></table-wrap><p>The table displays the detection findings of the original YOLOv7 model in the first row. The introduction of the DSC-SE module into the backbone extraction network results in a decrease of 30.4% in parameters and 47.8% in computational effort. However, the mAP value decreases due to the reduction of the improved model parameters and the number of convolutional layers. In the feature fusion part, the use of GSConv and VOVGSCSP modules instead of the standard convolution leads to a 13% and 11% decrease in calculation and parameter quantities, respectively. The mAP improves by 0.5% when using EIoU-loss as a localization loss without additional network size and complexity. The VOVGSCSP module results in a 0.7% increase in mAP for the entire model, but requires a 5% and 10% increase in complexity and computational effort, respectively. However, the overall improved model mAP reaches 93.8%, with a 37.8% reduction in the number of parameters and a 54.5% reduction in computational effort. This makes it more suitable for edge terminals with limited hardware configuration, small size, and low power consumption. Taking into account the trade-off between accuracy and number of parameters, the current improved model has advantages over other models.</p></sec><sec id="sec015"><title>5.2 Comparison of detection performance with other models</title><p>In order to determine the most suitable network model, we conducted a comprehensive experimental comparison of various common detection models from other architectures. As can be seen from <xref rid="pone.0289162.t004" ref-type="table">Table 4</xref>, our improved model achieved the highest accuracy of 95.2% and ranked second in FPS, while also having significantly less computational complexity than YOLOv5s, YOLOv6, and YOLOv7-tiny. Although the YOLOv4-tiny model performs better in detecting insulators at a faster speed, it is less effective in identifying small defective insulators. The decrease in the recall rate compared to the original algorithm is attributed to the reduction of network parameters. Based on a comprehensive comparison, our proposed algorithm proves the effectiveness of the improved algorithm.</p><table-wrap position="float" id="pone.0289162.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.t004</object-id><label>Table 4</label><caption><title>Performance comparison with other models.</title></caption><alternatives><graphic xlink:href="pone.0289162.t004" id="pone.0289162.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Models</th><th align="center" rowspan="1" colspan="1">Precision(%)</th><th align="center" rowspan="1" colspan="1">Recall(%)</th><th align="center" rowspan="1" colspan="1">GFLOPs(G)</th><th align="center" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">
<bold>YOLOv4-tiny</bold>
</td><td align="center" rowspan="1" colspan="1">86.1</td><td align="center" rowspan="1" colspan="1">81.4</td><td align="center" rowspan="1" colspan="1">3.43</td><td align="center" rowspan="1" colspan="1">63</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>YOLOv5s</bold>
</td><td align="center" rowspan="1" colspan="1">89.8</td><td align="center" rowspan="1" colspan="1">86.2</td><td align="center" rowspan="1" colspan="1">15.8</td><td align="center" rowspan="1" colspan="1">46</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>YOLOv6</bold>
</td><td align="center" rowspan="1" colspan="1">88.3</td><td align="center" rowspan="1" colspan="1">85.2</td><td align="center" rowspan="1" colspan="1">44.2</td><td align="center" rowspan="1" colspan="1">42</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>YOLOv7-tiny</bold>
</td><td align="center" rowspan="1" colspan="1">90.3</td><td align="center" rowspan="1" colspan="1">92.4</td><td align="center" rowspan="1" colspan="1">13.2</td><td align="center" rowspan="1" colspan="1">50</td></tr><tr><td align="center" rowspan="1" colspan="1">
<bold>Our Model</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>95.2</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>87.6</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>5.9</bold>
</td><td align="center" rowspan="1" colspan="1">
<bold>52</bold>
</td></tr></tbody></table></alternatives></table-wrap><p>This paper explores the effectiveness of lightweight defect detection models, specifically focusing on YOLOv7-tiny. However, it is worth noting that this model may struggle with detecting very small insulator defects due to its size in comparison to larger and more complex models. Insulators are considered small target defects as they are caused by surface defects such as erosion, cracking, broken insulator skirts, and exposed mandrels. It is possible that very small defects may not have sufficient salient features or fail to meet the model&#x02019;s requirements for size and contrast, which can result in reduced detection accuracy. To evaluate the detection performance of the improved model, a comparison test was conducted on the UAV-based aerial photography test set. <xref rid="pone.0289162.g010" ref-type="fig">Fig 10</xref> displays four different images from left to right, and the results obtained using YOLOv4-tiny, YOLOv5s, YOLOv6, YOLOv7-tiny, and our improved method detection are shown from top to bottom.</p><fig position="float" id="pone.0289162.g010"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.g010</object-id><label>Fig 10</label><caption><title>The detection results of different methods.</title></caption><graphic xlink:href="pone.0289162.g010" position="float"/></fig><p><xref rid="pone.0289162.g010" ref-type="fig">Fig 10A</xref> shows the case with low light, and <xref rid="pone.0289162.g010" ref-type="fig">Fig 10B&#x02013;10D</xref> show the case with background interference from towers and transmission lines. The comparative analysis shows that YOLOv4-tiny has a low detection accuracy for insulator defects on high towers. As seen in <xref rid="pone.0289162.g010" ref-type="fig">Fig 10C</xref>, YOLOv5s and YOLOv6 can detect small insulator defects in complex backgrounds, but with low accuracy. The YOLOv7-tiny model struggles with detecting small defects on insulator surfaces in low-contrast images. In <xref rid="pone.0289162.g010" ref-type="fig">Fig 10A</xref>, the model failed to detect minor defects in a poorly lit environment where the insulator was broken, resulting in an exposed mandrel. However, the improved model shows better performance in identifying broken defects on insulators and can accurately detect some insulator targets even in complex backgrounds.</p></sec><sec id="sec016"><title>5.3 Speed comparison experiment deployed on the Jetson Nano</title><p>Hardware for the Jetson Nano: The Jetson Nano&#x02019;s entire design is built on the ARM architecture of the chip, which is small, inexpensive, and low power. It features a quad-core ARM CPU, a 128-core GPU, and 4GB of LPDDR4 memory.</p><p>Jetson Nano environment builds: YOLOv7-tiny is based on the Pytorch framework and requires Pytorch version 1.7 or higher. To meet the requirements, install JetPack SDK 4.6.1 according to the official Nvidia documentation, which provides Ubuntu 18.04, CUDA 10.2, and CUDNN 8.2, and Create a standalone runtime environment for YOLOv7 using Archiconda, install Pytorch 1.8.0, Torchvision 0.9.0, sourced from the official NVIDIA website. The official system default swap partition Swap memory is 2G, when installing some software or running larger scale computing, there is often a pop-up to remind that Swap memory is insufficient, to avoid this, use the command to increase 6G Swap memory and turn on the maximum power mode, at this time the power is 10W.</p><p>At an image resolution of 640&#x000d7;640, the forward transmission time of this refined model is 33.3% faster than the original model, as shown in <xref rid="pone.0289162.t005" ref-type="table">Table 5</xref>. As the input resolution decreases, the inference speed of the model&#x02019;s forward transmission elapsed time gradually approaches, but this improved model is still the fastest.</p><table-wrap position="float" id="pone.0289162.t005"><object-id pub-id-type="doi">10.1371/journal.pone.0289162.t005</object-id><label>Table 5</label><caption><title>Comparison speed at Jetson Nano.</title></caption><alternatives><graphic xlink:href="pone.0289162.t005" id="pone.0289162.t005g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Image size</th><th align="center" rowspan="1" colspan="1">Models</th><th align="center" rowspan="1" colspan="1">Preprocessing (ms)</th><th align="center" rowspan="1" colspan="1">Inference(ms)</th><th align="center" rowspan="1" colspan="1">NMS(ms)</th><th align="center" rowspan="1" colspan="1">FP(ms)</th></tr></thead><tbody><tr><td align="center" rowspan="2" colspan="1">640&#x000d7;640</td><td align="center" rowspan="1" colspan="1">YOLOv7-tiny</td><td align="center" rowspan="1" colspan="1">7.2</td><td align="center" rowspan="1" colspan="1">128.2</td><td align="center" rowspan="1" colspan="1">37.8</td><td align="center" rowspan="1" colspan="1">173.2</td></tr><tr><td align="center" rowspan="1" colspan="1">Our model</td><td align="center" rowspan="1" colspan="1">2.9</td><td align="center" rowspan="1" colspan="1">90.7</td><td align="center" rowspan="1" colspan="1">24.8</td><td align="center" rowspan="1" colspan="1">115.5</td></tr><tr><td align="center" rowspan="2" colspan="1">512&#x000d7;512</td><td align="center" rowspan="1" colspan="1">YOLOv7-tiny</td><td align="center" rowspan="1" colspan="1">2.9</td><td align="center" rowspan="1" colspan="1">79.4</td><td align="center" rowspan="1" colspan="1">27.8</td><td align="center" rowspan="1" colspan="1">110.1</td></tr><tr><td align="center" rowspan="1" colspan="1">Our model</td><td align="center" rowspan="1" colspan="1">1.6</td><td align="center" rowspan="1" colspan="1">72.7</td><td align="center" rowspan="1" colspan="1">24.3</td><td align="center" rowspan="1" colspan="1">97.7</td></tr><tr><td align="center" rowspan="2" colspan="1">416&#x000d7;416</td><td align="center" rowspan="1" colspan="1">YOLOv7-tiny</td><td align="center" rowspan="1" colspan="1">1.3</td><td align="center" rowspan="1" colspan="1">69.5</td><td align="center" rowspan="1" colspan="1">28.3</td><td align="center" rowspan="1" colspan="1">99.1</td></tr><tr><td align="center" rowspan="1" colspan="1">Our model</td><td align="center" rowspan="1" colspan="1">1.3</td><td align="center" rowspan="1" colspan="1">65.8</td><td align="center" rowspan="1" colspan="1">18.2</td><td align="center" rowspan="1" colspan="1">85.3</td></tr></tbody></table></alternatives></table-wrap></sec></sec><sec sec-type="conclusions" id="sec017"><title>6 Conclusion</title><p>We propose a lightweight YOLOv7 insulator defect detection algorithm in this paper, which is available for UAV inspection tasks. This algorithm can address solve problems encountered in the process of detecting defective insulators, such as inadequate target extraction, Confusion of objectives and context, and a large number of network operations that are difficult to embed in edge devices. Insulator detection differs from other generic target detection tasks in that insulators have some specific shape, scale and texture characteristics. Also, insulator detection in complex backgrounds is challenging. These task-specific features require algorithms that can better capture and utilize this feature information when processing insulator detection. Therefore, the use of techniques such as DSC, SE, GSConv, and EIoU can help improve the insulator detector&#x02019;s ability to sense and understand insulator-specific features and thus improve detection performance. In addition, these techniques can also reduce computational complexity to a certain extent, enabling insulator detection algorithms to be real-time and efficient in practical applications. A series of experiments were conducted to evaluate the effectiveness of the DSC-SE module in reducing network parameters in the backbone network through depth-separable convolution. The results showed that the introduction of the SE attention mechanism improved the network&#x02019;s extraction capability. Additionally, the GSConv was found to be effective in retaining detailed information of the input image by using SC with DSC mixing and washing operation in feature fusion network. The accuracy was ensured by using the VOVGSCSP module based on GSConv. Finally, the EIoU loss function was found to increase the model&#x02019;s effectiveness in fitting the real boxes to the prediction frame during training. This resulted in faster model convergence and a 0.4% increase in mAP, without affecting the number of model parameters or computational effort. The model has a high detection accuracy of 95.2% on the insulator dataset. It has a relatively low parametric number of only 3.7&#x000d7;106, FLOPs of 5.9G, and a decent FPS of 13 when measured on the Jetson Nano.</p><p>To address the issue of insufficient computing power and platform portability, the use of 5G communication technology in the future will be considered to offload the processing work to the cloud. This will enable inspectors to carry display devices only, achieving intelligent real-time inspection.</p></sec></body><back><ack><p>The authors would like to thank the anonymous reviewers for their critical and constructive comments.</p></ack><ref-list><title>References</title><ref id="pone.0289162.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>J</given-names></name>, <name><surname>Liu</surname><given-names>Z</given-names></name>, <name><surname>Wang</surname><given-names>H</given-names></name>, <name><surname>Nunez</surname><given-names>A</given-names></name>, <name><surname>Han</surname><given-names>Z</given-names></name>. <article-title>Automatic Defect Detection of Fasteners on the Catenary Support Device Using Deep Convolutional Neural Network</article-title>. <source>Ieee Transactions on Instrumentation and Measurement</source>. <year>2018</year>;<volume>67</volume>(<issue>2</issue>):<fpage>257</fpage>&#x02013;<lpage>269</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TIM.2017.2775345</pub-id></mixed-citation></ref><ref id="pone.0289162.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Tao</surname><given-names>X</given-names></name>, <name><surname>Zhang</surname><given-names>D</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Liu</surname><given-names>X</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Xu</surname><given-names>D</given-names></name>. <article-title>Detection of Power Line Insulator Defects Using Aerial Images Analyzed With Convolutional Neural Networks</article-title>. <source>Ieee Transactions on Systems Man Cybernetics-Systems</source>. <year>2020</year>;<volume>50</volume>(<issue>4</issue>):<fpage>1486</fpage>&#x02013;<lpage>1498</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TSMC.2018.2871750</pub-id></mixed-citation></ref><ref id="pone.0289162.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Bo</surname><given-names>T</given-names></name>, <name><surname>Qiao</surname><given-names>Q</given-names></name>, <name><surname>Li</surname><given-names>H</given-names></name>. <article-title>Aerial image recognition of transmission line insulator strings based on color model and texture features</article-title>. <source>Journal of Electric Power Science and Technology</source>. <year>2020</year>;<volume>35</volume>(<issue>4</issue>):<fpage>13</fpage>&#x02013;<lpage>19</lpage>.</mixed-citation></ref><ref id="pone.0289162.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Tan</surname><given-names>P</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>, <name><surname>Xu</surname><given-names>J</given-names></name>, <name><surname>Ma</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>F</given-names></name>, <name><surname>Ding</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Catenary insulator defect detection based on contour features and gray similarity matching</article-title>. <source>Journal of Zhejiang University-SCIENCE A</source>. <year>2020</year>;<volume>21</volume>(<issue>1</issue>):<fpage>64</fpage>&#x02013;<lpage>73</lpage>.</mixed-citation></ref><ref id="pone.0289162.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>H</given-names></name>, <name><surname>Cheng</surname><given-names>L</given-names></name>, <name><surname>Liao</surname><given-names>R</given-names></name>, <name><surname>Zhang</surname><given-names>S</given-names></name>, <name><surname>Yang</surname><given-names>L</given-names></name>. <article-title>Nonlinear mechanical model of composite insulator interface and nondestructive testing method for weak bonding defects</article-title>. <source>Proceedings of the CSEE</source>. <year>2019</year>;<volume>39</volume>(<issue>3</issue>):<fpage>895</fpage>&#x02013;<lpage>905</lpage>.</mixed-citation></ref><ref id="pone.0289162.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Zhai</surname><given-names>Y</given-names></name>, <name><surname>Chen</surname><given-names>R</given-names></name>, <name><surname>Yang</surname><given-names>Q</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>, <name><surname>Zhao</surname><given-names>Z</given-names></name>. <article-title>Insulator fault detection based on spatial morphological features of aerial images.</article-title>
<source>IEEE Access</source>. <year>2018</year>;<volume>6</volume>:<fpage>35316</fpage>&#x02013;<lpage>35326</lpage>.</mixed-citation></ref><ref id="pone.0289162.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Ma</surname><given-names>J</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>, <name><surname>Fang</surname><given-names>Y</given-names></name>. <article-title>Insulator fault detection based on deep learning and Hu invariant moments.</article-title>
<source>China Railw. Soc</source>. <year>2021</year>;<volume>43</volume>:<fpage>71</fpage>&#x02013;<lpage>77</lpage>.</mixed-citation></ref><ref id="pone.0289162.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>C</given-names></name>, <name><surname>Wu</surname><given-names>Y</given-names></name>, <name><surname>Liu</surname><given-names>J</given-names></name>, <name><surname>Sun</surname><given-names>Z</given-names></name>, <name><surname>Xu</surname><given-names>H</given-names></name>. <article-title>Insulator Faults Detection in Aerial Images from High-Voltage Transmission Lines Based on Deep Learning Model.</article-title>
<source>Applied Sciences-Basel.</source>
<year>2021</year>;<volume>11</volume>(<issue>10</issue>). <pub-id pub-id-type="doi">10.3390/app11104647</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Rahman E</surname><given-names>U</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Ahmad</surname><given-names>S</given-names></name>, <name><surname>Ahmad H</surname><given-names>I</given-names></name>, <name><surname>Jobaer</surname><given-names>S</given-names></name>. <article-title>Autonomous Vision-Based Primary Distribution Systems Porcelain Insulators Inspection Using UAVs.</article-title>
<source>Sensors</source>. <year>2021</year>;<volume>21</volume>(<issue>3</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/s21030974</pub-id>
<pub-id pub-id-type="pmid">33540500</pub-id>
</mixed-citation></ref><ref id="pone.0289162.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Ma</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>Q</given-names></name>, <name><surname>Chu</surname><given-names>L</given-names></name>, <name><surname>Zhou</surname><given-names>Y</given-names></name>, <name><surname>Xu</surname><given-names>C</given-names></name>. <article-title>Real-Time Detection and Spatial Localization of Insulators for UAV Inspection Based on Binocular Stereo Vision.</article-title>
<source>Remote Sensing.</source>
<year>2021</year>;<volume>13</volume>(<issue>2</issue>). <pub-id pub-id-type="doi">10.3390/rs13020230</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Gkioxari</surname><given-names>G</given-names></name>, <name><surname>Doll&#x000e1;r</surname><given-names>P</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>. <article-title>Mask r-cnn. Proceedings of the IEEE international conference on computer vision.</article-title>
<year>2017</year>;<fpage>2961</fpage>&#x02013;<lpage>2939</lpage>. <pub-id pub-id-type="doi">10.48550/arXiv.1703.06870</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Girshick</surname><given-names>R.</given-names></name> Fast R-CNN. <source>IEEE International Conference on Computer Vision</source>. <year>2015</year>; <fpage>1440</fpage>&#x02013;<lpage>1448</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV.2015.169</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Ren</surname><given-names>S</given-names></name>, <name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <article-title>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.</article-title>
<source>IEEE Transactions on Pattern Analysis &#x00026; Machine Intelligence.</source>
<year>2017</year>; <volume>39</volume>(<issue>6</issue>): <fpage>1137</fpage>&#x02013;<lpage>1149</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id> .<pub-id pub-id-type="pmid">27295650</pub-id>
</mixed-citation></ref><ref id="pone.0289162.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Lin T</surname><given-names>Y</given-names></name>, <name><surname>Doll&#x000e1;r</surname><given-names>P</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>, <name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Hariharan</surname><given-names>B</given-names></name>, <name><surname>Belongie</surname><given-names>S</given-names></name>. <article-title>Feature pyramid networks for object detection</article-title>. <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>. <year>2017</year>. <pub-id pub-id-type="doi">10.48550/arXiv.1612.03144</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Redmon</surname><given-names>J</given-names></name>, <name><surname>Divvala</surname><given-names>S</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>, <name><surname>Farhadi</surname><given-names>A</given-names></name>. <article-title>You Only Look Once: Unified, Real-Time Object Detection</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</source>
<year>2016</year>; <fpage>779</fpage>&#x02013;<lpage>788</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Redmon</surname><given-names>J</given-names></name>, <name><surname>Farhadi</surname><given-names>A</given-names></name>. <article-title>YOLO9000: better, faster, stronger</article-title>. <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>. <year>2017</year>; <fpage>6517</fpage>&#x02013;<lpage>6525</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2017.690</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Redmon</surname><given-names>J</given-names></name>, <name><surname>Farhadi</surname><given-names>A</given-names></name>. <article-title>Yolov3: An incremental improvement.</article-title>
<source>arXiv preprint arXiv:1804.02767</source>, <year>2018</year>. <pub-id pub-id-type="doi">10.48550/arXiv.1804.02767</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Bochkovskiy</surname><given-names>A</given-names></name>, <name><surname>Wang C</surname><given-names>Y</given-names></name>, <name><surname>Liao H Y</surname><given-names>M</given-names></name>. <article-title>Yolov4: Optimal speed and accuracy of object detection.</article-title>
<source>arXiv preprint arXiv:2004.10934</source>, <year>2020</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2004.10934</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>W</given-names></name>, <name><surname>Anguelov</surname><given-names>D</given-names></name>, <name><surname>Erhan</surname><given-names>D</given-names></name>, <name><surname>Szegedy</surname><given-names>C</given-names></name>, <name><surname>Reed</surname><given-names>S</given-names></name>, <name><surname>Fu C</surname><given-names>Y</given-names></name>. <article-title>SSD: Single Shot MultiBox Detector.</article-title>
<source>14th European Conference on Computer Vision (ECCV).</source>
<year>2016</year>; <volume>9905</volume>: <fpage>21</fpage>&#x02013;<lpage>37</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-46448-0_2</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>Z</given-names></name>, <name><surname>Zhen</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>L</given-names></name>, <name><surname>Qi</surname><given-names>Y</given-names></name>, <name><surname>Kong</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>K</given-names></name>. <article-title>Insulator Detection Method in Inspection Image Based on Improved Faster R-CNN.</article-title>
<source>Energies</source>. <year>2019</year>;<volume>12</volume>(<issue>7</issue>). <pub-id pub-id-type="doi">10.3390/en12071204</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Tan</surname><given-names>P</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>, <name><surname>Ding</surname><given-names>J</given-names></name>, <name><surname>Cui Z</surname><given-names>S</given-names></name>, <name><surname>Ma J</surname><given-names>E</given-names></name>, <name><surname>Sun Y</surname><given-names>L</given-names></name>. <article-title>Mask R-CNN and multifeature clustering model for catenary insulator recognition and defect detection</article-title>. <source>Journal of Zhejiang University-Science A</source>. <year>2022</year>;<volume>23</volume>(<issue>9</issue>):<fpage>745</fpage>&#x02013;<lpage>56</lpage>. <pub-id pub-id-type="doi">10.1631/jzus.A2100494</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Ni</surname><given-names>M</given-names></name>, <name><surname>Lu</surname><given-names>Y</given-names></name>. <article-title>Insulator defect detection for power grid based on light correction enhancement and YOLOv5 model.</article-title>
<source>Energy Reports</source>. <year>2022</year>;<volume>8</volume>:<fpage>807</fpage>&#x02013;<lpage>814</lpage>. <pub-id pub-id-type="doi">10.1016/j.egyr.2022.08.027</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Feng</surname><given-names>Z</given-names></name>, <name><surname>Guo</surname><given-names>L</given-names></name>, <name><surname>Huang</surname><given-names>D</given-names></name>, <name><surname>Li</surname><given-names>R</given-names></name>. <article-title>Electrical insulator defects detection method based on yolov5.</article-title>
<source>2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS).</source>
<year>2021</year>: <fpage>979</fpage>&#x02013;<lpage>984</lpage>. <pub-id pub-id-type="doi">10.1109/DDCLS52934.2021.9455519</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>C</given-names></name>, <name><surname>Ma</surname><given-names>X</given-names></name>, <name><surname>Kong</surname><given-names>X</given-names></name>, <name><surname>Zhu</surname><given-names>H</given-names></name>. <article-title>Research on insulator defect detection algorithm of transmission line based on CenterNet.</article-title>
<source>Plos One.</source>
<year>2021</year>;<volume>16</volume>(<issue>7</issue>):<fpage>e0255135</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0255135</pub-id>
<pub-id pub-id-type="pmid">34324568</pub-id>
</mixed-citation></ref><ref id="pone.0289162.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Sadykova</surname><given-names>D</given-names></name>, <name><surname>Pernebayeva</surname><given-names>D</given-names></name>, <name><surname>Bagheri</surname><given-names>M</given-names></name>, <name><surname>James</surname><given-names>A</given-names></name>. <article-title>IN-YOLO: Real-Time Detection of Outdoor High Voltage Insulators Using UAV Imaging</article-title>. <source>Ieee Transactions on Power Delivery</source>. <year>2020</year>;<volume>35</volume>(<issue>3</issue>):<fpage>1599</fpage>&#x02013;<lpage>601</lpage>. <pub-id pub-id-type="doi">10.1109/TPWRD.2019.2944741</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>R</given-names></name>, <name><surname>Wen</surname><given-names>C</given-names></name>. <article-title>SOD&#x02010;YOLO: A Small Target Defect Detection Algorithm for Wind Turbine Blades Based on Improved YOLOv5.</article-title>
<source>Advanced Theory and Simulations.</source>
<year>2022</year>;<volume>5</volume>(<issue>7</issue>):<fpage>2100631</fpage>. <pub-id pub-id-type="doi">10.1002/adts.202100631</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Qiu</surname><given-names>Q</given-names></name>, <name><surname>Lau</surname><given-names>D</given-names></name>. <article-title>Real-time detection of cracks in tiled sidewalks using YOLO-based method applied to unmanned aerial vehicle (UAV) images.</article-title>
<source>Automation in Construction</source>. <year>2023</year>;<volume>147</volume>:<fpage>104745</fpage>. <pub-id pub-id-type="doi">10.1016/j.autcon.2023.104745</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Tulbure A</surname><given-names>A</given-names></name>, <name><surname>Tulbure A</surname><given-names>A</given-names></name>, <name><surname>Dulf E</surname><given-names>H</given-names></name>. <article-title>A review on modern defect detection models using DCNNs&#x02013;Deep convolutional neural networks</article-title>. <source>Journal of Advanced Research</source>. <year>2022</year>;<volume>35</volume>:<fpage>33</fpage>&#x02013;<lpage>48</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jare.2021.03.015</pub-id>
<pub-id pub-id-type="pmid">35024194</pub-id>
</mixed-citation></ref><ref id="pone.0289162.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Cao</surname><given-names>M</given-names></name>, <name><surname>Fu</surname><given-names>H</given-names></name>, <name><surname>Zhu</surname><given-names>J</given-names></name>, <name><surname>Cai</surname><given-names>C</given-names></name>. <article-title>Lightweight tea bud recognition network integrating GhostNet and YOLOv5.</article-title>
<source>Mathematical biosciences and engineering: MBE.</source>
<year>2022</year>;<volume>19</volume>(<issue>12</issue>):<fpage>12897</fpage>&#x02013;<lpage>12914</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3934/mbe.2022602</pub-id> .<pub-id pub-id-type="pmid">36654027</pub-id>
</mixed-citation></ref><ref id="pone.0289162.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>Z</given-names></name>, <name><surname>Xu</surname><given-names>Z</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>. <article-title>Bidirection-Fusion-YOLOv3: An Improved Method for Insulator Defect Detection Using UAV Image</article-title>. <source>IEEE Transactions on Instrumentation and Measurement</source>. <year>2022</year>;<volume>71</volume>:<fpage>1</fpage>&#x02013;<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1109/TIM.2022.3201499</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Wang C</surname><given-names>Y</given-names></name>, <name><surname>Bochkovskiy</surname><given-names>A</given-names></name>, <name><surname>Liao H Y</surname><given-names>M</given-names></name>. <article-title>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</article-title>. <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>. <year>2023</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2207.02696</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Hu</surname><given-names>J</given-names></name>, <name><surname>Shen</surname><given-names>L</given-names></name>, <name><surname>Sun</surname><given-names>G</given-names></name>. <article-title>Squeeze-and-Excitation Networks.</article-title>
<source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</source>
<year>2018</year>:<fpage>7132</fpage>&#x02013;<lpage>7141</lpage>. <pub-id pub-id-type="doi">10.48550/arXiv.1709.01507</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>H</given-names></name>, <name><surname>Li</surname><given-names>J</given-names></name>, <name><surname>Wei</surname><given-names>H</given-names></name>, <name><surname>Liu</surname><given-names>Z</given-names></name>, <name><surname>Zhan</surname><given-names>Z</given-names></name>, <name><surname>Ren</surname><given-names>Q</given-names></name>. <article-title>Slim-neck by GSConv: A better design paradigm of detector architectures for autonomous vehicles.</article-title>
<source>arXiv preprint arXiv:2206.02424</source>, <year>2022</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2206.02424</pub-id>.</mixed-citation></ref><ref id="pone.0289162.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>Y</given-names></name>, <name><surname>Hwang</surname><given-names>J</given-names></name>, <name><surname>Lee</surname><given-names>S</given-names></name>, <name><surname>Bae</surname><given-names>Y</given-names></name>, <name><surname>Park</surname><given-names>J</given-names></name>. <article-title>An energy and GPU-computation efficient backbone network for real-time object detection</article-title>. <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition workshops</source>. <year>2019</year>.</mixed-citation></ref><ref id="pone.0289162.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Wang C</surname><given-names>Y</given-names></name>, <name><surname>Liao H Y</surname><given-names>M</given-names></name>, <name><surname>Wu Y</surname><given-names>H</given-names></name>, <etal>et al</etal>. <article-title>CSPNet: A New Backbone that can Enhance Learning Capability of CNN</article-title>. <source>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).</source>
<year>2020</year>:<fpage>390</fpage>&#x02013;<lpage>391</lpage>.</mixed-citation></ref><ref id="pone.0289162.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Zhang Y</surname><given-names>F</given-names></name>, <name><surname>Ren</surname><given-names>W</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Jia</surname><given-names>Z</given-names></name>, <name><surname>Wang</surname><given-names>L</given-names></name>, <name><surname>Tan</surname><given-names>T</given-names></name>. <article-title>Focal and efficient IOU loss for accurate bounding box regression.</article-title>
<source>Neurocomputing</source>. <year>2022</year>;<volume>506</volume>:<fpage>146</fpage>&#x02013;<lpage>157</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neucom.2022.07.042</pub-id></mixed-citation></ref></ref-list></back><sub-article article-type="aggregated-review-documents" id="pone.0289162.r001" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0289162.r001</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yun</surname><given-names>Ji-Hoon</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2023 Ji-Hoon Yun</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Ji-Hoon Yun</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0289162" id="rel-obj001" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">8 May 2023</named-content>
</p><p><!-- <div> -->PONE-D-23-09299<!-- </div> --><!-- <div> -->A lightweight YOLOv7 insulator defect detection algorithm based on DSC-SE<!-- </div> --></p><p><!-- <div> -->PLOS ONE</p><p>Dear Dr. Wang,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please submit your revised manuscript by Jun 22 2023 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!-- </div> --></p><p>
<list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Ji-Hoon Yun</p><p>Academic Editor</p><p>PLOS ONE</p><p>Journal Requirements:&#x000a0;</p><p>When submitting your revision, we need you to address these additional requirements.</p><p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at <ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and&#x000a0;</p><p>
<ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
</p><p>2. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, all author-generated code must be made available without restrictions upon publication of the work. Please review our guidelines at <ext-link xlink:href="https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code" ext-link-type="uri">https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code</ext-link> and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse.</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!-- <font color="black"> -->
<bold>Comments to the Author</bold>
</p><p>1. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Partly</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->2. Has the statistical analysis been performed appropriately and rigorously? <!-- </font> --></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>Reviewer #3:&#x000a0;No</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->3. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;No</p><p>Reviewer #3:&#x000a0;No</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->4. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;No</p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->5. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!-- </font> --></p><p>Reviewer #1:&#x000a0;The study proposed a lightweight YOLOv7 for defect detection of insulator, which is a very interesting topic. The paper shows some state-of-the-art results in the application of computer vision. However, the paper should be polished and some descriptions should be more understandable. To be concrete, following are the comments for the authors:</p><p>1. The sentence &#x0201c;Unfortunately, manual inspection techniques are ineffective due to the constraints of the distribution range and environment of overhead transmission lines, and it is hardly feasible to satisfy the demands of insulator inspection by manual inspection alone.&#x0201d; is not easily understood. What is the situation of &#x0201c;environment of overhead transmission lines&#x0201d; in the way of manual inspection?</p><p>2. It is suggested to enrich the big picture of using YOLO detectors in defect detection. Many latest papers are recommended to be added as the references:</p><p>(1) Rui Zhang, Chuanbo Wen, SOD-YOLO: A Small Target Defect Detection Algorithm for Wind Turbine Blades Based on Improved YOLOv5, Advanced Theory and Simulations, 5, 2100631, 2022</p><p>(2) Qiwen Qiu, Denvid Lau, Real-time detection of cracks in tiled sidewalks using YOLO-based method applied to unmanned aerial vehicle (UAV) images, Automation in Construction, 147, 104745, 2023.</p><p>(3) Andrei-Alexandru Tulbure, Adrian-Alexandru Tulbure, Eva-Henrietta Dulf, A review on modern defect detection models using DCNNs &#x02013; Deep convolutional neural networks, Journal of Advanced Research, 35, 33-48, 2022.</p><p>3. The introduction does not review the feasibility and advantages of YOLOv7-tiny when using in UAV. A lot of terms such as &#x0201c;high model computational complexity&#x0201d;, &#x0201c;DSC-SE module&#x0201d;, &#x0201c;VOVGSCSP module&#x0201d; should be explained. For a reader from other fields, these technical expressions may be not easily understood.</p><p>4. In section 2.1, please mention the functionalities of &#x0201c;ELAN and MP structures&#x0201d;. Besides, details of YOLOv5 should be included to clearly show the improvements from YOLOv7.</p><p>5. The number of sub-section titles is wrongly organized.</p><p>6. Is it needed to place a reference for In Fig. 1?</p><p>7. For sentence &#x0201c;In addition, the feature map processed by the backbone network contains a large amount of target information, but the shallow network has a small perceptual field, limited extraction capability, and tends to view local information, making it difficult to perceive and extract the input image information comprehensively&#x0201d;, please demonstrate the limitations of backbone network technically in a graph.</p><p>8. How come the initial learning rate of 0.01 for training is set?</p><p>9. Increase the size of fonts in Fig. 6 as they are too small.</p><p>10. In Fig. 8, please describe more details of the defects under evaluation. How is the situation of cable can be defined as defect?</p><p>11. Suggest a note of the proposed model in Fig. 7, not just writing &#x0201c;Ours&#x0201d;. Regarding this comparison, why not train more previous YOLO versions and compare them all together in this graph. So the data would be more convincing and informative.</p><p>12. In Table 4, it is suggested to show the deviation of testing results for each model.</p><p>13. In Fig. 8, how do you collect the complex background images? Using UAV or other devices.</p><p>14. May be important to discuss the detectability of defect. How small the defect can be found by YOLOv7-tiny?</p><p>Reviewer #2:&#x000a0;This paper proposes a lightweight YOLOv7 insulator defect detection algorithm to address the defect detection speed and high model complexity, which designs a lightweight DCS-SE module and uses GSConv for feature fusion. Generally, this work achieves speed and accuracy improvements.</p><p>1. English expression in this paper needs better polishing.</p><p>2. There are two sections 2 please check it.</p><p>3. Many abbreviations are used in this paper, complete spellings should be given in the first mention.</p><p>4. In experimental parts, it is better to compare with more insulator defect detection methods.</p><p>5. Please show computational complexity analysis on the proposed method.</p><p>6. Recently, some insulator detection methods based on YOLO, which should be introduce in this paper.</p><p>D. Sadykova, D. Pernebayeva, M. Bagheri and A. James, "IN-YOLO: Real-Time Detection of Outdoor High Voltage Insulators Using UAV Imaging," IEEE Transactions on Power Delivery, vol. 35, no. 3, pp. 1599-1601, 2020.</p><p>Y. Li, M. Ni, Y. Lu. Insulator defect detection for power grid based on light correction enhancement and YOLOv5 model, Energy Reports, 2022, 13(8): 807-814.</p><p>Z. Yang, Z. Xu and Y. Wang, "Bidirection-Fusion-YOLOv3: An Improved Method for Insulator Defect Detection Using UAV Image," IEEE Transactions on Instrumentation and Measurement, vol. 71, pp. 1-8, 2022.</p><p>Reviewer #3:&#x000a0;The author presents an algorithm for detecting insulator defects using YOLOV7 and attention mechanism, which was a popular approach three or four years ago. The paper&#x02019;s primary objective was to reduce the model&#x02019;s complexity while maintaining detection accuracy. Real-time model performance was evaluated on a resource-limited platform (Nano), which supports the conclusion of the paper. To improve the manuscript&#x02019;s quality, the author is advised to make modifications in the following areas.</p><p>1. DSC, SE, GSConv, and EIoU are all mature and successful modules/loss, and that are also frequently used to improve the YOLO structure. However, the author needs to explain the applicability of these modules and the EIoU loss to the specific insulator detection task and how they can improve detection performance while reducing computational complexity. In other words, what makes insulator detection different from other generic object detection tasks that necessitates the use of these particular technologies?</p><p>2. The algorithm evaluated in the experiment is too traditional, such as Faster RCNN. The author should compare newer lightweight networks developed within the past three years, particularly those that have been designed for insulator detection. Furthermore, I have reservations regarding the efficacy of Faster RCNN in the context of insulator detection.</p><p>3. The conducted ablation experiment is insufficient in shedding light on the significance of the VOVGSCSP module. It would be interesting to see the results of a model trained without the VOVGSCSP module to ascertain the module&#x02019;s contribution to the overall results.</p><p>4. In the experiment, the author employed data augmentation to augment the dataset. However, there was a lack of proper data isolation during the division of training and verification sets, resulting in the possibility of duplicated images from the same original image in both sets. Such an experiment lacks persuasiveness. Additionally, the study did not include a test set, and all the reported results were based on the verification set, which indicates a relatively lax experimental design.</p><p>5.The introduction section has less content on insulator defect detection.</p><p>There are several issues with the article that need to be addressed:</p><p>1. There are several grammatical and spelling errors throughout the text, such as &#x0201c;pychar,&#x0201d; which the author should carefully review.</p><p>2. Some of the formulas in the text are not centered properly, and the length of some tables extends beyond the page width.</p><p>3. Many of the characters in the experimental results are difficult to read, especially in Figure 6.</p><p>4. The best results should be highlighted to provide clarity.</p><p>5. The source of the data set should be referenced.</p><p>6. The format of the references is messy.</p><p>Reviewer #4:&#x000a0;I believe that this work deals with a relevant and current theme and has good potential for publication. It presents some points of originality, especially regarding the adaptation of the architecture of the YOLOv7 model. In addition, the author managed to reduce the computational cost of the referred model and obtained better results than other works present in the literature, with an accuracy of insulator defect detection of 95.2% for the presented dataset (CPLID). It was even possible to use the Jetson Nano computer to evaluate the results.</p><p>In my evaluation, the weakness of the paper lies in the superficial way in which the author of the paper presented the results very much. It is necessary to reevaluate the writing of the whole chapter 4. Figures 6 and 7, for example, are practically illegible and the description of the results is not adequate. For Figure 8, I believe it would be interesting to present a larger number of images and demonstrate examples that the model did not perform well.</p><p>**********</p><p><!-- <font color="black"> -->6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!-- </font> --></p><p>Reviewer #1:&#x000a0;<bold>Yes:&#x000a0;</bold>Qiwen Qiu</p><p>Reviewer #2:&#x000a0;No</p><p>Reviewer #3:&#x000a0;No</p><p>Reviewer #4:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p></body></sub-article><sub-article article-type="author-comment" id="pone.0289162.r002"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0289162.r002</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0289162" id="rel-obj002" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">31 May 2023</named-content>
</p><p>Dear Editor and Reviewers&#x0ff1a;</p><p>Thank you for allowing us to revise our manuscript titled 'A Lightweight YOLOv7 Insulator Defect Detection Algorithm Based on DSC-SE'. We are grateful for the constructive comments and suggestions provided by the reviewers and editors, which have greatly aided us in improving our work. We have carefully reviewed and incorporated the suggested revisions, which are highlighted in red in the revised version. We respectfully submit this updated version for your consideration. A point-to-point reply is listed below where the comments of reviewers are in blue and the replies are in black.</p><p>Comments from the Editors:</p><p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming.</p><p>2. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, all author-generated code must be made available without restrictions upon publication of the work.</p><p>Rely: Thank you very much for your suggestion, we have revised the format according to the journal's requirements. Due to our research with the Institute, the code in the experiment is not available. If it must be uploaded, please lie with us to contact.</p><p>Reviewer #1 Comments to Author:</p><p>1. Comment: The sentence &#x0201c;Unfortunately, manual inspection techniques are ineffective due to the constraints of the distribution range and environment of overhead transmission lines, and it is hardly feasible to satisfy the demands of insulator inspection by manual inspection alone.&#x0201d; is not easily understood. What is the situation of &#x0201c;environment of overhead transmission lines&#x0201d; in the way of manual inspection?</p><p>1. Rely: Thank you for reviewing the manuscript carefully and valuable questions. We have described the environment for manual inspection of insulators in overhead lines in a revised manuscript. </p><p>Pages 2-3, lines 39-44: The limitations of the distribution range of overhead transmission lines and the overhead environment make it difficult to inspect insulators for minor surface defects through manual observation with the naked eye or binoculars. Climbing up the tower for a closer look is an inefficient method, making it challenging to meet the requirements of real-time insulator inspection with manual inspection alone.</p><p>2. Comment: It is suggested to enrich the big picture of using YOLO detectors in defect detection. (1) Rui Zhang, Chuanbo Wen, SOD-YOLO: A Small Target Defect Detection Algorithm for Wind Turbine Blades Based on Improved YOLOv5, Advanced Theory and Simulations, 5, 2100631, 2022</p><p>(2) Qiwen Qiu, Denvid Lau, Real-time detection of cracks in tiled sidewalks using YOLO-based method applied to unmanned aerial vehicle (UAV) images, Automation in Construction, 147, 104745, 2023.</p><p>(3) Andrei-Alexandru Tulbure, Adrian-Alexandru Tulbure, Eva-Henrietta Dulf, A review on modern defect detection models using DCNNs &#x02013; Deep convolutional neural networks, Journal of Advanced Research, 35, 33-48, 2022.</p><p>2. Rely: Thank you for your guidance and suggestions, we have added recent references to the revised manuscript. Specifically, we have cited lines 88-97 of the article.</p><p>3. Comment: The introduction does not review the feasibility and advantages of YOLOv7-tiny when using in UAV. A lot of terms such as &#x0201c;high model computational complexity&#x0201d;, &#x0201c;DSC-SE module&#x0201d;, &#x0201c;VOVGSCSP module&#x0201d; should be explained. For a reader from other fields, these technical expressions may be not easily understood.</p><p>3. Rely: I am thankful for your suggestions, and we have reviewed the advantages and feasibility of the YOLOv7-tiny for UAV applications in the introduction of the new paper. We also explain the model computational complexity, the DSC-SE module, and the VOVGSCSP module.</p><p>Pages 6, lines 109-114: In 2022, Wang et al. proposed YOLOv7 as the latest target detection model, which surpasses all current models in terms of both detection speed and accuracy. YOLOv7 performs exceptionally well in detecting objects in images, providing precise location and classification information. YOLOv7-tiny is a lighter version of YOLOv7, with fewer parameters and lower computational complexity, making it suitable for real-time object detection in resource-constrained environments.</p><p>Pages 5, lines 103-107: The computational complexity of the model is divided into spatial complexity and temporal complexity. The spatial complexity determines the number of parameters of the model, and the temporal complexity determines the detection time of the model.</p><p>Pages 6, lines 126-129: The GSConv, which is comparable to the standard convolutional extraction capability, is then used in the feature fusion network to reduce the time complexity of the model, and the VOVGSCSP module is designed with residual edges based on the GSConv to accelerate inference and maintain accuracy.</p><p>4. Comment: In section 2.1, please mention the functionalities of &#x0201c;ELAN and MP structures&#x0201d;. Besides, details of YOLOv5 should be included to clearly show the improvements from YOLOv7.</p><p>4. Rely: Thanks for your valuable comments, we have detailed the improved ELAN module and MP module of YOLOv5 in lines 134-148 of the revised manuscript.</p><p>5. Comment: The number of sub-section titles is wrongly organized.</p><p>5. Rely: Thank, you very much for your suggestions. We have carefully reorganized the sub-section titles in the new paper.</p><p>6. Comment: Is it needed to place a reference for In Fig. 1?</p><p>6. Rely: Thank you very much for your careful work. Since we have added a new figure, this figure is now Fig 2 and has been referenced in the new paper.</p><p>7. Comment: For sentence &#x0201c;In addition, the feature map processed by the backbone network contains a large amount of target information, but the shallow network has a small perceptual field, limited extraction capability, and tends to view local information, making it difficult to perceive and extract the input image information comprehensively&#x0201d;, please demonstrate the limitations of backbone network technically in a graph.</p><p>7. Rely: I appreciate your suggestions, and we introduced the limitations of the shallow network in the backbone network from Fig 3 in lines 196-205 of the revised manuscript.</p><p>8. Comment: How come the initial learning rate of 0.01 for training is set?</p><p>8. Rely: I refer to the paper "Cyclical Learning Rates for Training Neural Networks", the initial learning rate of 0.01 is more effective on the YOLO detector.</p><p>9. Comment: Increase the size of fonts in Fig. 6 as they are too small.</p><p>9. Rely: We greatly appreciate your valuable suggestion. In the new paper, the original Figure 6 was not obvious for the experimental results demonstrated, and we removed it.</p><p>10. Comment: In Fig. 8, please describe more details of the defects under evaluation. How is the situation of cable can be defined as defect?</p><p>10. Rely: Thanks for your valuable comments and professional question. We have described more about the definition of defects in insulators in the revised manuscript.</p><p>11. Comment: Suggest a note of the proposed model in Fig.7, not just writing &#x0201c;Ours&#x0201d;. Regarding this comparison, why not train more previous YOLO versions and compare them all together in this graph. So the data would be more convincing and informative.</p><p>11. Rely: Thanks for your suggestion, we have annotated the improved algorithm with new notes in the new paper and compared the training results of multiple YOLO versions in Fig 8.</p><p>12. Comment: In Table 4, it is suggested to show the deviation of testing results for each model.</p><p>12. Rely: Thanks for your valuable comments, our test results for each model of Table 4 are shown in Fig 9 in the new manuscript to increase the persuasiveness.</p><p>13. Comment: In Fig. 8, how do you collect the complex background images? Using UAV or other devices.</p><p>13. Rely: Thank you for reviewing the manuscript carefully and valuable questions. In the revised manuscript, the images tested are from aerial drone photography. And we show more examples of the test images, as in Fig 10.</p><p>14. Comment: May be important to discuss the detectability of defect. How small the defect can be found by YOLOv7-tiny?</p><p>14. Rely: Thanks a lot for your questions. We talked about the detectability of insulator defect in the new manuscript. We cannot give an exact indication of how small a defect the YOLOv7-tiny can detect. According to our experiments, YOLOv7-tiny will miss the detection of smaller defects with low contrast, as shown in Fig 10a.</p><p>Reviewer #2 Comments to Author:</p><p>1 and 2. Comment: English expression in this paper needs better polishing. There are two sections 2 please check it.</p><p>1. Rely: We greatly appreciate your valuable suggestion. We have had the article polished by someone who specializes in it.</p><p>3. Comment: Many abbreviations are used in this paper, complete spellings should be given in the first mention.</p><p>3. Rely: Thanks for your valuable comments, we have given the complete spelling of the abbreviations that appear for the first time in the paper.</p><p>4. Comment: In experimental parts, it is better to compare with more insulator defect detection methods.</p><p>4. Rely: Thank, you very much for your suggestions. In the experimental section, we have added different YOLO versions of the algorithm, including YOLOv4-tiny, YOLOv5s, and YOLOv6 for experimental comparison.</p><p>5. Comment: Please show computational complexity analysis on the proposed method.</p><p>5. Rely: Thanks for your valuable comments and professional suggestions. We have given the analysis of the computational complexity process of DSC-SE in a new manuscript.</p><p>6. Comment: Recently, some insulator detection methods based on YOLO, which should be introduce in this paper.</p><p>D. Sadykova, D. Pernebayeva, M. Bagheri and A. James, "IN-YOLO: Real-Time Detection of Outdoor High Voltage Insulators Using UAV Imaging," IEEE Transactions on Power Delivery, vol. 35, no. 3, pp. 1599-1601, 2020.</p><p>Y. Li, M. Ni, Y. Lu. Insulator defect detection for power grid based on light correction enhancement and YOLOv5 model, Energy Reports, 2022, 13(8): 807-814.</p><p>Z. Yang, Z. Xu and Y. Wang, "Bidirection-Fusion-YOLOv3: An Improved Method for Insulator Defect Detection Using UAV Image," IEEE Transactions on Instrumentation and Measurement, vol. 71, pp. 1-8, 2022.</p><p>6. Rely: We have added some articles on insulator detection based on the YOLO algorithm as references in the introduction section of the revised manuscript.</p><p>Reviewer #3 Comments to Author:</p><p>1. Comment: DSC, SE, GSConv, and EIoU are all mature and successful modules/loss, and that are also frequently used to improve the YOLO structure. However, the author needs to explain the applicability of these modules and the EIoU loss to the specific insulator detection task and how they can improve detection performance while reducing computational complexity. In other words, what makes insulator detection different from other generic object detection tasks that necessitates the use of these particular technologies?</p><p>1. Rely: They are indeed proven and successful techniques, and the modules and loss functions can play an important role in specific insulator detection tasks. Their applicability in insulator detection and how they can improve detection performance and reduce computational complexity are explained below:</p><p>Pages 12, lines 219-225: DSC (Depthwise Separable Convolution): Defects in insulator inspection tasks usually have small dimensions and low contrast, which makes accurate detection challenging. In this case, the DSC module plays an important role. By decomposing the standard convolution into depth and point-by-point convolutions, the DSC module reduces computational complexity while maintaining expressiveness. This is very beneficial for insulator detection because the task requires real-time or quasi-real-time analysis over a large number of image samples, so efficiency is critical.</p><p>Pages 10, 190-193: SE (Squeeze-and-Excitation): The SE module enhances feature representation by adaptively learning the relationships between feature channels. In insulator detection, the SE module helps the network to better understand the insulator features and improve the focus on critical information. This helps to improve the accuracy and robustness of insulator detectors.</p><p>Pages 14, lines 269-274: GSConv (Grid Sensitive Convolution): The GSConv module is an adaptive convolution mechanism that dynamically adjusts the shape and size of the convolution kernel based on the content of the input image. In insulator detection, the GSConv module enables the network to focus more on the grid region where the insulators are located and extract more accurate features by adaptive convolution. This helps to improve the localization accuracy and detection performance of the insulator detector.</p><p>Pages 18, lines 326-328: EIoU (Embedding IoU) loss function: The EIoU loss function is an optimized objective function that more accurately evaluates the degree of overlap between target and predicted frames during the training process. In insulator detection, the EIoU loss function can better measure the similarity between the detection results and the real insulator frames, thus improving the learning effect of the detector and enhancing the detection accuracy.</p><p>Pages 27, lines 480-489: Insulator detection differs from other generic target detection tasks in that insulators have some specific shape, scale, and texture characteristics. Also, insulator detection in complex backgrounds is challenging. These task-specific features require algorithms that can better capture and utilize this feature information when processing insulator detection. Therefore, the use of techniques such as DSC, SE, GSConv, and EIoU can help improve the insulator detector's ability to sense and understand insulator-specific features and thus improve detection performance. In addition, these techniques can also reduce computational complexity to a certain extent, enabling insulator detection algorithms to be real-time and efficient in practical applications.</p><p>2. Comment: The algorithm evaluated in the experiment is too traditional, such as Faster RCNN. The author should compare newer lightweight networks developed within the past three years, particularly those that have been designed for insulator detection. Furthermore, I have reservations regarding the efficacy of Faster RCNN in the context of insulator detection.</p><p>2. Rely: Thank you very much for your suggestions. We delete the inappropriate Faster-RCNN algorithm and evaluate it against the last three years of lightweight models including YOLOv4-tiny, YOLOv5s, and YOLOv6.</p><p>3. Comment: The conducted ablation experiment is insufficient in shedding light on the significance of the VOVGSCSP module. It would be interesting to see the results of a model trained without the VOVGSCSP module to ascertain the module&#x02019;s contribution to the overall results.</p><p>3. Rely: Thanks to your suggestion, we performed ablation experiments on the effect of the VOVGSCSP module on the whole model in the revised paper.</p><p>4. Comment: In the experiment, the author employed data augmentation to augment the dataset. However, there was a lack of proper data isolation during the division of training and verification sets, resulting in the possibility of duplicated images from the same original image in both sets. Such an experiment lacks persuasiveness. Additionally, the study did not include a test set, and all the reported results were based on the verification set, which indicates a relatively lax experimental design.</p><p>4. Rely: I am thankful for your suggestions, and we divided the dataset into training, validation, and test sets in the ratio of 8:1:1. We only perform data enhancement in the training set so that there are no duplicate images in the validation and test sets.</p><p>5. Comment: The introduction section has less content on insulator defect detection.</p><p>5. Rely: I appreciate your suggestions, and we have added a review of insulator detection algorithms from recent years in lines 77-102 of the revised manuscript.</p><p>6. Comment: There are several issues with the article that need to be addressed: 1. There are several grammatical and spelling errors throughout the text, such as &#x0201c;pychar,&#x0201d; which the author should carefully review.</p><p>2. Some of the formulas in the text are not centered properly, and the length of some tables extends beyond the page width.</p><p>3. Many of the characters in the experimental results are difficult to read, especially in Figure 6.</p><p>4. The best results should be highlighted to provide clarity.</p><p>5. The source of the data set should be referenced.</p><p>6. The format of the references is messy.</p><p>6. Rely: Thank you for your advice. We have had native English speakers polish the new manuscript. We have made changes to the formulas and tables that appear in the article in accordance with the requirements of this journal. Very sorry, we have redescribed Fig 6. We have bolded the font of the best results to highlight them and detailed the source of the dataset. We have modified the references in the new manuscript according to the format required by the journal.</p><p>Reviewer #4 Comments to Author:</p><p> 1. Comment: In my evaluation, the weakness of the paper lies in the superficial way in which the author of the paper presented the results very much. It is necessary to reevaluate the writing of the whole chapter 4. Figures 6 and 7, for example, are practically illegible and the description of the results is not adequate. For Figure 8, I believe it would be interesting to present a larger number of images and demonstrate examples that the model did not perform well.</p><p> 1. Rely: Thank you very much for your suggestions. We re-evaluated the experiments in Chapter 4 in the revised manuscript. And the results of the original Fig 6 and Fig 7 are described in detail. We removed the inappropriate Faster-RCNN algorithm and evaluated it against lightweight models from the past three years, including YOLOv4-tiny, YOLOv5s, and YOLOv6, shown in Fig 8 to increase persuasiveness. We also talked about the detectability of insulator defects by different models. We have shown more detection results in the new manuscript, including cases of poor results in dim environments.</p><supplementary-material id="pone.0289162.s001" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Response to Reviewers .docx</named-content></p></caption><media xlink:href="pone.0289162.s001.docx"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0289162.r003" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0289162.r003</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yun</surname><given-names>Ji-Hoon</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2023 Ji-Hoon Yun</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Ji-Hoon Yun</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0289162" id="rel-obj003" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">27 Jun 2023</named-content>
</p><p><!-- <div> -->PONE-D-23-09299R1<!-- </div> --></p><p>A lightweight YOLOv7 insulator defect detection algorithm based on DSC-SE</p><p>PLOS ONE</p><p>Dear Dr. Wang,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please submit your revised manuscript by Aug 11 2023 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:</p><p>
<list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p><!-- <div> -->If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Ji-Hoon Yun</p><p>Academic Editor</p><p>PLOS ONE</p><p>Journal Requirements:</p><p>Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article&#x02019;s retracted status in the References list and also include a citation and full reference for the retraction notice.</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!-- <font color="black"> -->
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.<!-- </font> --></p><p>Reviewer #1:&#x000a0;All comments have been addressed</p><p>Reviewer #2:&#x000a0;(No Response)</p><p>Reviewer #3:&#x000a0;All comments have been addressed</p><p>**********</p><p><!-- <font color="black"> -->2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!-- </font> --></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;(No Response)</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->3. Has the statistical analysis been performed appropriately and rigorously? <!-- </font> --></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;(No Response)</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!-- </font> --></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;(No Response)</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!-- </font> --></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;(No Response)</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!-- <font color="black"> -->6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!-- </font> --></p><p>Reviewer #1:&#x000a0;The study makes improvements in the lightweight YOLOv7 for defect detection, which is acceptable for publication. From the revised paper, some presentation typos can be avoided:</p><p>1. Please give descriptions of &#x0201c;(a), (b), (c), and (d)&#x0201d; in Fig. 10.</p><p>2. Journal names in some references [3, 27, &#x02026;] are missing. Please also check the spelling of author names in some references [22, 27, &#x02026;]</p><p>Reviewer #2:&#x000a0;Based on the revision, this paper is improved. However, the reference format should be re-phrased further.</p><p>Reviewer #3:&#x000a0;All my problems have been addressed well in this revised manuscript and response letter, and I have no more comments.</p><p>**********</p><p><!-- <font color="black"> -->7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!-- </font> --></p><p>Reviewer #1:&#x000a0;<bold>Yes:&#x000a0;</bold>Qiwen Qiu</p><p>Reviewer #2:&#x000a0;No</p><p>Reviewer #3:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p></body></sub-article><sub-article article-type="author-comment" id="pone.0289162.r004"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0289162.r004</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0289162" id="rel-obj004" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">5 Jul 2023</named-content>
</p><p>1. Comment: Please give descriptions of &#x0201c;(a), (b), (c), and (d)&#x0201d; in Fig. 10.</p><p>1. Rely: Thank you very much for your suggestion, and we have described Fig 10 in our revised paper. </p><p>Pages 25, lines 299-300: Fig 10a shows the case with low light, and Fig 10b, Fig 10c and Fig 10d show the case with background interference from towers and transmission lines.</p><p>Pages 26,lines 302-303: As seen in Fig 10c, YOLOv5s and YOLOv6 can detect small insulator defects in complex backgrounds, but with low accuracy.</p><p>2. Comment: Journal names in some references [3, 27, &#x02026;] are missing. Please also check the spelling of author names in some references [22, 27, &#x02026;]</p><p>2. Rely: Thanks for your valuable comments and professional suggestions. We have double-checked and revised the references in the revised paper.</p><p>Reviewer #2 Comments to Author:</p><p>1 and 2. Comment: Based on the revision, this paper is improved. However, the reference format should be re-phrased further.</p><p>1. Rely: We greatly appreciate your valuable suggestion. We have checked and rectified the formatting of the references in the new manuscript.</p><p>Reviewer #3 Comments to Author:</p><p>1. Comment: All my problems have been addressed well in this revised manuscript and response letter, and I have no more comments.</p><p>1. Rely: Thanks for your valuable comments and professional suggestions.</p><supplementary-material id="pone.0289162.s002" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Response to Reviewers .docx</named-content></p></caption><media xlink:href="pone.0289162.s002.docx"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></body></sub-article><sub-article article-type="editor-report" id="pone.0289162.r005" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0289162.r005</article-id><title-group><article-title>Decision Letter 2</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yun</surname><given-names>Ji-Hoon</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2023 Ji-Hoon Yun</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Ji-Hoon Yun</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0289162" id="rel-obj005" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">13 Jul 2023</named-content>
</p><p>A lightweight YOLOv7 insulator defect detection algorithm based on DSC-SE</p><p>PONE-D-23-09299R2</p><p>Dear Dr. Wang,</p><p>We&#x02019;re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p><p>Within one week, you&#x02019;ll receive an e-mail detailing the required amendments. When these have been addressed, you&#x02019;ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p><p>An invoice for payment will follow shortly after the formal acceptance. To ensure an efficient process, please log into Editorial Manager at <ext-link xlink:href="http://www.editorialmanager.com/pone/" ext-link-type="uri">http://www.editorialmanager.com/pone/</ext-link>, click the 'Update My Information' link at the top of the page, and double check that your user information is up-to-date. If you have any billing related questions, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p><p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they&#x02019;ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p><p>Kind regards,</p><p>Ji-Hoon Yun</p><p>Academic Editor</p><p>PLOS ONE</p><p>Additional Editor Comments (optional):</p><p>Reviewers' comments:</p></body></sub-article><sub-article article-type="editor-report" id="pone.0289162.r006" specific-use="acceptance-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0289162.r006</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yun</surname><given-names>Ji-Hoon</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2023 Ji-Hoon Yun</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Ji-Hoon Yun</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0289162" id="rel-obj006" related-article-type="reviewed-article"/></front-stub><body><p>
<named-content content-type="letter-date">3 Aug 2023</named-content>
</p><p>PONE-D-23-09299R2 </p><p>A lightweight YOLOv7 insulator defect detection algorithm based on DSC-SE </p><p>Dear Dr. Wang:</p><p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now with our production department. </p><p>If your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information please contact <email>onepress@plos.org</email>.</p><p>If we can help with anything else, please email us at <email>plosone@plos.org</email>. </p><p>Thank you for submitting your work to PLOS ONE and supporting open access. </p><p>Kind regards, </p><p>PLOS ONE Editorial Office Staff</p><p>on behalf of</p><p>Dr. Ji-Hoon Yun </p><p>Academic Editor</p><p>PLOS ONE</p></body></sub-article></article>
